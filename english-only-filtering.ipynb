{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b642e7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\ari\\anaconda3\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (1.1.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in c:\\users\\ari\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ari\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ari\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\ari\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\ari\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\ari\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ari\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e82d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b66e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ai4privacy/open-pii-masking-500k-ai4privacy\n",
      "Available splits: ['train', 'validation']\n",
      "\n",
      "  train: 464,150 examples\n",
      "  validation: 116,077 examples\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME: str = \"ai4privacy/open-pii-masking-500k-ai4privacy\"\n",
    "\n",
    "# Load only the splits we need (train and validation)\n",
    "# The dataset will be cached locally after the first download\n",
    "raw_dataset: DatasetDict = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    split=None,  # Load all available splits, then we select what we need\n",
    ")\n",
    "\n",
    "# Display basic information about the downloaded dataset\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Available splits: {list(raw_dataset.keys())}\")\n",
    "print()\n",
    "\n",
    "for split_name, split_data in raw_dataset.items():\n",
    "    print(f\"  {split_name}: {len(split_data):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad4f70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Features (Schema):\n",
      "{'source_text': Value('string'), 'masked_text': Value('string'), 'privacy_mask': List({'label': Value('string'), 'start': Value('int64'), 'end': Value('int64'), 'value': Value('string'), 'label_index': Value('int64')}), 'split': Value('string'), 'uid': Value('int64'), 'language': Value('string'), 'region': Value('string'), 'script': Value('string'), 'mbert_tokens': List(Value('string')), 'mbert_token_classes': List(Value('string'))}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the features (schema) of the dataset\n",
    "print(\"Dataset Features (Schema):\")\n",
    "print(raw_dataset[\"train\"].features)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "963a65f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample record from training set:\n",
      "  source_text: 20:10:26 Venanzius Höttermann Revés యొక్క వివాహం July/95 నాడు జరిగింది, Tadaలో Faizabad Road వద్ద.\n",
      "  masked_text: [TIME_1] [GIVENNAME_1] [SURNAME_1] యొక్క వివాహం [DATE_1] నాడు జరిగింది, [CITY_1]లో [STREET_1] వద్ద.\n",
      "  privacy_mask: [{'label': 'TIME', 'start': 0, 'end': 8, 'value': '20:10:26', 'label_index': 1}, {'label': 'GIVENNAME', 'start': 9, 'end': 18, 'value': 'Venanzius', 'label_index': 1}, {'label': 'SURNAME', 'start': 19...\n",
      "  split: train\n",
      "  uid: 5387382\n",
      "  language: te\n",
      "  region: IN\n",
      "  script: Telu\n",
      "  mbert_tokens: ['20', ':', '10', ':', '26', 'Ve', '##nan', '##ziu', '##s', 'H', '##ötter', '##mann', 'Rev', '##és', 'యొక్క', 'వి', '##వ', '##ా', '##హం', 'July', '/', '95', 'న', '##ాడు', 'జరిగింది', ',', 'Tada', '##ల...\n",
      "  mbert_token_classes: ['B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'B-GIVENNAME', 'I-GIVENNAME', 'I-GIVENNAME', 'I-GIVENNAME', 'B-SURNAME', 'I-SURNAME', 'I-SURNAME', 'I-SURNAME', 'I-SURNAME', 'O', 'O', 'O', 'O', 'O',...\n"
     ]
    }
   ],
   "source": [
    "# Display a single example to understand the data format\n",
    "print(\"Sample record from training set:\")\n",
    "sample_record: dict[str, Any] = raw_dataset[\"train\"][0]\n",
    "for key, value in sample_record.items():\n",
    "    # Truncate long values for readability\n",
    "    display_value = str(value)\n",
    "    if len(display_value) > 200:\n",
    "        display_value = display_value[:200] + \"...\"\n",
    "    print(f\"  {key}: {display_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c590f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language distribution in training set:\n",
      "  Unique languages: ['de', 'en', 'es', 'fr', 'hi', 'it', 'nl', 'te']\n"
     ]
    }
   ],
   "source": [
    "# Show the distribution of languages in the training set\n",
    "print(\"\\nLanguage distribution in training set:\")\n",
    "languages: list[str] = raw_dataset[\"train\"][\"language\"]\n",
    "unique_languages: set[str] = set(languages)\n",
    "print(f\"  Unique languages: {sorted(unique_languages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede33abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target language identifiers we consider as \"English\"\n",
    "ENGLISH_IDENTIFIERS: frozenset[str] = frozenset({\"en\", \"english\", \"English\"})\n",
    "\n",
    "\n",
    "def is_english(example: dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Determine if a dataset example is in English.\n",
    "    \n",
    "    Args:\n",
    "        example: A single record from the dataset containing a 'language' field.\n",
    "        \n",
    "    Returns:\n",
    "        True if the example's language is identified as English, False otherwise.\n",
    "    \"\"\"\n",
    "    language: str | None = example.get(\"language\")\n",
    "    if language is None:\n",
    "        return False\n",
    "    return language in ENGLISH_IDENTIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "887fd1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering train split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaba4f148e3a43c9b184990caf3bd2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering train for English:   0%|          | 0/464150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original: 464,150 → Filtered: 120,533 (26.0% retained)\n",
      "Filtering validation split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fde9d9c7a441e3934a5a4dc4ac53bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering validation for English:   0%|          | 0/116077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original: 116,077 → Filtered: 30,160 (26.0% retained)\n",
      "\n",
      "Filtering complete.\n"
     ]
    }
   ],
   "source": [
    "# Apply the filter to each split\n",
    "# The filter operation is lazy by default but we use batched=False for row-wise filtering\n",
    "filtered_dataset: DatasetDict = DatasetDict()\n",
    "\n",
    "for split_name in raw_dataset.keys():\n",
    "    print(f\"Filtering {split_name} split...\")\n",
    "    \n",
    "    original_count: int = len(raw_dataset[split_name])\n",
    "    \n",
    "    # Filter the split for English-only examples\n",
    "    filtered_split: Dataset = raw_dataset[split_name].filter(\n",
    "        is_english,\n",
    "        desc=f\"Filtering {split_name} for English\",\n",
    "    )\n",
    "    \n",
    "    filtered_count: int = len(filtered_split)\n",
    "    retained_pct: float = (filtered_count / original_count) * 100 if original_count > 0 else 0.0\n",
    "    \n",
    "    filtered_dataset[split_name] = filtered_split\n",
    "    \n",
    "    print(f\"  Original: {original_count:,} → Filtered: {filtered_count:,} ({retained_pct:.1f}% retained)\")\n",
    "\n",
    "print(\"\\nFiltering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6ebf82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification of filtered dataset:\n",
      "==================================================\n",
      "\n",
      "TRAIN split:\n",
      "  Total examples: 120,533\n",
      "  Languages present: ['en']\n",
      "  ✓ All examples are English\n",
      "\n",
      "VALIDATION split:\n",
      "  Total examples: 30,160\n",
      "  Languages present: ['en']\n",
      "  ✓ All examples are English\n",
      "\n",
      "==================================================\n",
      "Sample record from filtered training set:\n",
      "  Language: en\n",
      "  Source text (truncated): To-do list for 4th August 1942: meet with Brandy Haroon at 10:17 to discuss the volunteer service record of [ORGANISATIONPLACEHOLDER_14]....\n"
     ]
    }
   ],
   "source": [
    "print(\"Verification of filtered dataset:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for split_name, split_data in filtered_dataset.items():\n",
    "    print(f\"\\n{split_name.upper()} split:\")\n",
    "    print(f\"  Total examples: {len(split_data):,}\")\n",
    "    \n",
    "    # Check language distribution (should only contain English)\n",
    "    if len(split_data) > 0:\n",
    "        languages_in_split: list[str] = split_data[\"language\"]\n",
    "        unique_langs: set[str] = set(languages_in_split)\n",
    "        print(f\"  Languages present: {sorted(unique_langs)}\")\n",
    "        \n",
    "        # Sanity check: all should be English\n",
    "        non_english = [lang for lang in unique_langs if lang not in ENGLISH_IDENTIFIERS]\n",
    "        if non_english:\n",
    "            print(f\"  WARNING: Non-English languages found: {non_english}\")\n",
    "        else:\n",
    "            print(\"  ✓ All examples are English\")\n",
    "    else:\n",
    "        print(\"  WARNING: No examples in this split after filtering!\")\n",
    "\n",
    "# Display a sample from the filtered training data\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Sample record from filtered training set:\")\n",
    "if len(filtered_dataset[\"train\"]) > 0:\n",
    "    sample: dict[str, Any] = filtered_dataset[\"train\"][0]\n",
    "    print(f\"  Language: {sample['language']}\")\n",
    "    print(f\"  Source text (truncated): {str(sample.get('source_text', 'N/A'))[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93dbaa3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc5950c5fa6482ba9dda632f63480df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/120533 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcf9e55d223491eb9844e83c9040bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved to: ./data/ai4privacy_english_only\n",
      "To reload later, use:\n",
      "  from datasets import load_from_disk\n",
      "  dataset = load_from_disk(\"./data/ai4privacy_english_only\")\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR: str = \"./data/ai4privacy_english_only\"\n",
    "\n",
    "# Save the filtered dataset to disk in Arrow format (efficient for large datasets)\n",
    "filtered_dataset.save_to_disk(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Filtered dataset saved to: {OUTPUT_DIR}\")\n",
    "print(\"To reload later, use:\")\n",
    "print(f'  from datasets import load_from_disk')\n",
    "print(f'  dataset = load_from_disk(\"{OUTPUT_DIR}\")')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
