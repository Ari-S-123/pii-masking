{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7ac4f1",
   "metadata": {},
   "source": [
    "# Synthetic PII Data Generation\n",
    "\n",
    "Generate 11,000 rows of challenging PII examples targeting the six feature dimensions from Singh & Narayanan (2025)\n",
    "\"Unmasking the Reality of PII Masking Models\".\n",
    "\n",
    "Target dimensions: - basic: Straightforward, well-formatted entities - contextual: Entities requiring disambiguation -\n",
    "noisy: Real-world imperfections (typos, OCR errors, abbreviations) - evolving: New/emerging PII formats (crypto, UPI,\n",
    "modern handles) - multilingual: International PII formats in English text - adversarial: Intentionally confusing inputs\n",
    "designed to fool NER models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7318b07",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43d90e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xai-sdk in c:\\users\\ari\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: openai in c:\\users\\ari\\anaconda3\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ari\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: faker in c:\\users\\ari\\anaconda3\\lib\\site-packages (38.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ari\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pydantic in c:\\users\\ari\\anaconda3\\lib\\site-packages (2.10.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\ari\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: tenacity in c:\\users\\ari\\anaconda3\\lib\\site-packages (9.0.0)\n",
      "Requirement already satisfied: json-repair in c:\\users\\ari\\anaconda3\\lib\\site-packages (0.54.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from xai-sdk) (3.11.10)\n",
      "Requirement already satisfied: grpcio<2,>=1.72.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from xai-sdk) (1.76.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2,>=1.36.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from xai-sdk) (1.39.0)\n",
      "Requirement already satisfied: packaging<26,>=25.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from xai-sdk) (25.0)\n",
      "Requirement already satisfied: protobuf<7,>=5.29.4 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from xai-sdk) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.31.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from xai-sdk) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pydantic) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (1.18.0)\n",
      "Requirement already satisfied: opentelemetry-api==1.39.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from opentelemetry-sdk<2,>=1.36.0->xai-sdk) (1.39.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from opentelemetry-sdk<2,>=1.36.0->xai-sdk) (0.60b0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from opentelemetry-api==1.39.0->opentelemetry-sdk<2,>=1.36.0->xai-sdk) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api==1.39.0->opentelemetry-sdk<2,>=1.36.0->xai-sdk) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from requests<3,>=2.31.0->xai-sdk) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from requests<3,>=2.31.0->xai-sdk) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from requests<3,>=2.31.0->xai-sdk) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from requests<3,>=2.31.0->xai-sdk) (2025.4.26)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ari\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ari\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ari\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xai-sdk openai pandas faker tqdm pydantic python-dotenv tenacity json-repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fa1475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from faker import Faker\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e62f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment loaded successfully\n",
      "  XAI_API_KEY: ********...MXdq\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify required API keys are present\n",
    "XAI_API_KEY: str | None = os.getenv(\"XAI_API_KEY\")\n",
    "if not XAI_API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        \"XAI_API_KEY not found in environment. \"\n",
    "        \"Create a .env file with your xAI API key.\"\n",
    "    )\n",
    "\n",
    "print(\"✓ Environment loaded successfully\")\n",
    "print(f\"  XAI_API_KEY: {'*' * 8}...{XAI_API_KEY[-4:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d929522",
   "metadata": {},
   "source": [
    "## Label Mapping Configuration\n",
    "\n",
    "Label schema harmonization between the paper's 16 PII categories and ai4privacy dataset labels. All synthetic data uses\n",
    "this unified mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24cb5d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configured 16 PII types across 6 dimensions\n",
      "  PII Types: ['NAME', 'EMAIL', 'PHONE', 'DATE_OF_BIRTH', 'POSTAL_CODE', 'CREDIT_CARD', 'BANK_ACCOUNT', 'DRIVER_LICENSE', 'PASSPORT_NUMBER', 'NATIONAL_IDENTITY_SSN_AADHAR', 'OTHER_NATIONAL_IDENTITY', 'TAX_IDENTIFICATION', 'VEHICLE_REGISTRATION', 'INSURANCE_NUMBER', 'BANK_UPI_ID', 'NAMES_OF_PLACES_OR_NOUNS']\n",
      "  Dimensions: ['basic', 'contextual', 'noisy', 'evolving', 'multilingual', 'adversarial']\n",
      "  Locales: ['en_US', 'en_GB', 'en_IN', 'de_DE', 'fr_FR', 'en_AU', 'en_CA', 'it_IT', 'es_ES', 'nl_NL']\n"
     ]
    }
   ],
   "source": [
    "# Maps paper's categories → ai4privacy labels (primary label is first)\n",
    "PAPER_TO_AI4PRIVACY: dict[str, list[str]] = {\n",
    "    \"NAME\": [\"FIRSTNAME\", \"LASTNAME\", \"MIDDLENAME\"],\n",
    "    \"EMAIL\": [\"EMAIL\"],\n",
    "    \"PHONE\": [\"PHONENUMBER\"],\n",
    "    \"DATE_OF_BIRTH\": [\"DOB\"],\n",
    "    \"POSTAL_CODE\": [\"ZIPCODE\"],\n",
    "    \"CREDIT_CARD\": [\"CREDITCARDNUMBER\"],\n",
    "    \"BANK_ACCOUNT\": [\"ACCOUNTNUMBER\", \"IBAN\", \"BIC\"],\n",
    "    \"DRIVER_LICENSE\": [\"DRIVERLICENSE\"],\n",
    "    \"PASSPORT_NUMBER\": [\"PASSPORT\"],\n",
    "    \"NATIONAL_IDENTITY_SSN_AADHAR\": [\"SSN\"],\n",
    "    \"OTHER_NATIONAL_IDENTITY\": [\"NATIONALID\"],\n",
    "    \"TAX_IDENTIFICATION\": [\"TAXID\"],\n",
    "    \"VEHICLE_REGISTRATION\": [\"VEHICLEVRM\", \"VEHICLEVIN\"],\n",
    "    \"INSURANCE_NUMBER\": [\"INSURANCENUMBER\"],\n",
    "    \"BANK_UPI_ID\": [\"UPIID\"],\n",
    "    \"NAMES_OF_PLACES_OR_NOUNS\": [\"CITY\", \"STATE\", \"COUNTY\", \"STREET\"],\n",
    "}\n",
    "\n",
    "# Inverse mapping for evaluation (ai4privacy → paper categories)\n",
    "AI4PRIVACY_TO_PAPER: dict[str, str] = {\n",
    "    v: k for k, vs in PAPER_TO_AI4PRIVACY.items() for v in vs\n",
    "}\n",
    "\n",
    "# All base labels for synthetic generation (we use paper categories)\n",
    "ALL_PII_TYPES: list[str] = list(PAPER_TO_AI4PRIVACY.keys())\n",
    "\n",
    "# Feature dimensions from Singh & Narayanan (2025)\n",
    "FEATURE_DIMENSIONS: list[str] = [\n",
    "    \"basic\",\n",
    "    \"contextual\",\n",
    "    \"noisy\",\n",
    "    \"evolving\",\n",
    "    \"multilingual\",\n",
    "    \"adversarial\",\n",
    "]\n",
    "\n",
    "# Locales for international PII formats (all in English text context)\n",
    "SUPPORTED_LOCALES: dict[str, str] = {\n",
    "    \"en_US\": \"United States\",\n",
    "    \"en_GB\": \"United Kingdom\",\n",
    "    \"en_IN\": \"India\",\n",
    "    \"de_DE\": \"Germany\",\n",
    "    \"fr_FR\": \"France\",\n",
    "    \"en_AU\": \"Australia\",\n",
    "    \"en_CA\": \"Canada\",\n",
    "    \"it_IT\": \"Italy\",\n",
    "    \"es_ES\": \"Spain\",\n",
    "    \"nl_NL\": \"Netherlands\",\n",
    "}\n",
    "\n",
    "print(f\"✓ Configured {len(ALL_PII_TYPES)} PII types across {len(FEATURE_DIMENSIONS)} dimensions\")\n",
    "print(f\"  PII Types: {ALL_PII_TYPES}\")\n",
    "print(f\"  Dimensions: {FEATURE_DIMENSIONS}\")\n",
    "print(f\"  Locales: {list(SUPPORTED_LOCALES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a674a",
   "metadata": {},
   "source": [
    "## Pydantic Schemas for Synthetic Output\n",
    "\n",
    "Pydantic schemas defining the structure of synthetic PII samples. All generated data must conform to these schemas for\n",
    "validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34544ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pydantic schemas defined\n",
      "  SyntheticSample fields: ['text', 'entities', 'feature_dimension', 'seed_pii_type', 'seed_pii_value', 'seed_pii_locale', 'scenario', 'type_variant', 'generation_id', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "class FeatureDimension(str, Enum):\n",
    "    \"\"\"\n",
    "    The six NER failure mode dimensions from Singh & Narayanan (2025).\n",
    "    \n",
    "    Each dimension represents a specific type of challenge for PII detection:\n",
    "        - basic: Standard, well-formatted entities with clear boundaries\n",
    "        - contextual: Ambiguous entities requiring surrounding context\n",
    "        - noisy: Real-world text imperfections and formatting variations\n",
    "        - evolving: Modern/emerging PII formats not in traditional training data\n",
    "        - multilingual: International formats embedded in English prose\n",
    "        - adversarial: Intentionally deceptive patterns designed to evade detection\n",
    "    \"\"\"\n",
    "    BASIC = \"basic\"\n",
    "    CONTEXTUAL = \"contextual\"\n",
    "    NOISY = \"noisy\"\n",
    "    EVOLVING = \"evolving\"\n",
    "    MULTILINGUAL = \"multilingual\"\n",
    "    ADVERSARIAL = \"adversarial\"\n",
    "\n",
    "\n",
    "class EntitySpan(BaseModel):\n",
    "    \"\"\"\n",
    "    A single PII entity annotation with character-level span positions.\n",
    "    \n",
    "    Attributes:\n",
    "        start: Starting character index (0-based, inclusive)\n",
    "        end: Ending character index (exclusive, like Python slicing)\n",
    "        label: PII type label from the unified taxonomy\n",
    "        text: The actual text content of the entity (for verification)\n",
    "    \"\"\"\n",
    "    start: int = Field(..., ge=0, description=\"Start character index (inclusive)\")\n",
    "    end: int = Field(..., gt=0, description=\"End character index (exclusive)\")\n",
    "    label: str = Field(..., description=\"PII type label\")\n",
    "    text: str = Field(..., min_length=1, description=\"Entity text content\")\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_span_bounds(self) -> \"EntitySpan\":\n",
    "        \"\"\"Ensure start < end for valid span.\"\"\"\n",
    "        if self.start >= self.end:\n",
    "            raise ValueError(f\"Invalid span: start ({self.start}) must be < end ({self.end})\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class SyntheticSample(BaseModel):\n",
    "    \"\"\"\n",
    "    A complete synthetic PII training sample with text and annotations.\n",
    "    \n",
    "    This schema captures everything needed for training and validation:\n",
    "    the generated text, all entity annotations, metadata about the\n",
    "    generation process, and the feature dimension being targeted.\n",
    "    \n",
    "    Attributes:\n",
    "        text: The generated English text containing PII entities\n",
    "        entities: List of all PII entity annotations with spans\n",
    "        feature_dimension: Which NER challenge dimension this targets\n",
    "        seed_pii_type: The primary PII type used to seed generation\n",
    "        seed_pii_value: The actual PII value that was seeded\n",
    "        seed_pii_locale: Locale/region for international formats\n",
    "        scenario: Brief description of the text scenario/context\n",
    "        type_variant: Specific variant or sub-type of the PII\n",
    "        generation_id: Unique identifier for this generation attempt\n",
    "        timestamp: When this sample was generated\n",
    "    \"\"\"\n",
    "    text: str = Field(..., min_length=50, max_length=600, description=\"Generated text\")\n",
    "    entities: list[EntitySpan] = Field(..., min_length=1, description=\"Entity annotations\")\n",
    "    feature_dimension: FeatureDimension = Field(..., description=\"Target dimension\")\n",
    "    seed_pii_type: str = Field(..., description=\"Primary PII type\")\n",
    "    seed_pii_value: str = Field(..., description=\"Seeded PII value\")\n",
    "    seed_pii_locale: str | None = Field(None, description=\"Locale for international formats\")\n",
    "    scenario: str = Field(..., description=\"Text scenario description\")\n",
    "    type_variant: str = Field(..., description=\"PII format variant\")\n",
    "    generation_id: str = Field(..., description=\"Unique generation ID\")\n",
    "    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_entities_in_text(self) -> \"SyntheticSample\":\n",
    "        \"\"\"Verify all entity spans are valid within the text.\"\"\"\n",
    "        text_len = len(self.text)\n",
    "        for entity in self.entities:\n",
    "            if entity.end > text_len:\n",
    "                raise ValueError(\n",
    "                    f\"Entity span [{entity.start}:{entity.end}] exceeds \"\n",
    "                    f\"text length {text_len}\"\n",
    "                )\n",
    "            actual_text = self.text[entity.start:entity.end]\n",
    "            if actual_text != entity.text:\n",
    "                raise ValueError(\n",
    "                    f\"Entity text mismatch at [{entity.start}:{entity.end}]: \"\n",
    "                    f\"expected '{entity.text}', found '{actual_text}'\"\n",
    "                )\n",
    "        return self\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_seed_pii_present(self) -> \"SyntheticSample\":\n",
    "        \"\"\"Ensure the seed PII value appears in the text.\"\"\"\n",
    "        if self.seed_pii_value not in self.text:\n",
    "            raise ValueError(\n",
    "                f\"Seed PII value '{self.seed_pii_value}' not found in generated text\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "\n",
    "class GenerationBatch(BaseModel):\n",
    "    \"\"\"\n",
    "    A batch of synthetic samples with generation metadata.\n",
    "    \n",
    "    Attributes:\n",
    "        samples: List of generated samples in this batch\n",
    "        dimension: The feature dimension for all samples in batch\n",
    "        batch_id: Unique batch identifier\n",
    "        total_requested: How many samples were requested\n",
    "        successful: How many were successfully generated\n",
    "        failed: How many failed generation/validation\n",
    "    \"\"\"\n",
    "    samples: list[SyntheticSample] = Field(default_factory=list)\n",
    "    dimension: FeatureDimension\n",
    "    batch_id: str\n",
    "    total_requested: int\n",
    "    successful: int = 0\n",
    "    failed: int = 0\n",
    "\n",
    "\n",
    "print(\"✓ Pydantic schemas defined\")\n",
    "print(f\"  SyntheticSample fields: {list(SyntheticSample.model_fields.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4d2ff",
   "metadata": {},
   "source": [
    "## Faker-Based PII Value Generators\n",
    "\n",
    "Realistic PII value generation using Faker with locale support. These generators create seed PII values that will be\n",
    "embedded in synthetic text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1634fb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PIIGenerator initialized\n",
      "\n",
      "Sample generated PII values:\n",
      "  INSURANCE_NUMBER: INS-4625198781 (en_US)\n",
      "  CREDIT_CARD: 501883698711 (en_US)\n",
      "  NATIONAL_IDENTITY_SSN_AADHAR: 439-62-9242 (en_US)\n",
      "  BANK_ACCOUNT: GB97QJCW22811264702483 (en_US)\n",
      "  DATE_OF_BIRTH: 05/31/1999 (en_US)\n"
     ]
    }
   ],
   "source": [
    "class PIIGenerator:\n",
    "    \"\"\"\n",
    "    Generates realistic PII values across multiple locales using Faker.\n",
    "    \n",
    "    This class provides methods to generate each of the 16 PII types with\n",
    "    proper formatting for different countries/regions. All generated PII\n",
    "    is synthetic and safe for training data.\n",
    "    \n",
    "    Attributes:\n",
    "        fakers: Dictionary mapping locale codes to Faker instances\n",
    "        default_locale: Fallback locale when requested locale unavailable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, locales: list[str] | None = None):\n",
    "        \"\"\"\n",
    "        Initialize PII generators for specified locales.\n",
    "        \n",
    "        Args:\n",
    "            locales: List of locale codes (e.g., ['en_US', 'en_GB', 'de_DE']).\n",
    "                     Defaults to all supported locales if not specified.\n",
    "        \"\"\"\n",
    "        self.locales = locales or list(SUPPORTED_LOCALES.keys())\n",
    "        self.fakers: dict[str, Faker] = {}\n",
    "        self.default_locale = \"en_US\"\n",
    "        \n",
    "        for locale in self.locales:\n",
    "            try:\n",
    "                self.fakers[locale] = Faker(locale)\n",
    "                # Seed for reproducibility within session\n",
    "                self.fakers[locale].seed_instance(random.randint(0, 10000))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not initialize Faker for {locale}: {e}\")\n",
    "        \n",
    "        if not self.fakers:\n",
    "            raise RuntimeError(\"No Faker instances could be initialized\")\n",
    "    \n",
    "    def _get_faker(self, locale: str | None = None) -> tuple[Faker, str]:\n",
    "        \"\"\"Get Faker instance for locale, falling back to default.\"\"\"\n",
    "        if locale and locale in self.fakers:\n",
    "            return self.fakers[locale], locale\n",
    "        return self.fakers[self.default_locale], self.default_locale\n",
    "    \n",
    "    def generate_name(\n",
    "        self, \n",
    "        locale: str | None = None,\n",
    "        name_type: str = \"full\",\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Generate a realistic person name.\n",
    "        \n",
    "        Args:\n",
    "            locale: Target locale for name generation\n",
    "            name_type: One of 'full', 'first', 'last', 'middle'\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (name_value, actual_locale, label) where label is\n",
    "            FIRSTNAME, LASTNAME, MIDDLENAME, or NAME for full names.\n",
    "        \"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        if name_type == \"first\":\n",
    "            return faker.first_name(), actual_locale, \"FIRSTNAME\"\n",
    "        elif name_type == \"last\":\n",
    "            return faker.last_name(), actual_locale, \"LASTNAME\"\n",
    "        elif name_type == \"middle\":\n",
    "            return faker.first_name(), actual_locale, \"MIDDLENAME\"\n",
    "        else:\n",
    "            return faker.name(), actual_locale, \"NAME\"\n",
    "    \n",
    "    def generate_email(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a realistic email address.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        return faker.email(), actual_locale, \"EMAIL\"\n",
    "    \n",
    "    def generate_phone(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a phone number in locale-appropriate format.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        return faker.phone_number(), actual_locale, \"PHONE\"\n",
    "    \n",
    "    def generate_dob(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a date of birth with locale-appropriate formatting.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        dob = faker.date_of_birth(minimum_age=18, maximum_age=85)\n",
    "        \n",
    "        # Format varies by locale\n",
    "        if actual_locale in [\"en_US\", \"en_CA\"]:\n",
    "            formatted = dob.strftime(\"%m/%d/%Y\")\n",
    "        elif actual_locale in [\"en_GB\", \"en_AU\", \"en_IN\", \"de_DE\", \"fr_FR\", \"it_IT\", \"es_ES\", \"nl_NL\"]:\n",
    "            formatted = dob.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            formatted = dob.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        return formatted, actual_locale, \"DATE_OF_BIRTH\"\n",
    "    \n",
    "    def generate_postal_code(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a postal/ZIP code for the specified locale.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        return faker.postcode(), actual_locale, \"POSTAL_CODE\"\n",
    "    \n",
    "    def generate_credit_card(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a credit card number (Luhn-valid but fake).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        return faker.credit_card_number(), actual_locale, \"CREDIT_CARD\"\n",
    "    \n",
    "    def generate_bank_account(\n",
    "        self, \n",
    "        locale: str | None = None,\n",
    "        account_type: str = \"iban\",\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Generate a bank account identifier.\n",
    "        \n",
    "        Args:\n",
    "            locale: Target locale\n",
    "            account_type: 'iban' for European, 'account' for numeric, 'bic' for SWIFT\n",
    "        \"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        if account_type == \"iban\" and hasattr(faker, \"iban\"):\n",
    "            return faker.iban(), actual_locale, \"BANK_ACCOUNT\"\n",
    "        elif account_type == \"bic\" and hasattr(faker, \"swift\"):\n",
    "            return faker.swift(), actual_locale, \"BANK_ACCOUNT\"\n",
    "        else:\n",
    "            # Generate account-style number\n",
    "            return faker.bban(), actual_locale, \"BANK_ACCOUNT\"\n",
    "    \n",
    "    def generate_driver_license(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a driver's license number pattern for locale.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        # Different formats by country\n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_letter().upper()}{faker.random_number(digits=7, fix_len=True)}\",\n",
    "            \"en_GB\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_number(digits=6, fix_len=True)}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}99{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}\",\n",
    "            \"en_IN\": lambda: f\"{faker.state_abbr() if hasattr(faker, 'state_abbr') else 'MH'}{faker.random_number(digits=13, fix_len=True)}\",\n",
    "            \"de_DE\": lambda: f\"{faker.random_number(digits=11, fix_len=True)}\",\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, patterns[\"en_US\"])\n",
    "        return generator(), actual_locale, \"DRIVER_LICENSE\"\n",
    "    \n",
    "    def generate_passport(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a passport number pattern.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        # Common patterns: letter(s) + digits\n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_number(digits=9, fix_len=True)}\",\n",
    "            \"en_GB\": lambda: f\"{faker.random_number(digits=9, fix_len=True)}\",\n",
    "            \"en_IN\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_number(digits=7, fix_len=True)}\",\n",
    "            \"de_DE\": lambda: f\"C{faker.random_number(digits=8, fix_len=True)}\",\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, lambda: f\"{faker.random_uppercase_letter()}{faker.random_number(digits=8, fix_len=True)}\")\n",
    "        return generator(), actual_locale, \"PASSPORT_NUMBER\"\n",
    "    \n",
    "    def generate_ssn(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a national identity number (SSN, Aadhaar, NI, etc.).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_number(digits=3, fix_len=True)}-{faker.random_number(digits=2, fix_len=True)}-{faker.random_number(digits=4, fix_len=True)}\",\n",
    "            \"en_GB\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_number(digits=6, fix_len=True)}{faker.random_uppercase_letter()}\",\n",
    "            \"en_IN\": lambda: f\"{faker.random_number(digits=4, fix_len=True)} {faker.random_number(digits=4, fix_len=True)} {faker.random_number(digits=4, fix_len=True)}\",\n",
    "            \"de_DE\": lambda: f\"{faker.random_number(digits=11, fix_len=True)}\",\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, patterns[\"en_US\"])\n",
    "        return generator(), actual_locale, \"NATIONAL_IDENTITY_SSN_AADHAR\"\n",
    "    \n",
    "    def generate_other_national_id(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate other national identity formats (PAN, TFN, etc.).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        patterns = {\n",
    "            \"en_IN\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_number(digits=4, fix_len=True)}{faker.random_uppercase_letter()}\",  # PAN\n",
    "            \"en_AU\": lambda: f\"{faker.random_number(digits=9, fix_len=True)}\",  # TFN\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, lambda: f\"{faker.random_uppercase_letter()}{faker.random_number(digits=8, fix_len=True)}\")\n",
    "        return generator(), actual_locale, \"OTHER_NATIONAL_IDENTITY\"\n",
    "    \n",
    "    def generate_tax_id(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a tax identification number.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_number(digits=2, fix_len=True)}-{faker.random_number(digits=7, fix_len=True)}\",  # EIN\n",
    "            \"de_DE\": lambda: f\"DE{faker.random_number(digits=9, fix_len=True)}\",  # VAT\n",
    "            \"en_GB\": lambda: f\"GB{faker.random_number(digits=9, fix_len=True)}\",  # VAT\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, patterns[\"en_US\"])\n",
    "        return generator(), actual_locale, \"TAX_IDENTIFICATION\"\n",
    "    \n",
    "    def generate_vehicle_registration(\n",
    "        self, \n",
    "        locale: str | None = None,\n",
    "        reg_type: str = \"plate\",\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate vehicle registration (plate number or VIN).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        if reg_type == \"vin\":\n",
    "            # VIN is international 17-character format\n",
    "            chars = string.ascii_uppercase.replace(\"I\", \"\").replace(\"O\", \"\").replace(\"Q\", \"\") + string.digits\n",
    "            vin = \"\".join(random.choices(chars, k=17))\n",
    "            return vin, actual_locale, \"VEHICLE_REGISTRATION\"\n",
    "        \n",
    "        # License plate patterns\n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}-{faker.random_number(digits=4, fix_len=True)}\",\n",
    "            \"en_GB\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_number(digits=2, fix_len=True)} {faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}\",\n",
    "            \"de_DE\": lambda: f\"{faker.city()[:2].upper()}-{faker.random_uppercase_letter()}{faker.random_uppercase_letter()} {faker.random_number(digits=4, fix_len=True)}\",\n",
    "            \"en_IN\": lambda: f\"{faker.state_abbr() if hasattr(faker, 'state_abbr') else 'MH'}{faker.random_number(digits=2, fix_len=True)} {faker.random_uppercase_letter()}{faker.random_uppercase_letter()} {faker.random_number(digits=4, fix_len=True)}\",\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, patterns[\"en_US\"])\n",
    "        return generator(), actual_locale, \"VEHICLE_REGISTRATION\"\n",
    "    \n",
    "    def generate_insurance_number(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate an insurance policy/member number.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        prefixes = [\"INS\", \"POL\", \"MBR\", \"HLT\", \"AUT\"]\n",
    "        prefix = random.choice(prefixes)\n",
    "        number = faker.random_number(digits=10, fix_len=True)\n",
    "        \n",
    "        return f\"{prefix}-{number}\", actual_locale, \"INSURANCE_NUMBER\"\n",
    "    \n",
    "    def generate_upi_id(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate an Indian UPI ID (user@provider format).\"\"\"\n",
    "        faker, _ = self._get_faker(\"en_IN\")  # UPI is India-specific\n",
    "        \n",
    "        providers = [\"okicici\", \"okhdfcbank\", \"oksbi\", \"ybl\", \"paytm\", \"gpay\", \"phonepe\"]\n",
    "        username = faker.user_name().lower()\n",
    "        provider = random.choice(providers)\n",
    "        \n",
    "        return f\"{username}@{provider}\", \"en_IN\", \"BANK_UPI_ID\"\n",
    "    \n",
    "    def generate_place_name(\n",
    "        self, \n",
    "        locale: str | None = None,\n",
    "        place_type: str = \"city\",\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a place name (city, state, street, etc.).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        if place_type == \"city\":\n",
    "            return faker.city(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "        elif place_type == \"state\":\n",
    "            if hasattr(faker, \"state\"):\n",
    "                return faker.state(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "            return faker.city(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "        elif place_type == \"street\":\n",
    "            return faker.street_name(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "        else:\n",
    "            return faker.city(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "    \n",
    "    def generate_pii(\n",
    "        self, \n",
    "        pii_type: str, \n",
    "        locale: str | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Generate PII of the specified type.\n",
    "        \n",
    "        Args:\n",
    "            pii_type: One of the 16 PII types from ALL_PII_TYPES\n",
    "            locale: Target locale for generation\n",
    "            **kwargs: Additional arguments for specific generators\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (pii_value, actual_locale, label)\n",
    "        \"\"\"\n",
    "        generators = {\n",
    "            \"NAME\": self.generate_name,\n",
    "            \"EMAIL\": self.generate_email,\n",
    "            \"PHONE\": self.generate_phone,\n",
    "            \"DATE_OF_BIRTH\": self.generate_dob,\n",
    "            \"POSTAL_CODE\": self.generate_postal_code,\n",
    "            \"CREDIT_CARD\": self.generate_credit_card,\n",
    "            \"BANK_ACCOUNT\": self.generate_bank_account,\n",
    "            \"DRIVER_LICENSE\": self.generate_driver_license,\n",
    "            \"PASSPORT_NUMBER\": self.generate_passport,\n",
    "            \"NATIONAL_IDENTITY_SSN_AADHAR\": self.generate_ssn,\n",
    "            \"OTHER_NATIONAL_IDENTITY\": self.generate_other_national_id,\n",
    "            \"TAX_IDENTIFICATION\": self.generate_tax_id,\n",
    "            \"VEHICLE_REGISTRATION\": self.generate_vehicle_registration,\n",
    "            \"INSURANCE_NUMBER\": self.generate_insurance_number,\n",
    "            \"BANK_UPI_ID\": self.generate_upi_id,\n",
    "            \"NAMES_OF_PLACES_OR_NOUNS\": self.generate_place_name,\n",
    "        }\n",
    "        \n",
    "        if pii_type not in generators:\n",
    "            raise ValueError(f\"Unknown PII type: {pii_type}. Valid types: {list(generators.keys())}\")\n",
    "        \n",
    "        return generators[pii_type](locale=locale, **kwargs)\n",
    "\n",
    "\n",
    "# Initialize global PII generator\n",
    "pii_gen = PIIGenerator()\n",
    "\n",
    "# Test generation\n",
    "print(\"✓ PIIGenerator initialized\")\n",
    "print(\"\\nSample generated PII values:\")\n",
    "for pii_type in random.sample(ALL_PII_TYPES, 5):\n",
    "    value, locale, label = pii_gen.generate_pii(pii_type, locale=\"en_US\")\n",
    "    print(f\"  {pii_type}: {value} ({locale})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eff157",
   "metadata": {},
   "source": [
    "## Dimension-Specific Prompt Templates\n",
    "\n",
    "Prompt templates for each feature dimension, designed to generate challenging PII examples that specifically target NER\n",
    "model failure modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5025476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt templates configured for all 6 dimensions\n",
      "  basic: 527 chars\n",
      "  contextual: 737 chars\n",
      "  noisy: 629 chars\n",
      "  evolving: 669 chars\n",
      "  multilingual: 719 chars\n",
      "  adversarial: 710 chars\n"
     ]
    }
   ],
   "source": [
    "# Base system prompt for all dimensions\n",
    "SYSTEM_PROMPT_BASE: str = \"\"\"You are a synthetic data generator creating training examples for PII (Personally Identifiable Information) detection models.\n",
    "\n",
    "Your task is to generate realistic English text containing the provided PII value, properly annotated with character-level spans.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. The text MUST be natural, coherent English\n",
    "2. The provided PII value MUST appear EXACTLY as given (no modifications)\n",
    "3. You MUST include additional contextually-relevant PII entities beyond the seed\n",
    "4. All entity spans MUST be accurate character positions (0-indexed, exclusive end)\n",
    "5. The scenario MUST feel realistic and plausible\n",
    "\n",
    "OUTPUT FORMAT (strict JSON):\n",
    "{\n",
    "    \"text\": \"The generated text containing PII...\",\n",
    "    \"entities\": [\n",
    "        {\"start\": 0, \"end\": 10, \"label\": \"LABEL\", \"text\": \"exact text\"},\n",
    "        ...\n",
    "    ],\n",
    "    \"scenario\": \"Brief description of the scenario\"\n",
    "}\n",
    "\n",
    "IMPORTANT: \n",
    "- The \"text\" field in each entity MUST exactly match text[start:end]\n",
    "- Include 2-5 PII entities total (including the seed)\n",
    "- Text length should be 100-500 characters\n",
    "- Do NOT use markdown formatting in the text\"\"\"\n",
    "\n",
    "\n",
    "DIMENSION_PROMPTS: dict[str, str] = {\n",
    "    \"basic\": \"\"\"DIMENSION: BASIC\n",
    "Generate straightforward text where the PII is clearly formatted and easily identifiable.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- PII appears in standard, expected formats\n",
    "- Clear contextual cues (e.g., \"Email:\", \"Phone:\", \"SSN:\")\n",
    "- Well-structured sentences\n",
    "- No ambiguity about entity boundaries\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- Contact information in a directory entry\n",
    "- Form data confirmation message\n",
    "- Official document excerpt\n",
    "- Registration confirmation\n",
    "\n",
    "The goal is clean, well-formatted examples that establish baseline performance.\"\"\",\n",
    "\n",
    "    \"contextual\": \"\"\"DIMENSION: CONTEXTUAL\n",
    "Generate text where PII requires context to disambiguate from similar-looking non-PII.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- Potential false positives nearby (e.g., product codes that look like IDs)\n",
    "- Ambiguous strings that could be PII or not depending on context\n",
    "- Names that could be company names, place names, or person names\n",
    "- Numbers that could be IDs, prices, or dates\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- Email discussing both a person named \"Amazon\" and the company Amazon\n",
    "- Text containing both a date \"March 15\" and a person named \"March\"\n",
    "- Discussion mixing product serial numbers with actual SSNs\n",
    "- Street addresses where street names are also person names\n",
    "\n",
    "The goal is examples requiring semantic understanding, not pattern matching.\"\"\",\n",
    "\n",
    "    \"noisy\": \"\"\"DIMENSION: NOISY\n",
    "Generate text with real-world imperfections that challenge NER systems.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- Typos and misspellings in surrounding text (NOT in the PII itself)\n",
    "- OCR-style errors (l/1, O/0 confusion in context)\n",
    "- Inconsistent formatting and spacing\n",
    "- Abbreviations and informal language\n",
    "- Missing punctuation or extra whitespace\n",
    "- SMS/chat-style shortened text\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- Scanned document with OCR artifacts\n",
    "- Hastily typed customer service chat\n",
    "- Social media post with typos\n",
    "- Informal email with abbreviations\n",
    "\n",
    "The PII values themselves should remain accurate - the noise is in the surrounding text.\"\"\",\n",
    "\n",
    "    \"evolving\": \"\"\"DIMENSION: EVOLVING\n",
    "Generate text containing modern/emerging PII formats not in traditional training data.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- Cryptocurrency wallet addresses (Bitcoin, Ethereum, etc.)\n",
    "- UPI IDs (username@provider format)\n",
    "- Modern usernames/handles (@mentions, Discord tags)\n",
    "- Digital payment identifiers\n",
    "- Cloud service identifiers\n",
    "- API keys or tokens (realistic-looking fakes)\n",
    "- Modern two-factor authentication codes\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- Cryptocurrency transaction discussion\n",
    "- Digital payment confirmation\n",
    "- Tech support for modern apps\n",
    "- Social media account setup\n",
    "- Fintech application onboarding\n",
    "\n",
    "The goal is PII types that have emerged in the last 5-10 years.\"\"\",\n",
    "\n",
    "    \"multilingual\": \"\"\"DIMENSION: MULTILINGUAL\n",
    "Generate English text containing PII in international formats from various countries.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- International phone number formats (+44, +91, +49, etc.)\n",
    "- Non-US ID formats (IBAN, UK NI numbers, Indian Aadhaar, German Personalausweis)\n",
    "- International postal codes (UK postcodes, German PLZ, Indian PIN codes)\n",
    "- Date formats from different regions (DD/MM/YYYY vs MM/DD/YYYY)\n",
    "- International vehicle registration formats\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- International business correspondence\n",
    "- Immigration/visa documentation\n",
    "- International banking transaction\n",
    "- Multinational company HR records\n",
    "- Travel booking confirmation\n",
    "\n",
    "Text MUST be in English, but PII formats should be from non-US locales.\"\"\",\n",
    "\n",
    "    \"adversarial\": \"\"\"DIMENSION: ADVERSARIAL\n",
    "Generate text with patterns designed to confuse or evade NER systems.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- Unusual spacing or formatting within PII\n",
    "- PII split across sentence boundaries\n",
    "- Obfuscated but recognizable PII (spaces in SSN: \"123 45 6789\")\n",
    "- PII embedded in code snippets or technical text\n",
    "- Edge cases with unusual but valid formats\n",
    "- Deliberately misleading context\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- PII hidden in debug logs or error messages\n",
    "- Social engineering attempts with formatted PII\n",
    "- Technical documentation with embedded real values\n",
    "- PII in URLs, file paths, or JSON structures\n",
    "- Creatively formatted attempts to evade filters\n",
    "\n",
    "The goal is testing model robustness against evasion attempts.\"\"\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_generation_prompt(\n",
    "    dimension: str,\n",
    "    pii_type: str,\n",
    "    pii_value: str,\n",
    "    locale: str,\n",
    "    type_variant: str = \"standard\",\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Construct the complete prompt for synthetic data generation.\n",
    "    \n",
    "    Args:\n",
    "        dimension: Target feature dimension\n",
    "        pii_type: Type of PII being seeded\n",
    "        pii_value: The actual PII value to embed\n",
    "        locale: Locale/region for the PII\n",
    "        type_variant: Specific variant description\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (system_prompt, user_prompt)\n",
    "    \"\"\"\n",
    "    if dimension not in DIMENSION_PROMPTS:\n",
    "        raise ValueError(f\"Unknown dimension: {dimension}\")\n",
    "    \n",
    "    system_prompt = f\"{SYSTEM_PROMPT_BASE}\\n\\n{DIMENSION_PROMPTS[dimension]}\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Generate a {dimension.upper()} dimension training example.\n",
    "\n",
    "SEED PII:\n",
    "- Type: {pii_type}\n",
    "- Value: {pii_value}\n",
    "- Locale: {locale}\n",
    "- Variant: {type_variant}\n",
    "\n",
    "The provided PII value MUST appear exactly as shown in your generated text.\n",
    "Include 2-4 additional relevant PII entities.\n",
    "Ensure all entity spans are accurate character positions.\n",
    "\n",
    "Generate the JSON output now:\"\"\"\n",
    "    \n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "\n",
    "print(\"✓ Prompt templates configured for all 6 dimensions\")\n",
    "for dim in FEATURE_DIMENSIONS:\n",
    "    print(f\"  {dim}: {len(DIMENSION_PROMPTS[dim])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5814b4",
   "metadata": {},
   "source": [
    "## xAI API Client Wrapper\n",
    "\n",
    "xAI API client wrapper using the official xai_sdk.AsyncClient.\n",
    "\n",
    "This implementation follows the xAI async documentation: https://docs.x.ai/docs/guides/async\n",
    "\n",
    "The SDK is gRPC-based and handles:\n",
    "\n",
    "-   Connection pooling and management\n",
    "-   Authentication via API key\n",
    "-   Automatic retries for transient errors\n",
    "-   Proper timeout handling\n",
    "\n",
    "Rate Limit Strategy:\n",
    "\n",
    "-   xAI allows 480 requests/minute (8 req/sec average)\n",
    "-   We use asyncio.Semaphore to limit concurrent in-flight requests\n",
    "-   Batch processing with semaphore ensures we stay under limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfe29857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Grok API client initialized using official xai_sdk.AsyncClient\n",
      "  Model: grok-4-1-fast-non-reasoning\n",
      "  Max concurrent requests: 40\n",
      "  Min batch interval: 1.0s\n",
      "  Theoretical max throughput: 2400 req/min\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import json_repair\n",
    "from xai_sdk import AsyncClient\n",
    "from xai_sdk.chat import system, user, Response\n",
    "\n",
    "\n",
    "class GrokClient:\n",
    "    \"\"\"\n",
    "    Async client for xAI's Grok API using the official SDK.\n",
    "    \n",
    "    This client wraps xai_sdk.AsyncClient and provides:\n",
    "        - Semaphore-controlled concurrency for rate limit compliance\n",
    "        - Batch processing for high-throughput generation\n",
    "        - JSON response parsing with repair for malformed LLM output\n",
    "        - Consistent error handling across all requests\n",
    "    \n",
    "    The xAI SDK is gRPC-based, which provides better performance and\n",
    "    reliability compared to raw HTTP requests.\n",
    "    \n",
    "    Attributes:\n",
    "        client: The underlying xai_sdk.AsyncClient instance\n",
    "        model: Model identifier (e.g., 'grok-4-1-fast-non-reasoning')\n",
    "        max_concurrent: Maximum simultaneous in-flight requests\n",
    "        min_batch_interval: Minimum seconds between batch starts (for rate limiting)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rate limit: 480 requests/minute = 8 req/sec\n",
    "    # With 20 concurrent requests, we need ~2.5 seconds per batch minimum\n",
    "    # Using 3.0 seconds for safety margin\n",
    "    DEFAULT_BATCH_SIZE: int = 20\n",
    "    DEFAULT_MIN_BATCH_INTERVAL: float = 3.0\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str | None = None,\n",
    "        model: str = \"grok-4-1-fast-non-reasoning\",\n",
    "        max_concurrent: int = DEFAULT_BATCH_SIZE,\n",
    "        min_batch_interval: float = DEFAULT_MIN_BATCH_INTERVAL,\n",
    "        timeout: int = 900,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Grok API client with the official xAI SDK.\n",
    "        \n",
    "        Args:\n",
    "            api_key: xAI API key. If None, uses XAI_API_KEY environment variable.\n",
    "            model: Model to use for generation (default: grok-4-1-fast-non-reasoning).\n",
    "            max_concurrent: Maximum concurrent in-flight requests (default: 20).\n",
    "            min_batch_interval: Minimum seconds between batch starts (default: 3.0).\n",
    "            timeout: Request timeout in seconds (default: 900, the SDK default).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.min_batch_interval = min_batch_interval\n",
    "        \n",
    "        # Initialize the official xAI AsyncClient\n",
    "        # The SDK reads XAI_API_KEY from environment if api_key is None\n",
    "        self.client = AsyncClient(\n",
    "            api_key=api_key or os.getenv(\"XAI_API_KEY\"),\n",
    "            timeout=timeout,\n",
    "        )\n",
    "        \n",
    "        # Semaphore controls maximum concurrent in-flight requests\n",
    "        self._semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        \n",
    "        # Track timing for rate limit compliance\n",
    "        self._last_batch_start: float = 0.0\n",
    "    \n",
    "    async def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Close the client and release resources.\n",
    "        \n",
    "        The xAI SDK AsyncClient manages its own connection lifecycle,\n",
    "        but we provide this method for explicit cleanup if needed.\n",
    "        \"\"\"\n",
    "        # The AsyncClient doesn't have an explicit close method,\n",
    "        # but we reset our state\n",
    "        self._last_batch_start = 0.0\n",
    "    \n",
    "    async def __aenter__(self) -> \"GrokClient\":\n",
    "        \"\"\"Async context manager entry.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n",
    "        \"\"\"Async context manager exit.\"\"\"\n",
    "        await self.close()\n",
    "    \n",
    "    async def _generate_single(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1500,\n",
    "    ) -> dict[str, Any] | None:\n",
    "        \"\"\"\n",
    "        Generate a single response with semaphore-controlled concurrency.\n",
    "        \n",
    "        This method acquires the semaphore before making the request,\n",
    "        ensuring we never exceed max_concurrent simultaneous requests.\n",
    "        Uses the official xAI SDK chat interface.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt: System message with generation instructions.\n",
    "            user_prompt: User message with specific request.\n",
    "            temperature: Sampling temperature (0.0-1.0).\n",
    "            max_tokens: Maximum tokens in the response.\n",
    "        \n",
    "        Returns:\n",
    "            Parsed JSON response dict, or None if generation/parsing fails.\n",
    "        \"\"\"\n",
    "        async with self._semaphore:\n",
    "            try:\n",
    "                # Create a new chat instance with system message\n",
    "                chat = self.client.chat.create(\n",
    "                    model=self.model,\n",
    "                    messages=[system(system_prompt)],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                )\n",
    "                \n",
    "                # Append the user message\n",
    "                chat.append(user(user_prompt))\n",
    "                \n",
    "                # Sample a response (this is the actual API call)\n",
    "                response: Response = await chat.sample()\n",
    "                \n",
    "                # Extract the content from the response\n",
    "                raw_content: str = response.content\n",
    "                \n",
    "                # Parse JSON from the response content\n",
    "                return self._parse_json_response(raw_content)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Log error but don't crash - return None to indicate failure\n",
    "                print(f\"Generation error: {type(e).__name__}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    def _parse_json_response(self, raw_content: str) -> dict[str, Any] | None:\n",
    "        \"\"\"\n",
    "        Parse JSON from LLM response content.\n",
    "        \n",
    "        LLMs often wrap JSON in markdown code blocks or produce slightly\n",
    "        malformed JSON. This method handles common cases and uses json_repair\n",
    "        as a fallback for malformed output.\n",
    "        \n",
    "        Args:\n",
    "            raw_content: Raw string content from the LLM response.\n",
    "        \n",
    "        Returns:\n",
    "            Parsed JSON dict, or None if parsing fails completely.\n",
    "        \"\"\"\n",
    "        if not raw_content:\n",
    "            return None\n",
    "        \n",
    "        json_str = raw_content.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if \"```json\" in json_str:\n",
    "            # Extract content between ```json and ```\n",
    "            parts = json_str.split(\"```json\")\n",
    "            if len(parts) > 1:\n",
    "                json_str = parts[1].split(\"```\")[0]\n",
    "        elif \"```\" in json_str:\n",
    "            # Generic code block\n",
    "            parts = json_str.split(\"```\")\n",
    "            if len(parts) >= 2:\n",
    "                json_str = parts[1]\n",
    "        \n",
    "        json_str = json_str.strip()\n",
    "        \n",
    "        # Attempt standard JSON parsing first\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "        # Fall back to json_repair for malformed JSON\n",
    "        try:\n",
    "            return json_repair.loads(json_str)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    async def generate(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1500,\n",
    "    ) -> dict[str, Any] | None:\n",
    "        \"\"\"\n",
    "        Public interface for single generation.\n",
    "        \n",
    "        For high-throughput scenarios, use generate_batch() instead.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt: System message with generation instructions.\n",
    "            user_prompt: User message with specific request.\n",
    "            temperature: Sampling temperature.\n",
    "            max_tokens: Maximum tokens in response.\n",
    "        \n",
    "        Returns:\n",
    "            Parsed JSON response dict, or None on failure.\n",
    "        \"\"\"\n",
    "        return await self._generate_single(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "    \n",
    "    async def generate_batch(\n",
    "        self,\n",
    "        requests: list[tuple[str, str, float]],\n",
    "        max_tokens: int = 1500,\n",
    "    ) -> list[dict[str, Any] | None]:\n",
    "        \"\"\"\n",
    "        Generate multiple responses concurrently with rate limit compliance.\n",
    "        \n",
    "        This method implements the pattern from the xAI async documentation:\n",
    "        1. Create tasks for all requests\n",
    "        2. Use semaphore to limit concurrent in-flight requests\n",
    "        3. Use asyncio.gather() to execute all tasks\n",
    "        4. Enforce minimum batch interval for rate limiting\n",
    "        \n",
    "        The semaphore ensures that even if you pass 100 requests, only\n",
    "        max_concurrent will be in-flight at any given moment.\n",
    "        \n",
    "        Args:\n",
    "            requests: List of (system_prompt, user_prompt, temperature) tuples.\n",
    "            max_tokens: Maximum tokens per response.\n",
    "        \n",
    "        Returns:\n",
    "            List of results in the same order as input requests.\n",
    "            Each result is either a parsed JSON dict or None on failure.\n",
    "        \"\"\"\n",
    "        # Enforce minimum time since last batch started\n",
    "        now = time.time()\n",
    "        elapsed_since_last_batch = now - self._last_batch_start\n",
    "        if elapsed_since_last_batch < self.min_batch_interval:\n",
    "            wait_time = self.min_batch_interval - elapsed_since_last_batch\n",
    "            await asyncio.sleep(wait_time)\n",
    "        \n",
    "        # Record batch start time\n",
    "        self._last_batch_start = time.time()\n",
    "        \n",
    "        # Create async task for each request\n",
    "        # The semaphore inside _generate_single controls actual concurrency\n",
    "        async def process_request(\n",
    "            system_prompt: str,\n",
    "            user_prompt: str,\n",
    "            temperature: float,\n",
    "        ) -> dict[str, Any] | None:\n",
    "            return await self._generate_single(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "        \n",
    "        # Build task list\n",
    "        tasks = [\n",
    "            process_request(system_prompt, user_prompt, temperature)\n",
    "            for system_prompt, user_prompt, temperature in requests\n",
    "        ]\n",
    "        \n",
    "        # Execute all tasks concurrently\n",
    "        # The semaphore limits how many are actually in-flight simultaneously\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Convert exceptions to None for consistent return type\n",
    "        processed_results: list[dict[str, Any] | None] = []\n",
    "        for result in results:\n",
    "            if isinstance(result, Exception):\n",
    "                print(f\"Task exception: {type(result).__name__}: {result}\")\n",
    "                processed_results.append(None)\n",
    "            else:\n",
    "                processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "\n",
    "\n",
    "# Initialize client using the official SDK\n",
    "grok_client = GrokClient(\n",
    "    api_key=XAI_API_KEY,\n",
    "    model=\"grok-4-1-fast-non-reasoning\",\n",
    "    max_concurrent=40,          # Max 40 in-flight requests at once\n",
    "    min_batch_interval=1.0,     # Wait at least 3 seconds between batches\n",
    "    timeout=900,                # 15 minute timeout (SDK default)\n",
    ")\n",
    "\n",
    "print(\"✓ Grok API client initialized using official xai_sdk.AsyncClient\")\n",
    "print(f\"  Model: {grok_client.model}\")\n",
    "print(f\"  Max concurrent requests: {grok_client.max_concurrent}\")\n",
    "print(f\"  Min batch interval: {grok_client.min_batch_interval}s\")\n",
    "print(f\"  Theoretical max throughput: {60 / grok_client.min_batch_interval * grok_client.max_concurrent:.0f} req/min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f254287",
   "metadata": {},
   "source": [
    "## Sample Generation and Validation Functions\n",
    "\n",
    "Core generation logic: creates individual samples, validates them, and handles retry logic for failed generations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f7726de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generation functions defined\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def find_entity_span(text: str, entity_text: str, label: str) -> EntitySpan | None:\n",
    "    \"\"\"\n",
    "    Find the character span of an entity in text.\n",
    "    \n",
    "    Handles cases where the LLM might provide incorrect spans by\n",
    "    searching for the actual text position.\n",
    "    \n",
    "    Args:\n",
    "        text: The full text to search in\n",
    "        entity_text: The entity string to find\n",
    "        label: The entity label\n",
    "    \n",
    "    Returns:\n",
    "        EntitySpan if found, None otherwise\n",
    "    \"\"\"\n",
    "    start_idx = text.find(entity_text)\n",
    "    if start_idx == -1:\n",
    "        return None\n",
    "    \n",
    "    return EntitySpan(\n",
    "        start=start_idx,\n",
    "        end=start_idx + len(entity_text),\n",
    "        label=label,\n",
    "        text=entity_text,\n",
    "    )\n",
    "\n",
    "\n",
    "def repair_entity_spans(\n",
    "    text: str,\n",
    "    raw_entities: list[dict[str, Any]],\n",
    ") -> list[EntitySpan]:\n",
    "    \"\"\"\n",
    "    Repair and validate entity spans from LLM output.\n",
    "    \n",
    "    LLMs frequently produce incorrect character positions. This function:\n",
    "    1. Validates each span against the actual text\n",
    "    2. Attempts to find correct positions for misaligned entities\n",
    "    3. Filters out entities that cannot be located\n",
    "    \n",
    "    Args:\n",
    "        text: The generated text\n",
    "        raw_entities: List of entity dicts from LLM response\n",
    "    \n",
    "    Returns:\n",
    "        List of validated EntitySpan objects\n",
    "    \"\"\"\n",
    "    repaired: list[EntitySpan] = []\n",
    "    seen_spans: set[tuple[int, int]] = set()  # Avoid duplicates\n",
    "    \n",
    "    for entity_dict in raw_entities:\n",
    "        try:\n",
    "            start = entity_dict.get(\"start\", 0)\n",
    "            end = entity_dict.get(\"end\", 0)\n",
    "            label = entity_dict.get(\"label\", \"UNKNOWN\")\n",
    "            entity_text = entity_dict.get(\"text\", \"\")\n",
    "            \n",
    "            # First, check if provided span is correct\n",
    "            if 0 <= start < end <= len(text):\n",
    "                actual_text = text[start:end]\n",
    "                if actual_text == entity_text:\n",
    "                    # Span is correct\n",
    "                    span_key = (start, end)\n",
    "                    if span_key not in seen_spans:\n",
    "                        repaired.append(EntitySpan(\n",
    "                            start=start,\n",
    "                            end=end,\n",
    "                            label=label,\n",
    "                            text=entity_text,\n",
    "                        ))\n",
    "                        seen_spans.add(span_key)\n",
    "                    continue\n",
    "            \n",
    "            # Span is incorrect, try to find the text\n",
    "            if entity_text:\n",
    "                found_span = find_entity_span(text, entity_text, label)\n",
    "                if found_span:\n",
    "                    span_key = (found_span.start, found_span.end)\n",
    "                    if span_key not in seen_spans:\n",
    "                        repaired.append(found_span)\n",
    "                        seen_spans.add(span_key)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Skip malformed entities\n",
    "            continue\n",
    "    \n",
    "    return repaired\n",
    "\n",
    "\n",
    "async def generate_single_sample(\n",
    "    client: GrokClient,\n",
    "    dimension: str,\n",
    "    pii_type: str,\n",
    "    locale: str,\n",
    "    generation_id: str,\n",
    "    max_attempts: int = 3,\n",
    ") -> SyntheticSample | None:\n",
    "    \"\"\"\n",
    "    Generate a single synthetic sample with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        client: Initialized GrokClient\n",
    "        dimension: Target feature dimension\n",
    "        pii_type: Type of PII to generate\n",
    "        locale: Locale for PII formatting\n",
    "        generation_id: Unique ID for this generation\n",
    "        max_attempts: Max generation attempts before giving up\n",
    "    \n",
    "    Returns:\n",
    "        Validated SyntheticSample, or None if generation fails\n",
    "    \"\"\"\n",
    "    # Generate seed PII value\n",
    "    pii_value, actual_locale, label = pii_gen.generate_pii(pii_type, locale=locale)\n",
    "    \n",
    "    # Determine type variant based on PII type\n",
    "    type_variants = {\n",
    "        \"NAME\": [\"full name\", \"first name only\", \"formal with title\"],\n",
    "        \"EMAIL\": [\"personal\", \"work\", \"academic\"],\n",
    "        \"PHONE\": [\"mobile\", \"landline\", \"with extension\"],\n",
    "        \"CREDIT_CARD\": [\"Visa\", \"Mastercard\", \"Amex\"],\n",
    "        \"BANK_ACCOUNT\": [\"IBAN\", \"domestic\", \"SWIFT/BIC\"],\n",
    "    }\n",
    "    type_variant = random.choice(type_variants.get(pii_type, [\"standard\"]))\n",
    "    \n",
    "    # Get prompts\n",
    "    system_prompt, user_prompt = get_generation_prompt(\n",
    "        dimension=dimension,\n",
    "        pii_type=pii_type,\n",
    "        pii_value=pii_value,\n",
    "        locale=actual_locale,\n",
    "        type_variant=type_variant,\n",
    "    )\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Generate from LLM\n",
    "            response = await client.generate(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt,\n",
    "                temperature=0.7 + (attempt * 0.1),  # Increase temp on retries\n",
    "            )\n",
    "            \n",
    "            if response is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract fields\n",
    "            text = response.get(\"text\", \"\")\n",
    "            raw_entities = response.get(\"entities\", [])\n",
    "            scenario = response.get(\"scenario\", \"Unspecified scenario\")\n",
    "            \n",
    "            if not text or not raw_entities:\n",
    "                continue\n",
    "            \n",
    "            # Verify seed PII is in text\n",
    "            if pii_value not in text:\n",
    "                # Try to find a close match (case-insensitive)\n",
    "                if pii_value.lower() not in text.lower():\n",
    "                    continue\n",
    "            \n",
    "            # Repair entity spans\n",
    "            entities = repair_entity_spans(text, raw_entities)\n",
    "            \n",
    "            if not entities:\n",
    "                continue\n",
    "            \n",
    "            # Create and validate sample\n",
    "            sample = SyntheticSample(\n",
    "                text=text,\n",
    "                entities=entities,\n",
    "                feature_dimension=FeatureDimension(dimension),\n",
    "                seed_pii_type=pii_type,\n",
    "                seed_pii_value=pii_value,\n",
    "                seed_pii_locale=actual_locale,\n",
    "                scenario=scenario,\n",
    "                type_variant=type_variant,\n",
    "                generation_id=generation_id,\n",
    "            )\n",
    "            \n",
    "            return sample\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_attempts - 1:\n",
    "                await asyncio.sleep(1)  # Brief pause before retry\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"✓ Generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd0e1b",
   "metadata": {},
   "source": [
    "## Batch Generation with Checkpointing\n",
    "\n",
    "Batch generation orchestration with concurrent processing, progress tracking, checkpointing, and balanced sampling\n",
    "across dimensions and PII types.\n",
    "\n",
    "This processes samples in batches of 20 concurrent requests, dramatically improving throughput compared to sequential\n",
    "processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a23b3089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SyntheticDataGenerator initialized with concurrent batch processing\n",
      "  Target: 11000 samples\n",
      "  Batch size: 40 concurrent requests\n",
      "  Per dimension: 1833 samples\n",
      "  Checkpoint every: 100 samples\n",
      "  Output directory: ./data/synthetic\n",
      "  Estimated completion time: ~16 minutes\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for synthetic data generation run.\n",
    "    \n",
    "    Attributes:\n",
    "        total_samples: Total number of samples to generate\n",
    "        samples_per_dimension: Samples per feature dimension (auto-calculated if 0)\n",
    "        batch_size: Number of concurrent requests per batch\n",
    "        samples_per_checkpoint: How often to save checkpoints\n",
    "        output_dir: Directory for output files and checkpoints\n",
    "        checkpoint_prefix: Prefix for checkpoint filenames\n",
    "    \"\"\"\n",
    "    total_samples: int = 11000\n",
    "    samples_per_dimension: int = 0  # 0 = auto-calculate\n",
    "    batch_size: int = 20  # Concurrent requests per batch\n",
    "    samples_per_checkpoint: int = 100\n",
    "    output_dir: str = \"./data/synthetic\"\n",
    "    checkpoint_prefix: str = \"synthetic_checkpoint\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.samples_per_dimension == 0:\n",
    "            self.samples_per_dimension = self.total_samples // len(FEATURE_DIMENSIONS)\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class GenerationTask:\n",
    "    \"\"\"\n",
    "    A single generation task with all parameters needed.\n",
    "    \n",
    "    Attributes:\n",
    "        dimension: Target feature dimension\n",
    "        pii_type: Type of PII to generate\n",
    "        locale: Locale for PII formatting\n",
    "        generation_id: Unique identifier for this task\n",
    "        pii_value: Pre-generated seed PII value\n",
    "        actual_locale: Actual locale used (may differ if fallback)\n",
    "        label: PII label for the seed value\n",
    "        type_variant: Specific variant description\n",
    "        system_prompt: Complete system prompt\n",
    "        user_prompt: Complete user prompt\n",
    "    \"\"\"\n",
    "    dimension: str\n",
    "    pii_type: str\n",
    "    locale: str\n",
    "    generation_id: str\n",
    "    pii_value: str\n",
    "    actual_locale: str\n",
    "    label: str\n",
    "    type_variant: str\n",
    "    system_prompt: str\n",
    "    user_prompt: str\n",
    "\n",
    "\n",
    "class SyntheticDataGenerator:\n",
    "    \"\"\"\n",
    "    Orchestrates batch generation of synthetic PII data with concurrent processing.\n",
    "    \n",
    "    This generator processes samples in batches, firing multiple concurrent\n",
    "    requests to maximize throughput while respecting API rate limits.\n",
    "    \n",
    "    Features:\n",
    "        - Concurrent batch processing (default: 20 requests at a time)\n",
    "        - Balanced sampling across dimensions, PII types, and locales\n",
    "        - Periodic checkpointing to prevent data loss\n",
    "        - Progress tracking with accurate ETA estimation\n",
    "        - Automatic retry for failed generations within batches\n",
    "    \n",
    "    Attributes:\n",
    "        client: GrokClient for LLM generation\n",
    "        config: GenerationConfig with settings\n",
    "        generated_samples: List of all successfully generated samples\n",
    "        stats: Dictionary tracking generation statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client: GrokClient,\n",
    "        config: GenerationConfig | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the synthetic data generator.\n",
    "        \n",
    "        Args:\n",
    "            client: Initialized GrokClient with concurrency settings\n",
    "            config: Generation configuration (uses defaults if None)\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.config = config or GenerationConfig()\n",
    "        self.generated_samples: list[SyntheticSample] = []\n",
    "        self.failed_tasks: list[GenerationTask] = []  # Track failures for potential retry\n",
    "        self.stats: dict[str, Any] = {\n",
    "            \"total_attempts\": 0,\n",
    "            \"successful\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"by_dimension\": defaultdict(int),\n",
    "            \"by_pii_type\": defaultdict(int),\n",
    "            \"by_locale\": defaultdict(int),\n",
    "            \"batches_processed\": 0,\n",
    "            \"start_time\": None,\n",
    "        }\n",
    "    \n",
    "    def _create_generation_task(\n",
    "        self,\n",
    "        dimension: str,\n",
    "        pii_type: str,\n",
    "        locale: str,\n",
    "    ) -> GenerationTask:\n",
    "        \"\"\"\n",
    "        Create a complete generation task with pre-generated PII and prompts.\n",
    "        \n",
    "        This front-loads all the work that doesn't require API calls,\n",
    "        so batch processing only involves the actual LLM requests.\n",
    "        \n",
    "        Args:\n",
    "            dimension: Target feature dimension\n",
    "            pii_type: Type of PII to generate\n",
    "            locale: Target locale for PII formatting\n",
    "        \n",
    "        Returns:\n",
    "            GenerationTask with all fields populated\n",
    "        \"\"\"\n",
    "        generation_id = f\"{dimension}_{pii_type}_{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "        # Generate seed PII value\n",
    "        pii_value, actual_locale, label = pii_gen.generate_pii(pii_type, locale=locale)\n",
    "        \n",
    "        # Determine type variant\n",
    "        type_variants = {\n",
    "            \"NAME\": [\"full name\", \"first name only\", \"formal with title\"],\n",
    "            \"EMAIL\": [\"personal\", \"work\", \"academic\"],\n",
    "            \"PHONE\": [\"mobile\", \"landline\", \"with extension\"],\n",
    "            \"CREDIT_CARD\": [\"Visa\", \"Mastercard\", \"Amex\"],\n",
    "            \"BANK_ACCOUNT\": [\"IBAN\", \"domestic\", \"SWIFT/BIC\"],\n",
    "        }\n",
    "        type_variant = random.choice(type_variants.get(pii_type, [\"standard\"]))\n",
    "        \n",
    "        # Build prompts\n",
    "        system_prompt, user_prompt = get_generation_prompt(\n",
    "            dimension=dimension,\n",
    "            pii_type=pii_type,\n",
    "            pii_value=pii_value,\n",
    "            locale=actual_locale,\n",
    "            type_variant=type_variant,\n",
    "        )\n",
    "        \n",
    "        return GenerationTask(\n",
    "            dimension=dimension,\n",
    "            pii_type=pii_type,\n",
    "            locale=locale,\n",
    "            generation_id=generation_id,\n",
    "            pii_value=pii_value,\n",
    "            actual_locale=actual_locale,\n",
    "            label=label,\n",
    "            type_variant=type_variant,\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "        )\n",
    "    \n",
    "    def _get_generation_plan(self) -> list[tuple[str, str, str]]:\n",
    "        \"\"\"\n",
    "        Create a balanced generation plan across dimensions, PII types, and locales.\n",
    "        \n",
    "        Returns:\n",
    "            List of (dimension, pii_type, locale) tuples representing generation tasks\n",
    "        \"\"\"\n",
    "        plan: list[tuple[str, str, str]] = []\n",
    "        locales = list(SUPPORTED_LOCALES.keys())\n",
    "        \n",
    "        samples_per_dim = self.config.samples_per_dimension\n",
    "        samples_per_type_per_dim = max(1, samples_per_dim // len(ALL_PII_TYPES))\n",
    "        \n",
    "        for dimension in FEATURE_DIMENSIONS:\n",
    "            for pii_type in ALL_PII_TYPES:\n",
    "                for _ in range(samples_per_type_per_dim):\n",
    "                    locale = random.choice(locales)\n",
    "                    plan.append((dimension, pii_type, locale))\n",
    "        \n",
    "        # Shuffle to avoid sequential patterns and distribute load\n",
    "        random.shuffle(plan)\n",
    "        \n",
    "        return plan[:self.config.total_samples]\n",
    "    \n",
    "    def _process_response(\n",
    "        self,\n",
    "        task: GenerationTask,\n",
    "        response: dict[str, Any] | None,\n",
    "    ) -> SyntheticSample | None:\n",
    "        \"\"\"\n",
    "        Process an LLM response into a validated SyntheticSample.\n",
    "        \n",
    "        Args:\n",
    "            task: The generation task that produced this response\n",
    "            response: Parsed JSON response from the LLM, or None on failure\n",
    "        \n",
    "        Returns:\n",
    "            Validated SyntheticSample, or None if validation fails\n",
    "        \"\"\"\n",
    "        if response is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            text = response.get(\"text\", \"\")\n",
    "            raw_entities = response.get(\"entities\", [])\n",
    "            scenario = response.get(\"scenario\", \"Unspecified scenario\")\n",
    "            \n",
    "            if not text or not raw_entities:\n",
    "                return None\n",
    "            \n",
    "            # Verify seed PII is in text\n",
    "            if task.pii_value not in text:\n",
    "                if task.pii_value.lower() not in text.lower():\n",
    "                    return None\n",
    "            \n",
    "            # Repair entity spans (LLMs often get positions wrong)\n",
    "            entities = repair_entity_spans(text, raw_entities)\n",
    "            \n",
    "            if not entities:\n",
    "                return None\n",
    "            \n",
    "            # Create and validate sample using Pydantic\n",
    "            sample = SyntheticSample(\n",
    "                text=text,\n",
    "                entities=entities,\n",
    "                feature_dimension=FeatureDimension(task.dimension),\n",
    "                seed_pii_type=task.pii_type,\n",
    "                seed_pii_value=task.pii_value,\n",
    "                seed_pii_locale=task.actual_locale,\n",
    "                scenario=scenario,\n",
    "                type_variant=task.type_variant,\n",
    "                generation_id=task.generation_id,\n",
    "            )\n",
    "            \n",
    "            return sample\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    async def _process_batch(\n",
    "        self,\n",
    "        tasks: list[GenerationTask],\n",
    "    ) -> list[tuple[GenerationTask, SyntheticSample | None]]:\n",
    "        \"\"\"\n",
    "        Process a batch of generation tasks concurrently.\n",
    "        \n",
    "        This fires all requests in the batch simultaneously (respecting\n",
    "        the client's concurrency limit) and waits for all to complete.\n",
    "        \n",
    "        Args:\n",
    "            tasks: List of GenerationTask objects to process\n",
    "        \n",
    "        Returns:\n",
    "            List of (task, sample_or_none) tuples in the same order\n",
    "        \"\"\"\n",
    "        # Prepare request tuples for the client\n",
    "        requests = [\n",
    "            (task.system_prompt, task.user_prompt, 0.7)\n",
    "            for task in tasks\n",
    "        ]\n",
    "        \n",
    "        # Fire all requests concurrently\n",
    "        responses = await self.client.generate_batch(requests)\n",
    "        \n",
    "        # Process responses into samples\n",
    "        results: list[tuple[GenerationTask, SyntheticSample | None]] = []\n",
    "        for task, response in zip(tasks, responses):\n",
    "            sample = self._process_response(task, response)\n",
    "            results.append((task, sample))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _save_checkpoint(self, checkpoint_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Save current progress to a checkpoint file.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_num: Checkpoint sequence number\n",
    "        \n",
    "        Returns:\n",
    "            Path to saved checkpoint file\n",
    "        \"\"\"\n",
    "        checkpoint_path = (\n",
    "            Path(self.config.output_dir) / \n",
    "            f\"{self.config.checkpoint_prefix}_{checkpoint_num:04d}.json\"\n",
    "        )\n",
    "        \n",
    "        # Convert defaultdicts to regular dicts for JSON serialization\n",
    "        stats_copy = {\n",
    "            k: (dict(v) if isinstance(v, defaultdict) else v)\n",
    "            for k, v in self.stats.items()\n",
    "        }\n",
    "        \n",
    "        checkpoint_data = {\n",
    "            \"checkpoint_num\": checkpoint_num,\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"stats\": stats_copy,\n",
    "            \"samples\": [s.model_dump() for s in self.generated_samples],\n",
    "            \"failed_task_count\": len(self.failed_tasks),\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return str(checkpoint_path)\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: str) -> int:\n",
    "        \"\"\"\n",
    "        Load progress from a checkpoint file.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to checkpoint file\n",
    "        \n",
    "        Returns:\n",
    "            Number of samples loaded\n",
    "        \"\"\"\n",
    "        with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Restore stats with defaultdict behavior\n",
    "        loaded_stats = data.get(\"stats\", {})\n",
    "        self.stats = {\n",
    "            \"total_attempts\": loaded_stats.get(\"total_attempts\", 0),\n",
    "            \"successful\": loaded_stats.get(\"successful\", 0),\n",
    "            \"failed\": loaded_stats.get(\"failed\", 0),\n",
    "            \"by_dimension\": defaultdict(int, loaded_stats.get(\"by_dimension\", {})),\n",
    "            \"by_pii_type\": defaultdict(int, loaded_stats.get(\"by_pii_type\", {})),\n",
    "            \"by_locale\": defaultdict(int, loaded_stats.get(\"by_locale\", {})),\n",
    "            \"batches_processed\": loaded_stats.get(\"batches_processed\", 0),\n",
    "            \"start_time\": loaded_stats.get(\"start_time\"),\n",
    "        }\n",
    "        \n",
    "        self.generated_samples = [\n",
    "            SyntheticSample.model_validate(s) \n",
    "            for s in data.get(\"samples\", [])\n",
    "        ]\n",
    "        \n",
    "        print(f\"✓ Loaded {len(self.generated_samples)} samples from checkpoint\")\n",
    "        return len(self.generated_samples)\n",
    "    \n",
    "    async def generate_batch(\n",
    "        self,\n",
    "        start_from: int = 0,\n",
    "        progress_bar: bool = True,\n",
    "    ) -> list[SyntheticSample]:\n",
    "        \"\"\"\n",
    "        Generate a full batch of synthetic samples using concurrent processing.\n",
    "        \n",
    "        This method processes samples in batches of config.batch_size,\n",
    "        firing concurrent requests and waiting for each batch to complete\n",
    "        before starting the next.\n",
    "        \n",
    "        Args:\n",
    "            start_from: Index to start from (for resuming from checkpoint)\n",
    "            progress_bar: Whether to display a progress bar\n",
    "        \n",
    "        Returns:\n",
    "            List of all successfully generated samples\n",
    "        \"\"\"\n",
    "        # Get the full generation plan\n",
    "        plan = self._get_generation_plan()\n",
    "        \n",
    "        if start_from > 0:\n",
    "            plan = plan[start_from:]\n",
    "            print(f\"Resuming from sample {start_from}\")\n",
    "        \n",
    "        self.stats[\"start_time\"] = time.time()\n",
    "        \n",
    "        # Create all tasks upfront (this is fast, no API calls)\n",
    "        print(\"Preparing generation tasks...\")\n",
    "        all_tasks = [\n",
    "            self._create_generation_task(dimension, pii_type, locale)\n",
    "            for dimension, pii_type, locale in plan\n",
    "        ]\n",
    "        \n",
    "        # Split into batches\n",
    "        batch_size = self.config.batch_size\n",
    "        batches = [\n",
    "            all_tasks[i:i + batch_size] \n",
    "            for i in range(0, len(all_tasks), batch_size)\n",
    "        ]\n",
    "        \n",
    "        total_batches = len(batches)\n",
    "        samples_since_checkpoint = len(self.generated_samples) % self.config.samples_per_checkpoint\n",
    "        \n",
    "        print(f\"Processing {len(all_tasks)} tasks in {total_batches} batches of {batch_size}\")\n",
    "        \n",
    "        # Process batches with progress tracking\n",
    "        batch_iterator = tqdm(\n",
    "            enumerate(batches),\n",
    "            total=total_batches,\n",
    "            desc=\"Processing batches\",\n",
    "            disable=not progress_bar,\n",
    "        )\n",
    "        \n",
    "        for batch_idx, batch_tasks in batch_iterator:\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Process this batch concurrently\n",
    "            results = await self._process_batch(batch_tasks)\n",
    "            \n",
    "            # Update stats and collect samples\n",
    "            for task, sample in results:\n",
    "                self.stats[\"total_attempts\"] += 1\n",
    "                \n",
    "                if sample is not None:\n",
    "                    self.generated_samples.append(sample)\n",
    "                    self.stats[\"successful\"] += 1\n",
    "                    self.stats[\"by_dimension\"][task.dimension] += 1\n",
    "                    self.stats[\"by_pii_type\"][task.pii_type] += 1\n",
    "                    self.stats[\"by_locale\"][task.actual_locale] += 1\n",
    "                    samples_since_checkpoint += 1\n",
    "                else:\n",
    "                    self.stats[\"failed\"] += 1\n",
    "                    self.failed_tasks.append(task)\n",
    "            \n",
    "            self.stats[\"batches_processed\"] += 1\n",
    "            \n",
    "            # Calculate metrics for progress display\n",
    "            batch_elapsed = time.time() - batch_start_time\n",
    "            success_rate = self.stats[\"successful\"] / self.stats[\"total_attempts\"]\n",
    "            samples_per_second = len(batch_tasks) / batch_elapsed if batch_elapsed > 0 else 0\n",
    "            \n",
    "            batch_iterator.set_postfix({\n",
    "                \"success\": f\"{self.stats['successful']}/{self.stats['total_attempts']}\",\n",
    "                \"rate\": f\"{success_rate:.1%}\",\n",
    "                \"speed\": f\"{samples_per_second:.1f}/s\",\n",
    "            })\n",
    "            \n",
    "            # Checkpoint if needed\n",
    "            if samples_since_checkpoint >= self.config.samples_per_checkpoint:\n",
    "                cp_num = len(self.generated_samples) // self.config.samples_per_checkpoint\n",
    "                cp_path = self._save_checkpoint(cp_num)\n",
    "                batch_iterator.write(f\"  💾 Checkpoint {cp_num}: {cp_path}\")\n",
    "                samples_since_checkpoint = 0\n",
    "        \n",
    "        # Final checkpoint\n",
    "        self._save_checkpoint(9999)\n",
    "        \n",
    "        return self.generated_samples\n",
    "    \n",
    "    def get_statistics(self) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get comprehensive generation statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with generation metrics and breakdowns\n",
    "        \"\"\"\n",
    "        elapsed = time.time() - self.stats[\"start_time\"] if self.stats[\"start_time\"] else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_generated\": len(self.generated_samples),\n",
    "            \"total_attempts\": self.stats[\"total_attempts\"],\n",
    "            \"total_failed\": self.stats[\"failed\"],\n",
    "            \"success_rate\": (\n",
    "                self.stats[\"successful\"] / self.stats[\"total_attempts\"]\n",
    "                if self.stats[\"total_attempts\"] > 0 else 0\n",
    "            ),\n",
    "            \"batches_processed\": self.stats[\"batches_processed\"],\n",
    "            \"elapsed_seconds\": elapsed,\n",
    "            \"samples_per_second\": len(self.generated_samples) / elapsed if elapsed > 0 else 0,\n",
    "            \"by_dimension\": dict(self.stats[\"by_dimension\"]),\n",
    "            \"by_pii_type\": dict(self.stats[\"by_pii_type\"]),\n",
    "            \"by_locale\": dict(self.stats[\"by_locale\"]),\n",
    "        }\n",
    "    \n",
    "    def save_final_dataset(self, filename: str = \"synthetic_pii_data.json\") -> str:\n",
    "        \"\"\"\n",
    "        Save the complete dataset to a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            filename: Output filename\n",
    "        \n",
    "        Returns:\n",
    "            Path to saved file\n",
    "        \"\"\"\n",
    "        output_path = Path(self.config.output_dir) / filename\n",
    "        \n",
    "        dataset = {\n",
    "            \"metadata\": {\n",
    "                \"generated_at\": datetime.utcnow().isoformat(),\n",
    "                \"total_samples\": len(self.generated_samples),\n",
    "                \"dimensions\": FEATURE_DIMENSIONS,\n",
    "                \"pii_types\": ALL_PII_TYPES,\n",
    "                \"locales\": list(SUPPORTED_LOCALES.keys()),\n",
    "            },\n",
    "            \"statistics\": self.get_statistics(),\n",
    "            \"samples\": [s.model_dump() for s in self.generated_samples],\n",
    "        }\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"✓ Dataset saved to: {output_path}\")\n",
    "        return str(output_path)\n",
    "\n",
    "\n",
    "# Initialize generator with concurrent processing config\n",
    "config = GenerationConfig(\n",
    "    total_samples=11000,\n",
    "    batch_size=40,  # 40 concurrent requests per batch\n",
    "    samples_per_checkpoint=100,\n",
    "    output_dir=\"./data/synthetic\",\n",
    ")\n",
    "\n",
    "generator = SyntheticDataGenerator(\n",
    "    client=grok_client,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"✓ SyntheticDataGenerator initialized with concurrent batch processing\")\n",
    "print(f\"  Target: {config.total_samples} samples\")\n",
    "print(f\"  Batch size: {config.batch_size} concurrent requests\")\n",
    "print(f\"  Per dimension: {config.samples_per_dimension} samples\")\n",
    "print(f\"  Checkpoint every: {config.samples_per_checkpoint} samples\")\n",
    "print(f\"  Output directory: {config.output_dir}\")\n",
    "\n",
    "# Estimate completion time\n",
    "# With 20 concurrent requests, ~3 second batches, that's ~400 requests/min\n",
    "# 11,000 samples / 400 per min ≈ 27.5 minutes (plus overhead)\n",
    "estimated_batches = config.total_samples // config.batch_size\n",
    "estimated_time_minutes = estimated_batches * 3.5 / 60  # 3.5 sec per batch average\n",
    "print(f\"  Estimated completion time: ~{estimated_time_minutes:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b1023",
   "metadata": {},
   "source": [
    "## Quick Test Generation\n",
    "\n",
    "Quick test: Generate a small batch to verify everything works before running the full 11,000 sample generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd6e41bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test generation with 10 samples...\n",
      "Preparing generation tasks...\n",
      "Processing 10 tasks in 1 batches of 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dc7ffacea1470895a1cf66019fcbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test complete: 10 samples generated\n",
      "\n",
      "First sample:\n",
      "  Dimension: noisy\n",
      "  Text: hey mike  just got ur call abt the new hire docs  SIN is 948-39-8491  full name Johnathan R. Sm1th dob 12/15/1982  phone 416-555-01987  addy 123 Oak S...\n",
      "  Entities: 4\n",
      "    - NATIONAL_IDENTITY_SSN_AADHAR: '948-39-8491' [57:68]\n",
      "    - PERSON_FULL_NAME: 'Johnathan R. Sm1th' [80:98]\n",
      "    - DATE_OF_BIRTH: '12/15/1982' [103:113]\n",
      "    - PHONE_NUMBER: '416-555-01987' [121:134]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ari\\AppData\\Local\\Temp\\ipykernel_25576\\1235122109.py:73: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n",
      "C:\\Users\\Ari\\AppData\\Local\\Temp\\ipykernel_25576\\1920566619.py:298: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "async def test_generation(num_samples: int = 10) -> list[SyntheticSample]:\n",
    "    \"\"\"\n",
    "    Run a quick test generation with a small number of samples.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of test samples to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of generated test samples\n",
    "    \"\"\"\n",
    "    print(f\"Running test generation with {num_samples} samples...\")\n",
    "    \n",
    "    test_config = GenerationConfig(\n",
    "        total_samples=num_samples,\n",
    "        samples_per_checkpoint=num_samples + 1,  # No checkpoints for test\n",
    "        output_dir=\"./data/synthetic_test\",\n",
    "    )\n",
    "    \n",
    "    test_generator = SyntheticDataGenerator(\n",
    "        client=grok_client,\n",
    "        config=test_config,\n",
    "    )\n",
    "    \n",
    "    test_samples = await test_generator.generate_batch(progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nTest complete: {len(test_samples)} samples generated\")\n",
    "    \n",
    "    if test_samples:\n",
    "        print(\"\\nFirst sample:\")\n",
    "        s = test_samples[0]\n",
    "        print(f\"  Dimension: {s.feature_dimension.value}\")\n",
    "        print(f\"  Text: {s.text[:150]}...\")\n",
    "        print(f\"  Entities: {len(s.entities)}\")\n",
    "        for e in s.entities:\n",
    "            print(f\"    - {e.label}: '{e.text}' [{e.start}:{e.end}]\")\n",
    "    \n",
    "    return test_samples\n",
    "\n",
    "\n",
    "# Run quick test (comment out for full generation)\n",
    "test_samples = await test_generation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3e2cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SyntheticSample(text='hey mike  just got ur call abt the new hire docs  SIN is 948-39-8491  full name Johnathan R. Sm1th dob 12/15/1982  phone 416-555-01987  addy 123 Oak St Tor0nto ON  M5V2T6  plz verify asap thx!  cc: hr@compny.ca', entities=[EntitySpan(start=57, end=68, label='NATIONAL_IDENTITY_SSN_AADHAR', text='948-39-8491'), EntitySpan(start=80, end=98, label='PERSON_FULL_NAME', text='Johnathan R. Sm1th'), EntitySpan(start=103, end=113, label='DATE_OF_BIRTH', text='12/15/1982'), EntitySpan(start=121, end=134, label='PHONE_NUMBER', text='416-555-01987')], feature_dimension=<FeatureDimension.NOISY: 'noisy'>, seed_pii_type='NATIONAL_IDENTITY_SSN_AADHAR', seed_pii_value='948-39-8491', seed_pii_locale='en_CA', scenario='Hastily typed customer service chat with typos, OCR errors, and informal abbreviations about verifying new hire documents', type_variant='standard', generation_id='noisy_NATIONAL_IDENTITY_SSN_AADHAR_753e19a4', timestamp='2025-12-03T22:05:48.304903'), SyntheticSample(text='hey jan, kan je even checken die scan van je rijbewijs? driver license nr H1120499 staat erop, maar wat is je geboortedatum 12-05-1987 en adres Pieter de Hoochstr 45 2517JX Den Haag? oh en je BSN was  234567890 ik denk lol, bel asap want DMV belt morgen 071-2345678 !', entities=[EntitySpan(start=74, end=82, label='DRIVER_LICENSE', text='H1120499'), EntitySpan(start=124, end=134, label='DATE_OF_BIRTH', text='12-05-1987'), EntitySpan(start=144, end=165, label='STREET_ADDRESS', text='Pieter de Hoochstr 45'), EntitySpan(start=166, end=172, label='POSTAL_CODE', text='2517JX'), EntitySpan(start=173, end=181, label='CITY', text='Den Haag'), EntitySpan(start=201, end=210, label='BSN', text='234567890'), EntitySpan(start=254, end=265, label='PHONE_NUMBER', text='071-2345678')], feature_dimension=<FeatureDimension.NOISY: 'noisy'>, seed_pii_type='DRIVER_LICENSE', seed_pii_value='H1120499', seed_pii_locale='nl_NL', scenario=\"Hastily typed customer service chat about verifying a scanned Dutch driver's license with OCR artifacts, typos, and informal abbreviations\", type_variant='standard', generation_id='noisy_DRIVER_LICENSE_1f7b5380', timestamp='2025-12-03T22:05:48.304945'), SyntheticSample(text='Dear Mr. Dupont, thank you for your recent inquiry regarding our Paris office. Please contact our landline at 0483491955 for further assistance. Your account details are as follows: IBAN FR7612345689012345678901234, French social security number 1 85 06 123 456 789, and delivery address: 12 Rue de la Paix, 75002 Paris. Best regards, Sophie Martin (email: sophie.martin@entreprise.fr).', entities=[EntitySpan(start=110, end=120, label='PHONE', text='0483491955'), EntitySpan(start=187, end=214, label='IBAN', text='FR7612345689012345678901234'), EntitySpan(start=246, end=265, label='SSN_FR', text='1 85 06 123 456 789'), EntitySpan(start=308, end=313, label='POSTCODE', text='75002')], feature_dimension=<FeatureDimension.MULTILINGUAL: 'multilingual'>, seed_pii_type='PHONE', seed_pii_value='0483491955', seed_pii_locale='fr_FR', scenario='Customer service email from a French company providing contact and account details', type_variant='landline', generation_id='multilingual_PHONE_33b0de64', timestamp='2025-12-03T22:05:48.304967'), SyntheticSample(text=\"Hi support team, I'm having trouble with my recent SEPA transfer from my German bank account DE49668649169087363624 to the recipient IBAN LT12 3456 7890 1234 5678 9012. The transaction ID is tx_7f9a2b1c3d4e5f67890abcde and the UPI reference was netbanking@paytm. Can you check the status? My phone for 2FA is +49 176 12345678. Thanks!\", entities=[EntitySpan(start=93, end=115, label='BANK_ACCOUNT', text='DE49668649169087363624'), EntitySpan(start=138, end=167, label='BANK_ACCOUNT', text='LT12 3456 7890 1234 5678 9012'), EntitySpan(start=191, end=218, label='TXN_ID', text='tx_7f9a2b1c3d4e5f67890abcde'), EntitySpan(start=245, end=261, label='UPI_ID', text='netbanking@paytm'), EntitySpan(start=309, end=325, label='PHONE', text='+49 176 12345678')], feature_dimension=<FeatureDimension.EVOLVING: 'evolving'>, seed_pii_type='BANK_ACCOUNT', seed_pii_value='DE49668649169087363624', seed_pii_locale='de_DE', scenario='Customer contacting bank support about a failed SEPA international transfer with transaction details', type_variant='domestic', generation_id='evolving_BANK_ACCOUNT_c22edf63', timestamp='2025-12-03T22:05:48.304988'), SyntheticSample(text='Hi Marco, regarding your recent transfer from UniCredit Banca, the IBAN IT39F3238327156184526519426 has been debited €1,250.00 on 15/03/2024. Please confirm receipt at your Milan branch, Via Alessandro Manzoni 23, 20121 Milano. Note: our reference code IT39A123456789 looks similar but is just internal. Best, Luca Rossi, +39 02 1234567.', entities=[EntitySpan(start=72, end=99, label='BANK_ACCOUNT', text='IT39F3238327156184526519426'), EntitySpan(start=3, end=8, label='PERSON', text='Marco'), EntitySpan(start=310, end=320, label='PERSON', text='Luca Rossi'), EntitySpan(start=322, end=336, label='PHONE', text='+39 02 1234567'), EntitySpan(start=130, end=140, label='DATE', text='15/03/2024')], feature_dimension=<FeatureDimension.CONTEXTUAL: 'contextual'>, seed_pii_type='BANK_ACCOUNT', seed_pii_value='IT39F3238327156184526519426', seed_pii_locale='it_IT', scenario='Bank email confirming a transaction with customer IBAN, alongside similar-looking internal code, personal details, date, and phone for contextual disambiguation.', type_variant='IBAN', generation_id='contextual_BANK_ACCOUNT_4975f193', timestamp='2025-12-03T22:05:48.305010'), SyntheticSample(text='Hola Carlos, revisando el expediente del cliente, su DNI es S43908416, emitido en Madrid el 15/03/2023. Nota: el código del producto devuelto es S4390841H, que se parece pero no es el mismo. Contacta a María González al teléfono 915 123 456 para confirmar el reembolso. Saludos, Ana.', entities=[EntitySpan(start=60, end=69, label='OTHER_NATIONAL_IDENTITY', text='S43908416'), EntitySpan(start=5, end=11, label='PERSON', text='Carlos'), EntitySpan(start=202, end=216, label='PERSON', text='María González'), EntitySpan(start=229, end=240, label='PHONE', text='915 123 456'), EntitySpan(start=279, end=282, label='PERSON', text='Ana')], feature_dimension=<FeatureDimension.CONTEXTUAL: 'contextual'>, seed_pii_type='OTHER_NATIONAL_IDENTITY', seed_pii_value='S43908416', seed_pii_locale='es_ES', scenario='Internal customer service email in Spain distinguishing a real DNI from a similar-looking product code', type_variant='standard', generation_id='contextual_OTHER_NATIONAL_IDENTITY_ddca9988', timestamp='2025-12-03T22:05:48.305031'), SyntheticSample(text=\"Hey support, I'm setting up my new Coinbase account but verification is stuck. My DOB is 08/23/2007 and full name is Ethan M. Leclerc from 1425 Yonge St, Toronto, ON M4T 1Y7. Wallet address: bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh. UPI for payments: ethanleclerc@paytm. Sent the 2FA code 483920 already, what's next?\", entities=[EntitySpan(start=89, end=99, label='DATE_OF_BIRTH', text='08/23/2007'), EntitySpan(start=139, end=173, label='ADDRESS', text='1425 Yonge St, Toronto, ON M4T 1Y7'), EntitySpan(start=191, end=233, label='CRYPTO_WALLET', text='bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh'), EntitySpan(start=253, end=271, label='UPI_ID', text='ethanleclerc@paytm'), EntitySpan(start=291, end=297, label='TWO_FA_CODE', text='483920')], feature_dimension=<FeatureDimension.EVOLVING: 'evolving'>, seed_pii_type='DATE_OF_BIRTH', seed_pii_value='08/23/2007', seed_pii_locale='en_CA', scenario='User contacting crypto exchange support for account verification, providing DOB and modern digital identifiers', type_variant='standard', generation_id='evolving_DATE_OF_BIRTH_a70d50b5', timestamp='2025-12-03T22:05:48.305049'), SyntheticSample(text=\"Hey @TechSupportGuru, I'm having trouble with my crypto wallet in Christopherburgh. Can't send 0.5 BTC from bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh to my buddy's address bc1qar0srrr7xfkvy5l643lydnw9re59gtzzwf5mdq. My 2FA code is 847392 and UPI is john.doe@paytm for backup payments. Pls help ASAP!\", entities=[EntitySpan(start=66, end=82, label='NAMES_OF_PLACES_OR_NOUNS', text='Christopherburgh'), EntitySpan(start=4, end=20, label='USERNAME', text='@TechSupportGuru'), EntitySpan(start=108, end=150, label='CRYPTO_WALLET', text='bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh'), EntitySpan(start=173, end=215, label='CRYPTO_WALLET', text='bc1qar0srrr7xfkvy5l643lydnw9re59gtzzwf5mdq'), EntitySpan(start=232, end=238, label='TOTP', text='847392'), EntitySpan(start=250, end=264, label='UPI_ID', text='john.doe@paytm')], feature_dimension=<FeatureDimension.EVOLVING: 'evolving'>, seed_pii_type='NAMES_OF_PLACES_OR_NOUNS', seed_pii_value='Christopherburgh', seed_pii_locale='en_US', scenario='User seeking tech support for cryptocurrency wallet transaction issue via social media, mentioning location and digital payment details', type_variant='standard', generation_id='evolving_NAMES_OF_PLACES_OR_NOUNS_1bc9e22b', timestamp='2025-12-03T22:05:48.305069'), SyntheticSample(text='Dear Urvashi Narain,\\n\\nThank you for completing your bank account linkage. Your UPI ID is now active:\\n\\nUPI ID: narainurvashi@phonepe\\nAccount Number: 123456789012\\nIFSC Code: SBIN0001234\\nMobile Number: +91 98765 43210\\n\\nYou can now make payments using PhonePe. For support, contact 1800-123-4567.\\n\\nBest regards,\\nPhonePe Team', entities=[EntitySpan(start=110, end=131, label='BANK_UPI_ID', text='narainurvashi@phonepe'), EntitySpan(start=148, end=160, label='BANK_ACCOUNT_NUMBER', text='123456789012'), EntitySpan(start=172, end=183, label='BANK_IFSC_CODE', text='SBIN0001234'), EntitySpan(start=199, end=214, label='PHONE_NUMBER', text='+91 98765 43210')], feature_dimension=<FeatureDimension.BASIC: 'basic'>, seed_pii_type='BANK_UPI_ID', seed_pii_value='narainurvashi@phonepe', seed_pii_locale='en_IN', scenario='Bank UPI activation confirmation email from PhonePe', type_variant='standard', generation_id='basic_BANK_UPI_ID_e3f030c1', timestamp='2025-12-03T22:05:48.305086'), SyntheticSample(text='Customer Registration Confirmation\\n\\nDear John Smith,\\n\\nThank you for registering with our service. Here are your account details:\\n\\nName: John Smith\\nEmail: john.smith@email.com\\nPhone: 001-492-686-1703x36902\\nAddress: 123 Main St, Springfield, IL 62701\\n\\nPlease save this information for your records.\\n\\nBest regards,\\nCustomer Support', entities=[EntitySpan(start=41, end=51, label='NAME', text='John Smith'), EntitySpan(start=154, end=174, label='EMAIL', text='john.smith@email.com'), EntitySpan(start=182, end=204, label='PHONE', text='001-492-686-1703x36902'), EntitySpan(start=214, end=248, label='ADDRESS', text='123 Main St, Springfield, IL 62701')], feature_dimension=<FeatureDimension.BASIC: 'basic'>, seed_pii_type='PHONE', seed_pii_value='001-492-686-1703x36902', seed_pii_locale='en_US', scenario='Customer registration confirmation email with contact details', type_variant='with extension', generation_id='basic_PHONE_e295a2d5', timestamp='2025-12-03T22:05:48.305102')]\n"
     ]
    }
   ],
   "source": [
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c180d5f0",
   "metadata": {},
   "source": [
    "## Run Generation\n",
    "\n",
    "Main execution cell - runs the full generation process.\n",
    "\n",
    "WARNING: This will make ~11,000+ API calls to xAI. Ensure you have:\n",
    "\n",
    "1. Sufficient API credits\n",
    "2. Stable internet connection\n",
    "3. Time for completion\n",
    "\n",
    "To resume from a checkpoint, uncomment the load_checkpoint line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_generation():\n",
    "    \"\"\"Execute the full synthetic data generation pipeline.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"SYNTHETIC PII DATA GENERATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Start time: {datetime.now().isoformat()}\")\n",
    "    print(f\"Target samples: {generator.config.total_samples}\")\n",
    "    print()\n",
    "    \n",
    "    # Uncomment to resume from checkpoint:\n",
    "    # generator.load_checkpoint(\"./data/synthetic/synthetic_checkpoint_0050.json\")\n",
    "    # start_idx = len(generator.generated_samples)\n",
    "    start_idx = 0\n",
    "    \n",
    "    try:\n",
    "        samples = await generator.generate_batch(\n",
    "            start_from=start_idx,\n",
    "            progress_bar=True,\n",
    "        )\n",
    "        \n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(\"GENERATION COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = generator.get_statistics()\n",
    "        print(f\"\\nTotal generated: {stats['total_generated']}\")\n",
    "        print(f\"Success rate: {stats['success_rate']:.1%}\")\n",
    "        \n",
    "        print(\"\\nBy dimension:\")\n",
    "        for dim, count in stats[\"by_dimension\"].items():\n",
    "            print(f\"  {dim}: {count}\")\n",
    "        \n",
    "        print(\"\\nBy PII type:\")\n",
    "        for pii_type, count in sorted(stats[\"by_pii_type\"].items(), key=lambda x: -x[1])[:10]:\n",
    "            print(f\"  {pii_type}: {count}\")\n",
    "        \n",
    "        # Save final dataset\n",
    "        output_path = generator.save_final_dataset(\"synthetic_pii_data.json\")\n",
    "        \n",
    "        return samples\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nGeneration interrupted! Saving checkpoint...\")\n",
    "        generator._save_checkpoint(9998)\n",
    "        print(\"Checkpoint saved. Run again to resume.\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        await grok_client.close()\n",
    "\n",
    "\n",
    "# Run the generation\n",
    "# In Jupyter, use: await run_generation()\n",
    "# In script, use: asyncio.run(run_generation())\n",
    "\n",
    "# For Jupyter notebooks:\n",
    "samples = await run_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9444e",
   "metadata": {},
   "source": [
    "## Post-Generation Analysis and Export\n",
    "\n",
    "Analyze the generated dataset and prepare for the next notebook (validation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd839f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(samples: list[SyntheticSample]) -> None:\n",
    "    \"\"\"Print comprehensive analysis of generated dataset.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic counts\n",
    "    print(f\"\\nTotal samples: {len(samples)}\")\n",
    "    \n",
    "    # Dimension distribution\n",
    "    dim_counts = defaultdict(int)\n",
    "    for s in samples:\n",
    "        dim_counts[s.feature_dimension.value] += 1\n",
    "    \n",
    "    print(\"\\nDistribution by dimension:\")\n",
    "    for dim in FEATURE_DIMENSIONS:\n",
    "        count = dim_counts.get(dim, 0)\n",
    "        pct = count / len(samples) * 100 if samples else 0\n",
    "        print(f\"  {dim:15s}: {count:5d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # PII type distribution\n",
    "    pii_counts = defaultdict(int)\n",
    "    for s in samples:\n",
    "        pii_counts[s.seed_pii_type] += 1\n",
    "    \n",
    "    print(\"\\nDistribution by PII type:\")\n",
    "    for pii_type in sorted(pii_counts.keys()):\n",
    "        count = pii_counts[pii_type]\n",
    "        pct = count / len(samples) * 100 if samples else 0\n",
    "        print(f\"  {pii_type:30s}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Locale distribution\n",
    "    locale_counts = defaultdict(int)\n",
    "    for s in samples:\n",
    "        if s.seed_pii_locale:\n",
    "            locale_counts[s.seed_pii_locale] += 1\n",
    "    \n",
    "    print(\"\\nDistribution by locale:\")\n",
    "    for locale in sorted(locale_counts.keys()):\n",
    "        count = locale_counts[locale]\n",
    "        pct = count / len(samples) * 100 if samples else 0\n",
    "        print(f\"  {locale:10s}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Entity statistics\n",
    "    total_entities = sum(len(s.entities) for s in samples)\n",
    "    avg_entities = total_entities / len(samples) if samples else 0\n",
    "    \n",
    "    print(f\"\\nEntity statistics:\")\n",
    "    print(f\"  Total entities: {total_entities}\")\n",
    "    print(f\"  Average per sample: {avg_entities:.2f}\")\n",
    "    \n",
    "    # Text length statistics\n",
    "    text_lengths = [len(s.text) for s in samples]\n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(f\"  Min: {min(text_lengths) if text_lengths else 0}\")\n",
    "    print(f\"  Max: {max(text_lengths) if text_lengths else 0}\")\n",
    "    print(f\"  Mean: {sum(text_lengths) / len(text_lengths) if text_lengths else 0:.1f}\")\n",
    "    \n",
    "    # Sample examples\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE EXAMPLES (one per dimension)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    shown_dims = set()\n",
    "    for sample in samples:\n",
    "        if sample.feature_dimension.value not in shown_dims:\n",
    "            shown_dims.add(sample.feature_dimension.value)\n",
    "            print(f\"\\n[{sample.feature_dimension.value.upper()}]\")\n",
    "            print(f\"Text: {sample.text[:200]}...\")\n",
    "            print(f\"Entities: {len(sample.entities)}\")\n",
    "            for ent in sample.entities[:3]:\n",
    "                print(f\"  - [{ent.start}:{ent.end}] {ent.label}: '{ent.text}'\")\n",
    "            \n",
    "            if len(shown_dims) >= 6:\n",
    "                break\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "if 'samples' in dir() and samples:\n",
    "    analyze_dataset(samples)\n",
    "else:\n",
    "    print(\"No samples generated yet. Run the generation cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d7338",
   "metadata": {},
   "source": [
    "## Export to CSV/Parquet\n",
    "\n",
    "Export the generated data in formats suitable for later validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449ace09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_for_validation(\n",
    "    samples: list[SyntheticSample],\n",
    "    output_dir: str = \"./data/synthetic\",\n",
    ") -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Export samples in multiple formats for downstream processing.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of generated samples\n",
    "        output_dir: Output directory path\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping format names to file paths\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    exported_files = {}\n",
    "    \n",
    "    # 1. JSON Lines format (one sample per line, for streaming)\n",
    "    jsonl_path = output_dir / \"synthetic_samples.jsonl\"\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sample in samples:\n",
    "            f.write(json.dumps(sample.model_dump(), ensure_ascii=False) + \"\\n\")\n",
    "    exported_files[\"jsonl\"] = str(jsonl_path)\n",
    "    print(f\"✓ Exported JSONL: {jsonl_path}\")\n",
    "    \n",
    "    # 2. CSV format (flattened, for quick inspection)\n",
    "    csv_data = []\n",
    "    for sample in samples:\n",
    "        csv_data.append({\n",
    "            \"generation_id\": sample.generation_id,\n",
    "            \"text\": sample.text,\n",
    "            \"feature_dimension\": sample.feature_dimension.value,\n",
    "            \"seed_pii_type\": sample.seed_pii_type,\n",
    "            \"seed_pii_value\": sample.seed_pii_value,\n",
    "            \"seed_pii_locale\": sample.seed_pii_locale,\n",
    "            \"scenario\": sample.scenario,\n",
    "            \"type_variant\": sample.type_variant,\n",
    "            \"num_entities\": len(sample.entities),\n",
    "            \"entities_json\": json.dumps([e.model_dump() for e in sample.entities]),\n",
    "            \"timestamp\": sample.timestamp,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_path = output_dir / \"synthetic_samples.csv\"\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    exported_files[\"csv\"] = str(csv_path)\n",
    "    print(f\"✓ Exported CSV: {csv_path}\")\n",
    "    \n",
    "    # 3. Parquet format (efficient for large datasets)\n",
    "    try:\n",
    "        parquet_path = output_dir / \"synthetic_samples.parquet\"\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        exported_files[\"parquet\"] = str(parquet_path)\n",
    "        print(f\"✓ Exported Parquet: {parquet_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Parquet export skipped ({e})\")\n",
    "    \n",
    "    # 4. Dimension-specific JSON files (for targeted validation)\n",
    "    for dimension in FEATURE_DIMENSIONS:\n",
    "        dim_samples = [s for s in samples if s.feature_dimension.value == dimension]\n",
    "        if dim_samples:\n",
    "            dim_path = output_dir / f\"synthetic_{dimension}.json\"\n",
    "            with open(dim_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    [s.model_dump() for s in dim_samples],\n",
    "                    f,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "            exported_files[f\"json_{dimension}\"] = str(dim_path)\n",
    "    print(f\"✓ Exported dimension-specific JSON files\")\n",
    "    \n",
    "    print(f\"\\nTotal files exported: {len(exported_files)}\")\n",
    "    return exported_files\n",
    "\n",
    "\n",
    "# Export\n",
    "if 'samples' in dir() and samples:\n",
    "    exported = export_for_validation(samples)\n",
    "    print(\"\\nExported files:\")\n",
    "    for fmt, path in exported.items():\n",
    "        print(f\"  {fmt}: {path}\")\n",
    "else:\n",
    "    print(\"No samples to export. Run generation first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
