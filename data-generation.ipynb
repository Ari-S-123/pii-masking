{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7ac4f1",
   "metadata": {},
   "source": [
    "# Synthetic PII Data Generation\n",
    "\n",
    "Generate 11,000 rows of challenging PII examples targeting the six feature dimensions from Singh & Narayanan (2025)\n",
    "\"Unmasking the Reality of PII Masking Models\".\n",
    "\n",
    "Target dimensions: - basic: Straightforward, well-formatted entities - contextual: Entities requiring disambiguation -\n",
    "noisy: Real-world imperfections (typos, OCR errors, abbreviations) - evolving: New/emerging PII formats (crypto, UPI,\n",
    "modern handles) - multilingual: International PII formats in English text - adversarial: Intentionally confusing inputs\n",
    "designed to fool NER models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7318b07",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d90e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xai-sdk\n",
      "  Downloading xai_sdk-1.4.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\aritr\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Collecting faker\n",
      "  Downloading faker-38.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aritr\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pydantic in c:\\users\\aritr\\anaconda3\\lib\\site-packages (2.10.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\aritr\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: tenacity in c:\\users\\aritr\\anaconda3\\lib\\site-packages (9.0.0)\n",
      "Collecting json-repair\n",
      "  Downloading json_repair-0.54.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from xai-sdk) (3.11.10)\n",
      "Collecting grpcio<2,>=1.72.1 (from xai-sdk)\n",
      "  Using cached grpcio-1.76.0-cp313-cp313-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting opentelemetry-sdk<2,>=1.36.0 (from xai-sdk)\n",
      "  Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting packaging<26,>=25.0 (from xai-sdk)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting protobuf<7,>=5.29.4 (from xai-sdk)\n",
      "  Using cached protobuf-6.33.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.31.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from xai-sdk) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from pydantic) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->xai-sdk) (1.18.0)\n",
      "Collecting opentelemetry-api==1.39.0 (from opentelemetry-sdk<2,>=1.36.0->xai-sdk)\n",
      "  Downloading opentelemetry_api-1.39.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b0 (from opentelemetry-sdk<2,>=1.36.0->xai-sdk)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from opentelemetry-api==1.39.0->opentelemetry-sdk<2,>=1.36.0->xai-sdk) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api==1.39.0->opentelemetry-sdk<2,>=1.36.0->xai-sdk) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from requests<3,>=2.31.0->xai-sdk) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from requests<3,>=2.31.0->xai-sdk) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from requests<3,>=2.31.0->xai-sdk) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from requests<3,>=2.31.0->xai-sdk) (2025.4.26)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aritr\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading xai_sdk-1.4.1-py3-none-any.whl (191 kB)\n",
      "Downloading grpcio-1.76.0-cp313-cp313-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 2.1/4.7 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.7/4.7 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading opentelemetry_sdk-1.39.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_api-1.39.0-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b0-py3-none-any.whl (219 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached protobuf-6.33.1-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 16.8 MB/s eta 0:00:00\n",
      "Downloading jiter-0.12.0-cp313-cp313-win_amd64.whl (204 kB)\n",
      "Downloading faker-38.2.0-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 29.7 MB/s eta 0:00:00\n",
      "Downloading json_repair-0.54.2-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: protobuf, packaging, json-repair, jiter, grpcio, faker, opentelemetry-api, opentelemetry-semantic-conventions, openai, opentelemetry-sdk, xai-sdk\n",
      "\n",
      "  Attempting uninstall: protobuf\n",
      "\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "\n",
      "    Uninstalling protobuf-5.29.3:\n",
      "\n",
      "      Successfully uninstalled protobuf-5.29.3\n",
      "\n",
      "   ----------------------------------------  0/11 [protobuf]\n",
      "  Attempting uninstall: packaging\n",
      "   ----------------------------------------  0/11 [protobuf]\n",
      "    Found existing installation: packaging 24.2\n",
      "   ----------------------------------------  0/11 [protobuf]\n",
      "    Uninstalling packaging-24.2:\n",
      "   ----------------------------------------  0/11 [protobuf]\n",
      "      Successfully uninstalled packaging-24.2\n",
      "   ----------------------------------------  0/11 [protobuf]\n",
      "   --- ------------------------------------  1/11 [packaging]\n",
      "   -------------- -------------------------  4/11 [grpcio]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   ------------------ ---------------------  5/11 [faker]\n",
      "   --------------------- -----------  7/11 [opentelemetry-semantic-conventions]\n",
      "   --------------------- -----------  7/11 [opentelemetry-semantic-conventions]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   ----------------------------- ----------  8/11 [openai]\n",
      "   -------------------------------- -------  9/11 [opentelemetry-sdk]\n",
      "   ------------------------------------ --- 10/11 [xai-sdk]\n",
      "   ---------------------------------------- 11/11 [xai-sdk]\n",
      "\n",
      "Successfully installed faker-38.2.0 grpcio-1.76.0 jiter-0.12.0 json-repair-0.54.2 openai-2.8.1 opentelemetry-api-1.39.0 opentelemetry-sdk-1.39.0 opentelemetry-semantic-conventions-0.60b0 packaging-25.0 protobuf-6.33.1 xai-sdk-1.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.45.1 requires packaging<25,>=20, but you have packaging 25.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install xai-sdk openai pandas faker tqdm pydantic python-dotenv tenacity json-repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa1475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from faker import Faker\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    ")\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e62f5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment loaded successfully\n",
      "  XAI_API_KEY: ********...MXdq\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify required API keys are present\n",
    "XAI_API_KEY: str | None = os.getenv(\"XAI_API_KEY\")\n",
    "if not XAI_API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        \"XAI_API_KEY not found in environment. \"\n",
    "        \"Create a .env file with your xAI API key.\"\n",
    "    )\n",
    "\n",
    "print(\"✓ Environment loaded successfully\")\n",
    "print(f\"  XAI_API_KEY: {'*' * 8}...{XAI_API_KEY[-4:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d929522",
   "metadata": {},
   "source": [
    "## Label Mapping Configuration\n",
    "\n",
    "Label schema harmonization between the paper's 16 PII categories and ai4privacy dataset labels. All synthetic data uses\n",
    "this unified mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24cb5d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configured 16 PII types across 6 dimensions\n",
      "  PII Types: ['NAME', 'EMAIL', 'PHONE', 'DATE_OF_BIRTH', 'POSTAL_CODE', 'CREDIT_CARD', 'BANK_ACCOUNT', 'DRIVER_LICENSE', 'PASSPORT_NUMBER', 'NATIONAL_IDENTITY_SSN_AADHAR', 'OTHER_NATIONAL_IDENTITY', 'TAX_IDENTIFICATION', 'VEHICLE_REGISTRATION', 'INSURANCE_NUMBER', 'BANK_UPI_ID', 'NAMES_OF_PLACES_OR_NOUNS']\n",
      "  Dimensions: ['basic', 'contextual', 'noisy', 'evolving', 'multilingual', 'adversarial']\n",
      "  Locales: ['en_US', 'en_GB', 'en_IN', 'de_DE', 'fr_FR', 'en_AU', 'en_CA', 'it_IT', 'es_ES', 'nl_NL']\n"
     ]
    }
   ],
   "source": [
    "# Maps paper's categories → ai4privacy labels (primary label is first)\n",
    "PAPER_TO_AI4PRIVACY: dict[str, list[str]] = {\n",
    "    \"NAME\": [\"FIRSTNAME\", \"LASTNAME\", \"MIDDLENAME\"],\n",
    "    \"EMAIL\": [\"EMAIL\"],\n",
    "    \"PHONE\": [\"PHONENUMBER\"],\n",
    "    \"DATE_OF_BIRTH\": [\"DOB\"],\n",
    "    \"POSTAL_CODE\": [\"ZIPCODE\"],\n",
    "    \"CREDIT_CARD\": [\"CREDITCARDNUMBER\"],\n",
    "    \"BANK_ACCOUNT\": [\"ACCOUNTNUMBER\", \"IBAN\", \"BIC\"],\n",
    "    \"DRIVER_LICENSE\": [\"DRIVERLICENSE\"],\n",
    "    \"PASSPORT_NUMBER\": [\"PASSPORT\"],\n",
    "    \"NATIONAL_IDENTITY_SSN_AADHAR\": [\"SSN\"],\n",
    "    \"OTHER_NATIONAL_IDENTITY\": [\"NATIONALID\"],\n",
    "    \"TAX_IDENTIFICATION\": [\"TAXID\"],\n",
    "    \"VEHICLE_REGISTRATION\": [\"VEHICLEVRM\", \"VEHICLEVIN\"],\n",
    "    \"INSURANCE_NUMBER\": [\"INSURANCENUMBER\"],\n",
    "    \"BANK_UPI_ID\": [\"UPIID\"],\n",
    "    \"NAMES_OF_PLACES_OR_NOUNS\": [\"CITY\", \"STATE\", \"COUNTY\", \"STREET\"],\n",
    "}\n",
    "\n",
    "# Inverse mapping for evaluation (ai4privacy → paper categories)\n",
    "AI4PRIVACY_TO_PAPER: dict[str, str] = {\n",
    "    v: k for k, vs in PAPER_TO_AI4PRIVACY.items() for v in vs\n",
    "}\n",
    "\n",
    "# All base labels for synthetic generation (we use paper categories)\n",
    "ALL_PII_TYPES: list[str] = list(PAPER_TO_AI4PRIVACY.keys())\n",
    "\n",
    "# Feature dimensions from Singh & Narayanan (2025)\n",
    "FEATURE_DIMENSIONS: list[str] = [\n",
    "    \"basic\",\n",
    "    \"contextual\",\n",
    "    \"noisy\",\n",
    "    \"evolving\",\n",
    "    \"multilingual\",\n",
    "    \"adversarial\",\n",
    "]\n",
    "\n",
    "# Locales for international PII formats (all in English text context)\n",
    "SUPPORTED_LOCALES: dict[str, str] = {\n",
    "    \"en_US\": \"United States\",\n",
    "    \"en_GB\": \"United Kingdom\",\n",
    "    \"en_IN\": \"India\",\n",
    "    \"de_DE\": \"Germany\",\n",
    "    \"fr_FR\": \"France\",\n",
    "    \"en_AU\": \"Australia\",\n",
    "    \"en_CA\": \"Canada\",\n",
    "    \"it_IT\": \"Italy\",\n",
    "    \"es_ES\": \"Spain\",\n",
    "    \"nl_NL\": \"Netherlands\",\n",
    "}\n",
    "\n",
    "print(f\"✓ Configured {len(ALL_PII_TYPES)} PII types across {len(FEATURE_DIMENSIONS)} dimensions\")\n",
    "print(f\"  PII Types: {ALL_PII_TYPES}\")\n",
    "print(f\"  Dimensions: {FEATURE_DIMENSIONS}\")\n",
    "print(f\"  Locales: {list(SUPPORTED_LOCALES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a674a",
   "metadata": {},
   "source": [
    "## Pydantic Schemas for Synthetic Output\n",
    "\n",
    "Pydantic schemas defining the structure of synthetic PII samples. All generated data must conform to these schemas for\n",
    "validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34544ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pydantic schemas defined\n",
      "  SyntheticSample fields: ['text', 'entities', 'feature_dimension', 'seed_pii_type', 'seed_pii_value', 'seed_pii_locale', 'scenario', 'type_variant', 'generation_id', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "class FeatureDimension(str, Enum):\n",
    "    \"\"\"\n",
    "    The six NER failure mode dimensions from Singh & Narayanan (2025).\n",
    "    \n",
    "    Each dimension represents a specific type of challenge for PII detection:\n",
    "        - basic: Standard, well-formatted entities with clear boundaries\n",
    "        - contextual: Ambiguous entities requiring surrounding context\n",
    "        - noisy: Real-world text imperfections and formatting variations\n",
    "        - evolving: Modern/emerging PII formats not in traditional training data\n",
    "        - multilingual: International formats embedded in English prose\n",
    "        - adversarial: Intentionally deceptive patterns designed to evade detection\n",
    "    \"\"\"\n",
    "    BASIC = \"basic\"\n",
    "    CONTEXTUAL = \"contextual\"\n",
    "    NOISY = \"noisy\"\n",
    "    EVOLVING = \"evolving\"\n",
    "    MULTILINGUAL = \"multilingual\"\n",
    "    ADVERSARIAL = \"adversarial\"\n",
    "\n",
    "\n",
    "class EntitySpan(BaseModel):\n",
    "    \"\"\"\n",
    "    A single PII entity annotation with character-level span positions.\n",
    "    \n",
    "    Attributes:\n",
    "        start: Starting character index (0-based, inclusive)\n",
    "        end: Ending character index (exclusive, like Python slicing)\n",
    "        label: PII type label from the unified taxonomy\n",
    "        text: The actual text content of the entity (for verification)\n",
    "    \"\"\"\n",
    "    start: int = Field(..., ge=0, description=\"Start character index (inclusive)\")\n",
    "    end: int = Field(..., gt=0, description=\"End character index (exclusive)\")\n",
    "    label: str = Field(..., description=\"PII type label\")\n",
    "    text: str = Field(..., min_length=1, description=\"Entity text content\")\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_span_bounds(self) -> \"EntitySpan\":\n",
    "        \"\"\"Ensure start < end for valid span.\"\"\"\n",
    "        if self.start >= self.end:\n",
    "            raise ValueError(f\"Invalid span: start ({self.start}) must be < end ({self.end})\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class SyntheticSample(BaseModel):\n",
    "    \"\"\"\n",
    "    A complete synthetic PII training sample with text and annotations.\n",
    "    \n",
    "    This schema captures everything needed for training and validation:\n",
    "    the generated text, all entity annotations, metadata about the\n",
    "    generation process, and the feature dimension being targeted.\n",
    "    \n",
    "    Attributes:\n",
    "        text: The generated English text containing PII entities\n",
    "        entities: List of all PII entity annotations with spans\n",
    "        feature_dimension: Which NER challenge dimension this targets\n",
    "        seed_pii_type: The primary PII type used to seed generation\n",
    "        seed_pii_value: The actual PII value that was seeded\n",
    "        seed_pii_locale: Locale/region for international formats\n",
    "        scenario: Brief description of the text scenario/context\n",
    "        type_variant: Specific variant or sub-type of the PII\n",
    "        generation_id: Unique identifier for this generation attempt\n",
    "        timestamp: When this sample was generated\n",
    "    \"\"\"\n",
    "    text: str = Field(..., min_length=50, max_length=600, description=\"Generated text\")\n",
    "    entities: list[EntitySpan] = Field(..., min_length=1, description=\"Entity annotations\")\n",
    "    feature_dimension: FeatureDimension = Field(..., description=\"Target dimension\")\n",
    "    seed_pii_type: str = Field(..., description=\"Primary PII type\")\n",
    "    seed_pii_value: str = Field(..., description=\"Seeded PII value\")\n",
    "    seed_pii_locale: str | None = Field(None, description=\"Locale for international formats\")\n",
    "    scenario: str = Field(..., description=\"Text scenario description\")\n",
    "    type_variant: str = Field(..., description=\"PII format variant\")\n",
    "    generation_id: str = Field(..., description=\"Unique generation ID\")\n",
    "    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_entities_in_text(self) -> \"SyntheticSample\":\n",
    "        \"\"\"Verify all entity spans are valid within the text.\"\"\"\n",
    "        text_len = len(self.text)\n",
    "        for entity in self.entities:\n",
    "            if entity.end > text_len:\n",
    "                raise ValueError(\n",
    "                    f\"Entity span [{entity.start}:{entity.end}] exceeds \"\n",
    "                    f\"text length {text_len}\"\n",
    "                )\n",
    "            actual_text = self.text[entity.start:entity.end]\n",
    "            if actual_text != entity.text:\n",
    "                raise ValueError(\n",
    "                    f\"Entity text mismatch at [{entity.start}:{entity.end}]: \"\n",
    "                    f\"expected '{entity.text}', found '{actual_text}'\"\n",
    "                )\n",
    "        return self\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_seed_pii_present(self) -> \"SyntheticSample\":\n",
    "        \"\"\"Ensure the seed PII value appears in the text.\"\"\"\n",
    "        if self.seed_pii_value not in self.text:\n",
    "            raise ValueError(\n",
    "                f\"Seed PII value '{self.seed_pii_value}' not found in generated text\"\n",
    "            )\n",
    "        return self\n",
    "\n",
    "\n",
    "class GenerationBatch(BaseModel):\n",
    "    \"\"\"\n",
    "    A batch of synthetic samples with generation metadata.\n",
    "    \n",
    "    Attributes:\n",
    "        samples: List of generated samples in this batch\n",
    "        dimension: The feature dimension for all samples in batch\n",
    "        batch_id: Unique batch identifier\n",
    "        total_requested: How many samples were requested\n",
    "        successful: How many were successfully generated\n",
    "        failed: How many failed generation/validation\n",
    "    \"\"\"\n",
    "    samples: list[SyntheticSample] = Field(default_factory=list)\n",
    "    dimension: FeatureDimension\n",
    "    batch_id: str\n",
    "    total_requested: int\n",
    "    successful: int = 0\n",
    "    failed: int = 0\n",
    "\n",
    "\n",
    "print(\"✓ Pydantic schemas defined\")\n",
    "print(f\"  SyntheticSample fields: {list(SyntheticSample.model_fields.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb4d2ff",
   "metadata": {},
   "source": [
    "## Faker-Based PII Value Generators\n",
    "\n",
    "Realistic PII value generation using Faker with locale support. These generators create seed PII values that will be\n",
    "embedded in synthetic text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1634fb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PIIGenerator initialized\n",
      "\n",
      "Sample generated PII values:\n",
      "  NATIONAL_IDENTITY_SSN_AADHAR: 397-60-1868 (en_US)\n",
      "  DRIVER_LICENSE: C7637049 (en_US)\n",
      "  NAME: Eduardo Barnes (en_US)\n",
      "  POSTAL_CODE: 77492 (en_US)\n",
      "  EMAIL: harriscarolyn@example.net (en_US)\n"
     ]
    }
   ],
   "source": [
    "class PIIGenerator:\n",
    "    \"\"\"\n",
    "    Generates realistic PII values across multiple locales using Faker.\n",
    "    \n",
    "    This class provides methods to generate each of the 16 PII types with\n",
    "    proper formatting for different countries/regions. All generated PII\n",
    "    is synthetic and safe for training data.\n",
    "    \n",
    "    Attributes:\n",
    "        fakers: Dictionary mapping locale codes to Faker instances\n",
    "        default_locale: Fallback locale when requested locale unavailable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, locales: list[str] | None = None):\n",
    "        \"\"\"\n",
    "        Initialize PII generators for specified locales.\n",
    "        \n",
    "        Args:\n",
    "            locales: List of locale codes (e.g., ['en_US', 'en_GB', 'de_DE']).\n",
    "                     Defaults to all supported locales if not specified.\n",
    "        \"\"\"\n",
    "        self.locales = locales or list(SUPPORTED_LOCALES.keys())\n",
    "        self.fakers: dict[str, Faker] = {}\n",
    "        self.default_locale = \"en_US\"\n",
    "        \n",
    "        for locale in self.locales:\n",
    "            try:\n",
    "                self.fakers[locale] = Faker(locale)\n",
    "                # Seed for reproducibility within session\n",
    "                self.fakers[locale].seed_instance(random.randint(0, 10000))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not initialize Faker for {locale}: {e}\")\n",
    "        \n",
    "        if not self.fakers:\n",
    "            raise RuntimeError(\"No Faker instances could be initialized\")\n",
    "    \n",
    "    def _get_faker(self, locale: str | None = None) -> tuple[Faker, str]:\n",
    "        \"\"\"Get Faker instance for locale, falling back to default.\"\"\"\n",
    "        if locale and locale in self.fakers:\n",
    "            return self.fakers[locale], locale\n",
    "        return self.fakers[self.default_locale], self.default_locale\n",
    "    \n",
    "    def generate_name(\n",
    "        self, \n",
    "        locale: str | None = None,\n",
    "        name_type: str = \"full\",\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Generate a realistic person name.\n",
    "        \n",
    "        Args:\n",
    "            locale: Target locale for name generation\n",
    "            name_type: One of 'full', 'first', 'last', 'middle'\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (name_value, actual_locale, label) where label is\n",
    "            FIRSTNAME, LASTNAME, MIDDLENAME, or NAME for full names.\n",
    "        \"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        if name_type == \"first\":\n",
    "            return faker.first_name(), actual_locale, \"FIRSTNAME\"\n",
    "        elif name_type == \"last\":\n",
    "            return faker.last_name(), actual_locale, \"LASTNAME\"\n",
    "        elif name_type == \"middle\":\n",
    "            return faker.first_name(), actual_locale, \"MIDDLENAME\"\n",
    "        else:\n",
    "            return faker.name(), actual_locale, \"NAME\"\n",
    "    \n",
    "    def generate_email(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a realistic email address.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        return faker.email(), actual_locale, \"EMAIL\"\n",
    "    \n",
    "    def generate_phone(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a phone number in locale-appropriate format.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        return faker.phone_number(), actual_locale, \"PHONE\"\n",
    "    \n",
    "    def generate_dob(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a date of birth with locale-appropriate formatting.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        dob = faker.date_of_birth(minimum_age=18, maximum_age=85)\n",
    "        \n",
    "        # Format varies by locale\n",
    "        if actual_locale in [\"en_US\", \"en_CA\"]:\n",
    "            formatted = dob.strftime(\"%m/%d/%Y\")\n",
    "        elif actual_locale in [\"en_GB\", \"en_AU\", \"en_IN\", \"de_DE\", \"fr_FR\", \"it_IT\", \"es_ES\", \"nl_NL\"]:\n",
    "            formatted = dob.strftime(\"%d/%m/%Y\")\n",
    "        else:\n",
    "            formatted = dob.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        return formatted, actual_locale, \"DATE_OF_BIRTH\"\n",
    "    \n",
    "    def generate_postal_code(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a postal/ZIP code for the specified locale.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        return faker.postcode(), actual_locale, \"POSTAL_CODE\"\n",
    "    \n",
    "    def generate_credit_card(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a credit card number (Luhn-valid but fake).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        return faker.credit_card_number(), actual_locale, \"CREDIT_CARD\"\n",
    "    \n",
    "    def generate_bank_account(\n",
    "        self, \n",
    "        locale: str | None = None,\n",
    "        account_type: str = \"iban\",\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Generate a bank account identifier.\n",
    "        \n",
    "        Args:\n",
    "            locale: Target locale\n",
    "            account_type: 'iban' for European, 'account' for numeric, 'bic' for SWIFT\n",
    "        \"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        if account_type == \"iban\" and hasattr(faker, \"iban\"):\n",
    "            return faker.iban(), actual_locale, \"BANK_ACCOUNT\"\n",
    "        elif account_type == \"bic\" and hasattr(faker, \"swift\"):\n",
    "            return faker.swift(), actual_locale, \"BANK_ACCOUNT\"\n",
    "        else:\n",
    "            # Generate account-style number\n",
    "            return faker.bban(), actual_locale, \"BANK_ACCOUNT\"\n",
    "    \n",
    "    def generate_driver_license(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a driver's license number pattern for locale.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        # Different formats by country\n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_letter().upper()}{faker.random_number(digits=7, fix_len=True)}\",\n",
    "            \"en_GB\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_number(digits=6, fix_len=True)}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}99{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}\",\n",
    "            \"en_IN\": lambda: f\"{faker.state_abbr() if hasattr(faker, 'state_abbr') else 'MH'}{faker.random_number(digits=13, fix_len=True)}\",\n",
    "            \"de_DE\": lambda: f\"{faker.random_number(digits=11, fix_len=True)}\",\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, patterns[\"en_US\"])\n",
    "        return generator(), actual_locale, \"DRIVER_LICENSE\"\n",
    "    \n",
    "    def generate_passport(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a passport number pattern.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        # Common patterns: letter(s) + digits\n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_number(digits=9, fix_len=True)}\",\n",
    "            \"en_GB\": lambda: f\"{faker.random_number(digits=9, fix_len=True)}\",\n",
    "            \"en_IN\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_number(digits=7, fix_len=True)}\",\n",
    "            \"de_DE\": lambda: f\"C{faker.random_number(digits=8, fix_len=True)}\",\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, lambda: f\"{faker.random_uppercase_letter()}{faker.random_number(digits=8, fix_len=True)}\")\n",
    "        return generator(), actual_locale, \"PASSPORT_NUMBER\"\n",
    "    \n",
    "    def generate_ssn(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a national identity number (SSN, Aadhaar, NI, etc.).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_number(digits=3, fix_len=True)}-{faker.random_number(digits=2, fix_len=True)}-{faker.random_number(digits=4, fix_len=True)}\",\n",
    "            \"en_GB\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_number(digits=6, fix_len=True)}{faker.random_uppercase_letter()}\",\n",
    "            \"en_IN\": lambda: f\"{faker.random_number(digits=4, fix_len=True)} {faker.random_number(digits=4, fix_len=True)} {faker.random_number(digits=4, fix_len=True)}\",\n",
    "            \"de_DE\": lambda: f\"{faker.random_number(digits=11, fix_len=True)}\",\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, patterns[\"en_US\"])\n",
    "        return generator(), actual_locale, \"NATIONAL_IDENTITY_SSN_AADHAR\"\n",
    "    \n",
    "    def generate_other_national_id(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate other national identity formats (PAN, TFN, etc.).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        patterns = {\n",
    "            \"en_IN\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_number(digits=4, fix_len=True)}{faker.random_uppercase_letter()}\",  # PAN\n",
    "            \"en_AU\": lambda: f\"{faker.random_number(digits=9, fix_len=True)}\",  # TFN\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, lambda: f\"{faker.random_uppercase_letter()}{faker.random_number(digits=8, fix_len=True)}\")\n",
    "        return generator(), actual_locale, \"OTHER_NATIONAL_IDENTITY\"\n",
    "    \n",
    "    def generate_tax_id(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a tax identification number.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_number(digits=2, fix_len=True)}-{faker.random_number(digits=7, fix_len=True)}\",  # EIN\n",
    "            \"de_DE\": lambda: f\"DE{faker.random_number(digits=9, fix_len=True)}\",  # VAT\n",
    "            \"en_GB\": lambda: f\"GB{faker.random_number(digits=9, fix_len=True)}\",  # VAT\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, patterns[\"en_US\"])\n",
    "        return generator(), actual_locale, \"TAX_IDENTIFICATION\"\n",
    "    \n",
    "    def generate_vehicle_registration(\n",
    "        self, \n",
    "        locale: str | None = None,\n",
    "        reg_type: str = \"plate\",\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate vehicle registration (plate number or VIN).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        if reg_type == \"vin\":\n",
    "            # VIN is international 17-character format\n",
    "            chars = string.ascii_uppercase.replace(\"I\", \"\").replace(\"O\", \"\").replace(\"Q\", \"\") + string.digits\n",
    "            vin = \"\".join(random.choices(chars, k=17))\n",
    "            return vin, actual_locale, \"VEHICLE_REGISTRATION\"\n",
    "        \n",
    "        # License plate patterns\n",
    "        patterns = {\n",
    "            \"en_US\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}-{faker.random_number(digits=4, fix_len=True)}\",\n",
    "            \"en_GB\": lambda: f\"{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_number(digits=2, fix_len=True)} {faker.random_uppercase_letter()}{faker.random_uppercase_letter()}{faker.random_uppercase_letter()}\",\n",
    "            \"de_DE\": lambda: f\"{faker.city()[:2].upper()}-{faker.random_uppercase_letter()}{faker.random_uppercase_letter()} {faker.random_number(digits=4, fix_len=True)}\",\n",
    "            \"en_IN\": lambda: f\"{faker.state_abbr() if hasattr(faker, 'state_abbr') else 'MH'}{faker.random_number(digits=2, fix_len=True)} {faker.random_uppercase_letter()}{faker.random_uppercase_letter()} {faker.random_number(digits=4, fix_len=True)}\",\n",
    "        }\n",
    "        \n",
    "        generator = patterns.get(actual_locale, patterns[\"en_US\"])\n",
    "        return generator(), actual_locale, \"VEHICLE_REGISTRATION\"\n",
    "    \n",
    "    def generate_insurance_number(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate an insurance policy/member number.\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        prefixes = [\"INS\", \"POL\", \"MBR\", \"HLT\", \"AUT\"]\n",
    "        prefix = random.choice(prefixes)\n",
    "        number = faker.random_number(digits=10, fix_len=True)\n",
    "        \n",
    "        return f\"{prefix}-{number}\", actual_locale, \"INSURANCE_NUMBER\"\n",
    "    \n",
    "    def generate_upi_id(self, locale: str | None = None) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate an Indian UPI ID (user@provider format).\"\"\"\n",
    "        faker, _ = self._get_faker(\"en_IN\")  # UPI is India-specific\n",
    "        \n",
    "        providers = [\"okicici\", \"okhdfcbank\", \"oksbi\", \"ybl\", \"paytm\", \"gpay\", \"phonepe\"]\n",
    "        username = faker.user_name().lower()\n",
    "        provider = random.choice(providers)\n",
    "        \n",
    "        return f\"{username}@{provider}\", \"en_IN\", \"BANK_UPI_ID\"\n",
    "    \n",
    "    def generate_place_name(\n",
    "        self, \n",
    "        locale: str | None = None,\n",
    "        place_type: str = \"city\",\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"Generate a place name (city, state, street, etc.).\"\"\"\n",
    "        faker, actual_locale = self._get_faker(locale)\n",
    "        \n",
    "        if place_type == \"city\":\n",
    "            return faker.city(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "        elif place_type == \"state\":\n",
    "            if hasattr(faker, \"state\"):\n",
    "                return faker.state(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "            return faker.city(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "        elif place_type == \"street\":\n",
    "            return faker.street_name(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "        else:\n",
    "            return faker.city(), actual_locale, \"NAMES_OF_PLACES_OR_NOUNS\"\n",
    "    \n",
    "    def generate_pii(\n",
    "        self, \n",
    "        pii_type: str, \n",
    "        locale: str | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Generate PII of the specified type.\n",
    "        \n",
    "        Args:\n",
    "            pii_type: One of the 16 PII types from ALL_PII_TYPES\n",
    "            locale: Target locale for generation\n",
    "            **kwargs: Additional arguments for specific generators\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (pii_value, actual_locale, label)\n",
    "        \"\"\"\n",
    "        generators = {\n",
    "            \"NAME\": self.generate_name,\n",
    "            \"EMAIL\": self.generate_email,\n",
    "            \"PHONE\": self.generate_phone,\n",
    "            \"DATE_OF_BIRTH\": self.generate_dob,\n",
    "            \"POSTAL_CODE\": self.generate_postal_code,\n",
    "            \"CREDIT_CARD\": self.generate_credit_card,\n",
    "            \"BANK_ACCOUNT\": self.generate_bank_account,\n",
    "            \"DRIVER_LICENSE\": self.generate_driver_license,\n",
    "            \"PASSPORT_NUMBER\": self.generate_passport,\n",
    "            \"NATIONAL_IDENTITY_SSN_AADHAR\": self.generate_ssn,\n",
    "            \"OTHER_NATIONAL_IDENTITY\": self.generate_other_national_id,\n",
    "            \"TAX_IDENTIFICATION\": self.generate_tax_id,\n",
    "            \"VEHICLE_REGISTRATION\": self.generate_vehicle_registration,\n",
    "            \"INSURANCE_NUMBER\": self.generate_insurance_number,\n",
    "            \"BANK_UPI_ID\": self.generate_upi_id,\n",
    "            \"NAMES_OF_PLACES_OR_NOUNS\": self.generate_place_name,\n",
    "        }\n",
    "        \n",
    "        if pii_type not in generators:\n",
    "            raise ValueError(f\"Unknown PII type: {pii_type}. Valid types: {list(generators.keys())}\")\n",
    "        \n",
    "        return generators[pii_type](locale=locale, **kwargs)\n",
    "\n",
    "\n",
    "# Initialize global PII generator\n",
    "pii_gen = PIIGenerator()\n",
    "\n",
    "# Test generation\n",
    "print(\"✓ PIIGenerator initialized\")\n",
    "print(\"\\nSample generated PII values:\")\n",
    "for pii_type in random.sample(ALL_PII_TYPES, 5):\n",
    "    value, locale, label = pii_gen.generate_pii(pii_type, locale=\"en_US\")\n",
    "    print(f\"  {pii_type}: {value} ({locale})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eff157",
   "metadata": {},
   "source": [
    "## Dimension-Specific Prompt Templates\n",
    "\n",
    "Prompt templates for each feature dimension, designed to generate challenging PII examples that specifically target NER\n",
    "model failure modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5025476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt templates configured for all 6 dimensions\n",
      "  basic: 527 chars\n",
      "  contextual: 737 chars\n",
      "  noisy: 629 chars\n",
      "  evolving: 669 chars\n",
      "  multilingual: 719 chars\n",
      "  adversarial: 710 chars\n"
     ]
    }
   ],
   "source": [
    "# Base system prompt for all dimensions\n",
    "SYSTEM_PROMPT_BASE: str = \"\"\"You are a synthetic data generator creating training examples for PII (Personally Identifiable Information) detection models.\n",
    "\n",
    "Your task is to generate realistic English text containing the provided PII value, properly annotated with character-level spans.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "1. The text MUST be natural, coherent English\n",
    "2. The provided PII value MUST appear EXACTLY as given (no modifications)\n",
    "3. You MUST include additional contextually-relevant PII entities beyond the seed\n",
    "4. All entity spans MUST be accurate character positions (0-indexed, exclusive end)\n",
    "5. The scenario MUST feel realistic and plausible\n",
    "\n",
    "OUTPUT FORMAT (strict JSON):\n",
    "{\n",
    "    \"text\": \"The generated text containing PII...\",\n",
    "    \"entities\": [\n",
    "        {\"start\": 0, \"end\": 10, \"label\": \"LABEL\", \"text\": \"exact text\"},\n",
    "        ...\n",
    "    ],\n",
    "    \"scenario\": \"Brief description of the scenario\"\n",
    "}\n",
    "\n",
    "IMPORTANT: \n",
    "- The \"text\" field in each entity MUST exactly match text[start:end]\n",
    "- Include 2-5 PII entities total (including the seed)\n",
    "- Text length should be 100-500 characters\n",
    "- Do NOT use markdown formatting in the text\"\"\"\n",
    "\n",
    "\n",
    "DIMENSION_PROMPTS: dict[str, str] = {\n",
    "    \"basic\": \"\"\"DIMENSION: BASIC\n",
    "Generate straightforward text where the PII is clearly formatted and easily identifiable.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- PII appears in standard, expected formats\n",
    "- Clear contextual cues (e.g., \"Email:\", \"Phone:\", \"SSN:\")\n",
    "- Well-structured sentences\n",
    "- No ambiguity about entity boundaries\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- Contact information in a directory entry\n",
    "- Form data confirmation message\n",
    "- Official document excerpt\n",
    "- Registration confirmation\n",
    "\n",
    "The goal is clean, well-formatted examples that establish baseline performance.\"\"\",\n",
    "\n",
    "    \"contextual\": \"\"\"DIMENSION: CONTEXTUAL\n",
    "Generate text where PII requires context to disambiguate from similar-looking non-PII.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- Potential false positives nearby (e.g., product codes that look like IDs)\n",
    "- Ambiguous strings that could be PII or not depending on context\n",
    "- Names that could be company names, place names, or person names\n",
    "- Numbers that could be IDs, prices, or dates\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- Email discussing both a person named \"Amazon\" and the company Amazon\n",
    "- Text containing both a date \"March 15\" and a person named \"March\"\n",
    "- Discussion mixing product serial numbers with actual SSNs\n",
    "- Street addresses where street names are also person names\n",
    "\n",
    "The goal is examples requiring semantic understanding, not pattern matching.\"\"\",\n",
    "\n",
    "    \"noisy\": \"\"\"DIMENSION: NOISY\n",
    "Generate text with real-world imperfections that challenge NER systems.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- Typos and misspellings in surrounding text (NOT in the PII itself)\n",
    "- OCR-style errors (l/1, O/0 confusion in context)\n",
    "- Inconsistent formatting and spacing\n",
    "- Abbreviations and informal language\n",
    "- Missing punctuation or extra whitespace\n",
    "- SMS/chat-style shortened text\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- Scanned document with OCR artifacts\n",
    "- Hastily typed customer service chat\n",
    "- Social media post with typos\n",
    "- Informal email with abbreviations\n",
    "\n",
    "The PII values themselves should remain accurate - the noise is in the surrounding text.\"\"\",\n",
    "\n",
    "    \"evolving\": \"\"\"DIMENSION: EVOLVING\n",
    "Generate text containing modern/emerging PII formats not in traditional training data.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- Cryptocurrency wallet addresses (Bitcoin, Ethereum, etc.)\n",
    "- UPI IDs (username@provider format)\n",
    "- Modern usernames/handles (@mentions, Discord tags)\n",
    "- Digital payment identifiers\n",
    "- Cloud service identifiers\n",
    "- API keys or tokens (realistic-looking fakes)\n",
    "- Modern two-factor authentication codes\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- Cryptocurrency transaction discussion\n",
    "- Digital payment confirmation\n",
    "- Tech support for modern apps\n",
    "- Social media account setup\n",
    "- Fintech application onboarding\n",
    "\n",
    "The goal is PII types that have emerged in the last 5-10 years.\"\"\",\n",
    "\n",
    "    \"multilingual\": \"\"\"DIMENSION: MULTILINGUAL\n",
    "Generate English text containing PII in international formats from various countries.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- International phone number formats (+44, +91, +49, etc.)\n",
    "- Non-US ID formats (IBAN, UK NI numbers, Indian Aadhaar, German Personalausweis)\n",
    "- International postal codes (UK postcodes, German PLZ, Indian PIN codes)\n",
    "- Date formats from different regions (DD/MM/YYYY vs MM/DD/YYYY)\n",
    "- International vehicle registration formats\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- International business correspondence\n",
    "- Immigration/visa documentation\n",
    "- International banking transaction\n",
    "- Multinational company HR records\n",
    "- Travel booking confirmation\n",
    "\n",
    "Text MUST be in English, but PII formats should be from non-US locales.\"\"\",\n",
    "\n",
    "    \"adversarial\": \"\"\"DIMENSION: ADVERSARIAL\n",
    "Generate text with patterns designed to confuse or evade NER systems.\n",
    "\n",
    "CHARACTERISTICS:\n",
    "- Unusual spacing or formatting within PII\n",
    "- PII split across sentence boundaries\n",
    "- Obfuscated but recognizable PII (spaces in SSN: \"123 45 6789\")\n",
    "- PII embedded in code snippets or technical text\n",
    "- Edge cases with unusual but valid formats\n",
    "- Deliberately misleading context\n",
    "\n",
    "EXAMPLE SCENARIOS:\n",
    "- PII hidden in debug logs or error messages\n",
    "- Social engineering attempts with formatted PII\n",
    "- Technical documentation with embedded real values\n",
    "- PII in URLs, file paths, or JSON structures\n",
    "- Creatively formatted attempts to evade filters\n",
    "\n",
    "The goal is testing model robustness against evasion attempts.\"\"\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_generation_prompt(\n",
    "    dimension: str,\n",
    "    pii_type: str,\n",
    "    pii_value: str,\n",
    "    locale: str,\n",
    "    type_variant: str = \"standard\",\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Construct the complete prompt for synthetic data generation.\n",
    "    \n",
    "    Args:\n",
    "        dimension: Target feature dimension\n",
    "        pii_type: Type of PII being seeded\n",
    "        pii_value: The actual PII value to embed\n",
    "        locale: Locale/region for the PII\n",
    "        type_variant: Specific variant description\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (system_prompt, user_prompt)\n",
    "    \"\"\"\n",
    "    if dimension not in DIMENSION_PROMPTS:\n",
    "        raise ValueError(f\"Unknown dimension: {dimension}\")\n",
    "    \n",
    "    system_prompt = f\"{SYSTEM_PROMPT_BASE}\\n\\n{DIMENSION_PROMPTS[dimension]}\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Generate a {dimension.upper()} dimension training example.\n",
    "\n",
    "SEED PII:\n",
    "- Type: {pii_type}\n",
    "- Value: {pii_value}\n",
    "- Locale: {locale}\n",
    "- Variant: {type_variant}\n",
    "\n",
    "The provided PII value MUST appear exactly as shown in your generated text.\n",
    "Include 2-4 additional relevant PII entities.\n",
    "Ensure all entity spans are accurate character positions.\n",
    "\n",
    "Generate the JSON output now:\"\"\"\n",
    "    \n",
    "    return system_prompt, user_prompt\n",
    "\n",
    "\n",
    "print(\"✓ Prompt templates configured for all 6 dimensions\")\n",
    "for dim in FEATURE_DIMENSIONS:\n",
    "    print(f\"  {dim}: {len(DIMENSION_PROMPTS[dim])} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5814b4",
   "metadata": {},
   "source": [
    "## xAI API Client Wrapper\n",
    "\n",
    "xAI API client wrapper using the official xai_sdk.AsyncClient.\n",
    "\n",
    "This implementation follows the xAI async documentation: https://docs.x.ai/docs/guides/async\n",
    "\n",
    "The SDK is gRPC-based and handles:\n",
    "\n",
    "-   Connection pooling and management\n",
    "-   Authentication via API key\n",
    "-   Automatic retries for transient errors\n",
    "-   Proper timeout handling\n",
    "\n",
    "Rate Limit Strategy:\n",
    "\n",
    "-   xAI allows 480 requests/minute (8 req/sec average)\n",
    "-   We use asyncio.Semaphore to limit concurrent in-flight requests\n",
    "-   Batch processing with semaphore ensures we stay under limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe29857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Grok API client initialized using official xai_sdk.AsyncClient\n",
      "  Model: grok-4-1-fast-non-reasoning\n",
      "  Max concurrent requests: 40\n",
      "  Min batch interval: 1.0s\n",
      "  Theoretical max throughput: 2400 req/min\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import json_repair\n",
    "from xai_sdk import AsyncClient\n",
    "from xai_sdk.chat import system, user, Response\n",
    "\n",
    "\n",
    "class GrokClient:\n",
    "    \"\"\"\n",
    "    Async client for xAI's Grok API using the official SDK.\n",
    "    \n",
    "    This client wraps xai_sdk.AsyncClient and provides:\n",
    "        - Semaphore-controlled concurrency for rate limit compliance\n",
    "        - Batch processing for high-throughput generation\n",
    "        - JSON response parsing with repair for malformed LLM output\n",
    "        - Consistent error handling across all requests\n",
    "    \n",
    "    The xAI SDK is gRPC-based, which provides better performance and\n",
    "    reliability compared to raw HTTP requests.\n",
    "    \n",
    "    Attributes:\n",
    "        client: The underlying xai_sdk.AsyncClient instance\n",
    "        model: Model identifier (e.g., 'grok-4-1-fast-non-reasoning')\n",
    "        max_concurrent: Maximum simultaneous in-flight requests\n",
    "        min_batch_interval: Minimum seconds between batch starts (for rate limiting)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rate limit: 480 requests/minute = 8 req/sec\n",
    "    # With 20 concurrent requests, we need ~2.5 seconds per batch minimum\n",
    "    # Using 3.0 seconds for safety margin\n",
    "    DEFAULT_BATCH_SIZE: int = 20\n",
    "    DEFAULT_MIN_BATCH_INTERVAL: float = 3.0\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str | None = None,\n",
    "        model: str = \"grok-4-1-fast-non-reasoning\",\n",
    "        max_concurrent: int = DEFAULT_BATCH_SIZE,\n",
    "        min_batch_interval: float = DEFAULT_MIN_BATCH_INTERVAL,\n",
    "        timeout: int = 900,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Grok API client with the official xAI SDK.\n",
    "        \n",
    "        Args:\n",
    "            api_key: xAI API key. If None, uses XAI_API_KEY environment variable.\n",
    "            model: Model to use for generation (default: grok-4-1-fast-non-reasoning).\n",
    "            max_concurrent: Maximum concurrent in-flight requests (default: 20).\n",
    "            min_batch_interval: Minimum seconds between batch starts (default: 3.0).\n",
    "            timeout: Request timeout in seconds (default: 900, the SDK default).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.min_batch_interval = min_batch_interval\n",
    "        \n",
    "        # Initialize the official xAI AsyncClient\n",
    "        # The SDK reads XAI_API_KEY from environment if api_key is None\n",
    "        self.client = AsyncClient(\n",
    "            api_key=api_key or os.getenv(\"XAI_API_KEY\"),\n",
    "            timeout=timeout,\n",
    "        )\n",
    "        \n",
    "        # Semaphore controls maximum concurrent in-flight requests\n",
    "        self._semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        \n",
    "        # Track timing for rate limit compliance\n",
    "        self._last_batch_start: float = 0.0\n",
    "    \n",
    "    async def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Close the client and release resources.\n",
    "        \n",
    "        The xAI SDK AsyncClient manages its own connection lifecycle,\n",
    "        but we provide this method for explicit cleanup if needed.\n",
    "        \"\"\"\n",
    "        # The AsyncClient doesn't have an explicit close method,\n",
    "        # but we reset our state\n",
    "        self._last_batch_start = 0.0\n",
    "    \n",
    "    async def __aenter__(self) -> \"GrokClient\":\n",
    "        \"\"\"Async context manager entry.\"\"\"\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n",
    "        \"\"\"Async context manager exit.\"\"\"\n",
    "        await self.close()\n",
    "    \n",
    "    async def _generate_single(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1500,\n",
    "    ) -> dict[str, Any] | None:\n",
    "        \"\"\"\n",
    "        Generate a single response with semaphore-controlled concurrency.\n",
    "        \n",
    "        This method acquires the semaphore before making the request,\n",
    "        ensuring we never exceed max_concurrent simultaneous requests.\n",
    "        Uses the official xAI SDK chat interface.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt: System message with generation instructions.\n",
    "            user_prompt: User message with specific request.\n",
    "            temperature: Sampling temperature (0.0-1.0).\n",
    "            max_tokens: Maximum tokens in the response.\n",
    "        \n",
    "        Returns:\n",
    "            Parsed JSON response dict, or None if generation/parsing fails.\n",
    "        \"\"\"\n",
    "        async with self._semaphore:\n",
    "            try:\n",
    "                # Create a new chat instance with system message\n",
    "                chat = self.client.chat.create(\n",
    "                    model=self.model,\n",
    "                    messages=[system(system_prompt)],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                )\n",
    "                \n",
    "                # Append the user message\n",
    "                chat.append(user(user_prompt))\n",
    "                \n",
    "                # Sample a response (this is the actual API call)\n",
    "                response: Response = await chat.sample()\n",
    "                \n",
    "                # Extract the content from the response\n",
    "                raw_content: str = response.content\n",
    "                \n",
    "                # Parse JSON from the response content\n",
    "                return self._parse_json_response(raw_content)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Log error but don't crash - return None to indicate failure\n",
    "                print(f\"Generation error: {type(e).__name__}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    def _parse_json_response(self, raw_content: str) -> dict[str, Any] | None:\n",
    "        \"\"\"\n",
    "        Parse JSON from LLM response content.\n",
    "        \n",
    "        LLMs often wrap JSON in markdown code blocks or produce slightly\n",
    "        malformed JSON. This method handles common cases and uses json_repair\n",
    "        as a fallback for malformed output.\n",
    "        \n",
    "        Args:\n",
    "            raw_content: Raw string content from the LLM response.\n",
    "        \n",
    "        Returns:\n",
    "            Parsed JSON dict, or None if parsing fails completely.\n",
    "        \"\"\"\n",
    "        if not raw_content:\n",
    "            return None\n",
    "        \n",
    "        json_str = raw_content.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if \"```json\" in json_str:\n",
    "            # Extract content between ```json and ```\n",
    "            parts = json_str.split(\"```json\")\n",
    "            if len(parts) > 1:\n",
    "                json_str = parts[1].split(\"```\")[0]\n",
    "        elif \"```\" in json_str:\n",
    "            # Generic code block\n",
    "            parts = json_str.split(\"```\")\n",
    "            if len(parts) >= 2:\n",
    "                json_str = parts[1]\n",
    "        \n",
    "        json_str = json_str.strip()\n",
    "        \n",
    "        # Attempt standard JSON parsing first\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "        # Fall back to json_repair for malformed JSON\n",
    "        try:\n",
    "            return json_repair.loads(json_str)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    async def generate(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1500,\n",
    "    ) -> dict[str, Any] | None:\n",
    "        \"\"\"\n",
    "        Public interface for single generation.\n",
    "        \n",
    "        For high-throughput scenarios, use generate_batch() instead.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt: System message with generation instructions.\n",
    "            user_prompt: User message with specific request.\n",
    "            temperature: Sampling temperature.\n",
    "            max_tokens: Maximum tokens in response.\n",
    "        \n",
    "        Returns:\n",
    "            Parsed JSON response dict, or None on failure.\n",
    "        \"\"\"\n",
    "        return await self._generate_single(\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "    \n",
    "    async def generate_batch(\n",
    "        self,\n",
    "        requests: list[tuple[str, str, float]],\n",
    "        max_tokens: int = 1500,\n",
    "    ) -> list[dict[str, Any] | None]:\n",
    "        \"\"\"\n",
    "        Generate multiple responses concurrently with rate limit compliance.\n",
    "        \n",
    "        This method implements the pattern from the xAI async documentation:\n",
    "        1. Create tasks for all requests\n",
    "        2. Use semaphore to limit concurrent in-flight requests\n",
    "        3. Use asyncio.gather() to execute all tasks\n",
    "        4. Enforce minimum batch interval for rate limiting\n",
    "        \n",
    "        The semaphore ensures that even if you pass 100 requests, only\n",
    "        max_concurrent will be in-flight at any given moment.\n",
    "        \n",
    "        Args:\n",
    "            requests: List of (system_prompt, user_prompt, temperature) tuples.\n",
    "            max_tokens: Maximum tokens per response.\n",
    "        \n",
    "        Returns:\n",
    "            List of results in the same order as input requests.\n",
    "            Each result is either a parsed JSON dict or None on failure.\n",
    "        \"\"\"\n",
    "        # Enforce minimum time since last batch started\n",
    "        now = time.time()\n",
    "        elapsed_since_last_batch = now - self._last_batch_start\n",
    "        if elapsed_since_last_batch < self.min_batch_interval:\n",
    "            wait_time = self.min_batch_interval - elapsed_since_last_batch\n",
    "            await asyncio.sleep(wait_time)\n",
    "        \n",
    "        # Record batch start time\n",
    "        self._last_batch_start = time.time()\n",
    "        \n",
    "        # Create async task for each request\n",
    "        # The semaphore inside _generate_single controls actual concurrency\n",
    "        async def process_request(\n",
    "            system_prompt: str,\n",
    "            user_prompt: str,\n",
    "            temperature: float,\n",
    "        ) -> dict[str, Any] | None:\n",
    "            return await self._generate_single(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "        \n",
    "        # Build task list\n",
    "        tasks = [\n",
    "            process_request(system_prompt, user_prompt, temperature)\n",
    "            for system_prompt, user_prompt, temperature in requests\n",
    "        ]\n",
    "        \n",
    "        # Execute all tasks concurrently\n",
    "        # The semaphore limits how many are actually in-flight simultaneously\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Convert exceptions to None for consistent return type\n",
    "        processed_results: list[dict[str, Any] | None] = []\n",
    "        for result in results:\n",
    "            if isinstance(result, Exception):\n",
    "                print(f\"Task exception: {type(result).__name__}: {result}\")\n",
    "                processed_results.append(None)\n",
    "            else:\n",
    "                processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "\n",
    "\n",
    "# Initialize client using the official SDK\n",
    "grok_client = GrokClient(\n",
    "    api_key=XAI_API_KEY,\n",
    "    model=\"grok-4-1-fast-non-reasoning\",\n",
    "    max_concurrent=40,          # Max 40 in-flight requests at once\n",
    "    min_batch_interval=1.0,     # Wait at least 3 seconds between batches\n",
    "    timeout=900,                # 15 minute timeout (SDK default)\n",
    ")\n",
    "\n",
    "print(\"✓ Grok API client initialized using official xai_sdk.AsyncClient\")\n",
    "print(f\"  Model: {grok_client.model}\")\n",
    "print(f\"  Max concurrent requests: {grok_client.max_concurrent}\")\n",
    "print(f\"  Min batch interval: {grok_client.min_batch_interval}s\")\n",
    "print(f\"  Theoretical max throughput: {60 / grok_client.min_batch_interval * grok_client.max_concurrent:.0f} req/min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f254287",
   "metadata": {},
   "source": [
    "## Sample Generation and Validation Functions\n",
    "\n",
    "Core generation logic: creates individual samples, validates them, and handles retry logic for failed generations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7726de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generation functions defined\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def find_entity_span(text: str, entity_text: str, label: str) -> EntitySpan | None:\n",
    "    \"\"\"\n",
    "    Find the character span of an entity in text.\n",
    "    \n",
    "    Handles cases where the LLM might provide incorrect spans by\n",
    "    searching for the actual text position.\n",
    "    \n",
    "    Args:\n",
    "        text: The full text to search in\n",
    "        entity_text: The entity string to find\n",
    "        label: The entity label\n",
    "    \n",
    "    Returns:\n",
    "        EntitySpan if found, None otherwise\n",
    "    \"\"\"\n",
    "    start_idx = text.find(entity_text)\n",
    "    if start_idx == -1:\n",
    "        return None\n",
    "    \n",
    "    return EntitySpan(\n",
    "        start=start_idx,\n",
    "        end=start_idx + len(entity_text),\n",
    "        label=label,\n",
    "        text=entity_text,\n",
    "    )\n",
    "\n",
    "\n",
    "def repair_entity_spans(\n",
    "    text: str,\n",
    "    raw_entities: list[dict[str, Any]],\n",
    ") -> list[EntitySpan]:\n",
    "    \"\"\"\n",
    "    Repair and validate entity spans from LLM output.\n",
    "    \n",
    "    LLMs frequently produce incorrect character positions. This function:\n",
    "    1. Validates each span against the actual text\n",
    "    2. Attempts to find correct positions for misaligned entities\n",
    "    3. Filters out entities that cannot be located\n",
    "    \n",
    "    Args:\n",
    "        text: The generated text\n",
    "        raw_entities: List of entity dicts from LLM response\n",
    "    \n",
    "    Returns:\n",
    "        List of validated EntitySpan objects\n",
    "    \"\"\"\n",
    "    repaired: list[EntitySpan] = []\n",
    "    seen_spans: set[tuple[int, int]] = set()  # Avoid duplicates\n",
    "    \n",
    "    for entity_dict in raw_entities:\n",
    "        try:\n",
    "            start = entity_dict.get(\"start\", 0)\n",
    "            end = entity_dict.get(\"end\", 0)\n",
    "            label = entity_dict.get(\"label\", \"UNKNOWN\")\n",
    "            entity_text = entity_dict.get(\"text\", \"\")\n",
    "            \n",
    "            # First, check if provided span is correct\n",
    "            if 0 <= start < end <= len(text):\n",
    "                actual_text = text[start:end]\n",
    "                if actual_text == entity_text:\n",
    "                    # Span is correct\n",
    "                    span_key = (start, end)\n",
    "                    if span_key not in seen_spans:\n",
    "                        repaired.append(EntitySpan(\n",
    "                            start=start,\n",
    "                            end=end,\n",
    "                            label=label,\n",
    "                            text=entity_text,\n",
    "                        ))\n",
    "                        seen_spans.add(span_key)\n",
    "                    continue\n",
    "            \n",
    "            # Span is incorrect, try to find the text\n",
    "            if entity_text:\n",
    "                found_span = find_entity_span(text, entity_text, label)\n",
    "                if found_span:\n",
    "                    span_key = (found_span.start, found_span.end)\n",
    "                    if span_key not in seen_spans:\n",
    "                        repaired.append(found_span)\n",
    "                        seen_spans.add(span_key)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Skip malformed entities\n",
    "            continue\n",
    "    \n",
    "    return repaired\n",
    "\n",
    "\n",
    "async def generate_single_sample(\n",
    "    client: GrokClient,\n",
    "    dimension: str,\n",
    "    pii_type: str,\n",
    "    locale: str,\n",
    "    generation_id: str,\n",
    "    max_attempts: int = 3,\n",
    ") -> SyntheticSample | None:\n",
    "    \"\"\"\n",
    "    Generate a single synthetic sample with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        client: Initialized GrokClient\n",
    "        dimension: Target feature dimension\n",
    "        pii_type: Type of PII to generate\n",
    "        locale: Locale for PII formatting\n",
    "        generation_id: Unique ID for this generation\n",
    "        max_attempts: Max generation attempts before giving up\n",
    "    \n",
    "    Returns:\n",
    "        Validated SyntheticSample, or None if generation fails\n",
    "    \"\"\"\n",
    "    # Generate seed PII value\n",
    "    pii_value, actual_locale, label = pii_gen.generate_pii(pii_type, locale=locale)\n",
    "    \n",
    "    # Determine type variant based on PII type\n",
    "    type_variants = {\n",
    "        \"NAME\": [\"full name\", \"first name only\", \"formal with title\"],\n",
    "        \"EMAIL\": [\"personal\", \"work\", \"academic\"],\n",
    "        \"PHONE\": [\"mobile\", \"landline\", \"with extension\"],\n",
    "        \"CREDIT_CARD\": [\"Visa\", \"Mastercard\", \"Amex\"],\n",
    "        \"BANK_ACCOUNT\": [\"IBAN\", \"domestic\", \"SWIFT/BIC\"],\n",
    "    }\n",
    "    type_variant = random.choice(type_variants.get(pii_type, [\"standard\"]))\n",
    "    \n",
    "    # Get prompts\n",
    "    system_prompt, user_prompt = get_generation_prompt(\n",
    "        dimension=dimension,\n",
    "        pii_type=pii_type,\n",
    "        pii_value=pii_value,\n",
    "        locale=actual_locale,\n",
    "        type_variant=type_variant,\n",
    "    )\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            # Generate from LLM\n",
    "            response = await client.generate(\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt,\n",
    "                temperature=0.7 + (attempt * 0.1),  # Increase temp on retries\n",
    "            )\n",
    "            \n",
    "            if response is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract fields\n",
    "            text = response.get(\"text\", \"\")\n",
    "            raw_entities = response.get(\"entities\", [])\n",
    "            scenario = response.get(\"scenario\", \"Unspecified scenario\")\n",
    "            \n",
    "            if not text or not raw_entities:\n",
    "                continue\n",
    "            \n",
    "            # Verify seed PII is in text\n",
    "            if pii_value not in text:\n",
    "                # Try to find a close match (case-insensitive)\n",
    "                if pii_value.lower() not in text.lower():\n",
    "                    continue\n",
    "            \n",
    "            # Repair entity spans\n",
    "            entities = repair_entity_spans(text, raw_entities)\n",
    "            \n",
    "            if not entities:\n",
    "                continue\n",
    "            \n",
    "            # Create and validate sample\n",
    "            sample = SyntheticSample(\n",
    "                text=text,\n",
    "                entities=entities,\n",
    "                feature_dimension=FeatureDimension(dimension),\n",
    "                seed_pii_type=pii_type,\n",
    "                seed_pii_value=pii_value,\n",
    "                seed_pii_locale=actual_locale,\n",
    "                scenario=scenario,\n",
    "                type_variant=type_variant,\n",
    "                generation_id=generation_id,\n",
    "            )\n",
    "            \n",
    "            return sample\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_attempts - 1:\n",
    "                await asyncio.sleep(1)  # Brief pause before retry\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"✓ Generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd0e1b",
   "metadata": {},
   "source": [
    "## Batch Generation with Checkpointing\n",
    "\n",
    "Batch generation orchestration with concurrent processing, progress tracking, checkpointing, and balanced sampling\n",
    "across dimensions and PII types.\n",
    "\n",
    "This processes samples in batches of 20 concurrent requests, dramatically improving throughput compared to sequential\n",
    "processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a23b3089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SyntheticDataGenerator initialized with concurrent batch processing\n",
      "  Target: 11000 samples\n",
      "  Batch size: 40 concurrent requests\n",
      "  Per dimension: 1833 samples\n",
      "  Checkpoint every: 100 samples\n",
      "  Output directory: ./data/synthetic\n",
      "  Estimated completion time: ~16 minutes\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for synthetic data generation run.\n",
    "    \n",
    "    Attributes:\n",
    "        total_samples: Total number of samples to generate\n",
    "        samples_per_dimension: Samples per feature dimension (auto-calculated if 0)\n",
    "        batch_size: Number of concurrent requests per batch\n",
    "        samples_per_checkpoint: How often to save checkpoints\n",
    "        output_dir: Directory for output files and checkpoints\n",
    "        checkpoint_prefix: Prefix for checkpoint filenames\n",
    "    \"\"\"\n",
    "    total_samples: int = 11000\n",
    "    samples_per_dimension: int = 0  # 0 = auto-calculate\n",
    "    batch_size: int = 20  # Concurrent requests per batch\n",
    "    samples_per_checkpoint: int = 100\n",
    "    output_dir: str = \"./data/synthetic\"\n",
    "    checkpoint_prefix: str = \"synthetic_checkpoint\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.samples_per_dimension == 0:\n",
    "            self.samples_per_dimension = self.total_samples // len(FEATURE_DIMENSIONS)\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        Path(self.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class GenerationTask:\n",
    "    \"\"\"\n",
    "    A single generation task with all parameters needed.\n",
    "    \n",
    "    Attributes:\n",
    "        dimension: Target feature dimension\n",
    "        pii_type: Type of PII to generate\n",
    "        locale: Locale for PII formatting\n",
    "        generation_id: Unique identifier for this task\n",
    "        pii_value: Pre-generated seed PII value\n",
    "        actual_locale: Actual locale used (may differ if fallback)\n",
    "        label: PII label for the seed value\n",
    "        type_variant: Specific variant description\n",
    "        system_prompt: Complete system prompt\n",
    "        user_prompt: Complete user prompt\n",
    "    \"\"\"\n",
    "    dimension: str\n",
    "    pii_type: str\n",
    "    locale: str\n",
    "    generation_id: str\n",
    "    pii_value: str\n",
    "    actual_locale: str\n",
    "    label: str\n",
    "    type_variant: str\n",
    "    system_prompt: str\n",
    "    user_prompt: str\n",
    "\n",
    "\n",
    "class SyntheticDataGenerator:\n",
    "    \"\"\"\n",
    "    Orchestrates batch generation of synthetic PII data with concurrent processing.\n",
    "    \n",
    "    This generator processes samples in batches, firing multiple concurrent\n",
    "    requests to maximize throughput while respecting API rate limits.\n",
    "    \n",
    "    Features:\n",
    "        - Concurrent batch processing (default: 20 requests at a time)\n",
    "        - Balanced sampling across dimensions, PII types, and locales\n",
    "        - Periodic checkpointing to prevent data loss\n",
    "        - Progress tracking with accurate ETA estimation\n",
    "        - Automatic retry for failed generations within batches\n",
    "    \n",
    "    Attributes:\n",
    "        client: GrokClient for LLM generation\n",
    "        config: GenerationConfig with settings\n",
    "        generated_samples: List of all successfully generated samples\n",
    "        stats: Dictionary tracking generation statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client: GrokClient,\n",
    "        config: GenerationConfig | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the synthetic data generator.\n",
    "        \n",
    "        Args:\n",
    "            client: Initialized GrokClient with concurrency settings\n",
    "            config: Generation configuration (uses defaults if None)\n",
    "        \"\"\"\n",
    "        self.client = client\n",
    "        self.config = config or GenerationConfig()\n",
    "        self.generated_samples: list[SyntheticSample] = []\n",
    "        self.failed_tasks: list[GenerationTask] = []  # Track failures for potential retry\n",
    "        self.stats: dict[str, Any] = {\n",
    "            \"total_attempts\": 0,\n",
    "            \"successful\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"by_dimension\": defaultdict(int),\n",
    "            \"by_pii_type\": defaultdict(int),\n",
    "            \"by_locale\": defaultdict(int),\n",
    "            \"batches_processed\": 0,\n",
    "            \"start_time\": None,\n",
    "        }\n",
    "    \n",
    "    def _create_generation_task(\n",
    "        self,\n",
    "        dimension: str,\n",
    "        pii_type: str,\n",
    "        locale: str,\n",
    "    ) -> GenerationTask:\n",
    "        \"\"\"\n",
    "        Create a complete generation task with pre-generated PII and prompts.\n",
    "        \n",
    "        This front-loads all the work that doesn't require API calls,\n",
    "        so batch processing only involves the actual LLM requests.\n",
    "        \n",
    "        Args:\n",
    "            dimension: Target feature dimension\n",
    "            pii_type: Type of PII to generate\n",
    "            locale: Target locale for PII formatting\n",
    "        \n",
    "        Returns:\n",
    "            GenerationTask with all fields populated\n",
    "        \"\"\"\n",
    "        generation_id = f\"{dimension}_{pii_type}_{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "        # Generate seed PII value\n",
    "        pii_value, actual_locale, label = pii_gen.generate_pii(pii_type, locale=locale)\n",
    "        \n",
    "        # Determine type variant\n",
    "        type_variants = {\n",
    "            \"NAME\": [\"full name\", \"first name only\", \"formal with title\"],\n",
    "            \"EMAIL\": [\"personal\", \"work\", \"academic\"],\n",
    "            \"PHONE\": [\"mobile\", \"landline\", \"with extension\"],\n",
    "            \"CREDIT_CARD\": [\"Visa\", \"Mastercard\", \"Amex\"],\n",
    "            \"BANK_ACCOUNT\": [\"IBAN\", \"domestic\", \"SWIFT/BIC\"],\n",
    "        }\n",
    "        type_variant = random.choice(type_variants.get(pii_type, [\"standard\"]))\n",
    "        \n",
    "        # Build prompts\n",
    "        system_prompt, user_prompt = get_generation_prompt(\n",
    "            dimension=dimension,\n",
    "            pii_type=pii_type,\n",
    "            pii_value=pii_value,\n",
    "            locale=actual_locale,\n",
    "            type_variant=type_variant,\n",
    "        )\n",
    "        \n",
    "        return GenerationTask(\n",
    "            dimension=dimension,\n",
    "            pii_type=pii_type,\n",
    "            locale=locale,\n",
    "            generation_id=generation_id,\n",
    "            pii_value=pii_value,\n",
    "            actual_locale=actual_locale,\n",
    "            label=label,\n",
    "            type_variant=type_variant,\n",
    "            system_prompt=system_prompt,\n",
    "            user_prompt=user_prompt,\n",
    "        )\n",
    "    \n",
    "    def _get_generation_plan(self) -> list[tuple[str, str, str]]:\n",
    "        \"\"\"\n",
    "        Create a balanced generation plan across dimensions, PII types, and locales.\n",
    "        \n",
    "        Returns:\n",
    "            List of (dimension, pii_type, locale) tuples representing generation tasks\n",
    "        \"\"\"\n",
    "        plan: list[tuple[str, str, str]] = []\n",
    "        locales = list(SUPPORTED_LOCALES.keys())\n",
    "        \n",
    "        samples_per_dim = self.config.samples_per_dimension\n",
    "        samples_per_type_per_dim = max(1, samples_per_dim // len(ALL_PII_TYPES))\n",
    "        \n",
    "        for dimension in FEATURE_DIMENSIONS:\n",
    "            for pii_type in ALL_PII_TYPES:\n",
    "                for _ in range(samples_per_type_per_dim):\n",
    "                    locale = random.choice(locales)\n",
    "                    plan.append((dimension, pii_type, locale))\n",
    "        \n",
    "        # Shuffle to avoid sequential patterns and distribute load\n",
    "        random.shuffle(plan)\n",
    "        \n",
    "        return plan[:self.config.total_samples]\n",
    "    \n",
    "    def _process_response(\n",
    "        self,\n",
    "        task: GenerationTask,\n",
    "        response: dict[str, Any] | None,\n",
    "    ) -> SyntheticSample | None:\n",
    "        \"\"\"\n",
    "        Process an LLM response into a validated SyntheticSample.\n",
    "        \n",
    "        Args:\n",
    "            task: The generation task that produced this response\n",
    "            response: Parsed JSON response from the LLM, or None on failure\n",
    "        \n",
    "        Returns:\n",
    "            Validated SyntheticSample, or None if validation fails\n",
    "        \"\"\"\n",
    "        if response is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            text = response.get(\"text\", \"\")\n",
    "            raw_entities = response.get(\"entities\", [])\n",
    "            scenario = response.get(\"scenario\", \"Unspecified scenario\")\n",
    "            \n",
    "            if not text or not raw_entities:\n",
    "                return None\n",
    "            \n",
    "            # Verify seed PII is in text\n",
    "            if task.pii_value not in text:\n",
    "                if task.pii_value.lower() not in text.lower():\n",
    "                    return None\n",
    "            \n",
    "            # Repair entity spans (LLMs often get positions wrong)\n",
    "            entities = repair_entity_spans(text, raw_entities)\n",
    "            \n",
    "            if not entities:\n",
    "                return None\n",
    "            \n",
    "            # Create and validate sample using Pydantic\n",
    "            sample = SyntheticSample(\n",
    "                text=text,\n",
    "                entities=entities,\n",
    "                feature_dimension=FeatureDimension(task.dimension),\n",
    "                seed_pii_type=task.pii_type,\n",
    "                seed_pii_value=task.pii_value,\n",
    "                seed_pii_locale=task.actual_locale,\n",
    "                scenario=scenario,\n",
    "                type_variant=task.type_variant,\n",
    "                generation_id=task.generation_id,\n",
    "            )\n",
    "            \n",
    "            return sample\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    async def _process_batch(\n",
    "        self,\n",
    "        tasks: list[GenerationTask],\n",
    "    ) -> list[tuple[GenerationTask, SyntheticSample | None]]:\n",
    "        \"\"\"\n",
    "        Process a batch of generation tasks concurrently.\n",
    "        \n",
    "        This fires all requests in the batch simultaneously (respecting\n",
    "        the client's concurrency limit) and waits for all to complete.\n",
    "        \n",
    "        Args:\n",
    "            tasks: List of GenerationTask objects to process\n",
    "        \n",
    "        Returns:\n",
    "            List of (task, sample_or_none) tuples in the same order\n",
    "        \"\"\"\n",
    "        # Prepare request tuples for the client\n",
    "        requests = [\n",
    "            (task.system_prompt, task.user_prompt, 0.7)\n",
    "            for task in tasks\n",
    "        ]\n",
    "        \n",
    "        # Fire all requests concurrently\n",
    "        responses = await self.client.generate_batch(requests)\n",
    "        \n",
    "        # Process responses into samples\n",
    "        results: list[tuple[GenerationTask, SyntheticSample | None]] = []\n",
    "        for task, response in zip(tasks, responses):\n",
    "            sample = self._process_response(task, response)\n",
    "            results.append((task, sample))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _save_checkpoint(self, checkpoint_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Save current progress to a checkpoint file.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_num: Checkpoint sequence number\n",
    "        \n",
    "        Returns:\n",
    "            Path to saved checkpoint file\n",
    "        \"\"\"\n",
    "        checkpoint_path = (\n",
    "            Path(self.config.output_dir) / \n",
    "            f\"{self.config.checkpoint_prefix}_{checkpoint_num:04d}.json\"\n",
    "        )\n",
    "        \n",
    "        # Convert defaultdicts to regular dicts for JSON serialization\n",
    "        stats_copy = {\n",
    "            k: (dict(v) if isinstance(v, defaultdict) else v)\n",
    "            for k, v in self.stats.items()\n",
    "        }\n",
    "        \n",
    "        checkpoint_data = {\n",
    "            \"checkpoint_num\": checkpoint_num,\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"stats\": stats_copy,\n",
    "            \"samples\": [s.model_dump() for s in self.generated_samples],\n",
    "            \"failed_task_count\": len(self.failed_tasks),\n",
    "        }\n",
    "        \n",
    "        with open(checkpoint_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return str(checkpoint_path)\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path: str) -> int:\n",
    "        \"\"\"\n",
    "        Load progress from a checkpoint file.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to checkpoint file\n",
    "        \n",
    "        Returns:\n",
    "            Number of samples loaded\n",
    "        \"\"\"\n",
    "        with open(checkpoint_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Restore stats with defaultdict behavior\n",
    "        loaded_stats = data.get(\"stats\", {})\n",
    "        self.stats = {\n",
    "            \"total_attempts\": loaded_stats.get(\"total_attempts\", 0),\n",
    "            \"successful\": loaded_stats.get(\"successful\", 0),\n",
    "            \"failed\": loaded_stats.get(\"failed\", 0),\n",
    "            \"by_dimension\": defaultdict(int, loaded_stats.get(\"by_dimension\", {})),\n",
    "            \"by_pii_type\": defaultdict(int, loaded_stats.get(\"by_pii_type\", {})),\n",
    "            \"by_locale\": defaultdict(int, loaded_stats.get(\"by_locale\", {})),\n",
    "            \"batches_processed\": loaded_stats.get(\"batches_processed\", 0),\n",
    "            \"start_time\": loaded_stats.get(\"start_time\"),\n",
    "        }\n",
    "        \n",
    "        self.generated_samples = [\n",
    "            SyntheticSample.model_validate(s) \n",
    "            for s in data.get(\"samples\", [])\n",
    "        ]\n",
    "        \n",
    "        print(f\"✓ Loaded {len(self.generated_samples)} samples from checkpoint\")\n",
    "        return len(self.generated_samples)\n",
    "    \n",
    "    async def generate_batch(\n",
    "        self,\n",
    "        start_from: int = 0,\n",
    "        progress_bar: bool = True,\n",
    "    ) -> list[SyntheticSample]:\n",
    "        \"\"\"\n",
    "        Generate a full batch of synthetic samples using concurrent processing.\n",
    "        \n",
    "        This method processes samples in batches of config.batch_size,\n",
    "        firing concurrent requests and waiting for each batch to complete\n",
    "        before starting the next.\n",
    "        \n",
    "        Args:\n",
    "            start_from: Index to start from (for resuming from checkpoint)\n",
    "            progress_bar: Whether to display a progress bar\n",
    "        \n",
    "        Returns:\n",
    "            List of all successfully generated samples\n",
    "        \"\"\"\n",
    "        # Get the full generation plan\n",
    "        plan = self._get_generation_plan()\n",
    "        \n",
    "        if start_from > 0:\n",
    "            plan = plan[start_from:]\n",
    "            print(f\"Resuming from sample {start_from}\")\n",
    "        \n",
    "        self.stats[\"start_time\"] = time.time()\n",
    "        \n",
    "        # Create all tasks upfront (this is fast, no API calls)\n",
    "        print(\"Preparing generation tasks...\")\n",
    "        all_tasks = [\n",
    "            self._create_generation_task(dimension, pii_type, locale)\n",
    "            for dimension, pii_type, locale in plan\n",
    "        ]\n",
    "        \n",
    "        # Split into batches\n",
    "        batch_size = self.config.batch_size\n",
    "        batches = [\n",
    "            all_tasks[i:i + batch_size] \n",
    "            for i in range(0, len(all_tasks), batch_size)\n",
    "        ]\n",
    "        \n",
    "        total_batches = len(batches)\n",
    "        samples_since_checkpoint = len(self.generated_samples) % self.config.samples_per_checkpoint\n",
    "        \n",
    "        print(f\"Processing {len(all_tasks)} tasks in {total_batches} batches of {batch_size}\")\n",
    "        \n",
    "        # Process batches with progress tracking\n",
    "        batch_iterator = tqdm(\n",
    "            enumerate(batches),\n",
    "            total=total_batches,\n",
    "            desc=\"Processing batches\",\n",
    "            disable=not progress_bar,\n",
    "        )\n",
    "        \n",
    "        for batch_idx, batch_tasks in batch_iterator:\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Process this batch concurrently\n",
    "            results = await self._process_batch(batch_tasks)\n",
    "            \n",
    "            # Update stats and collect samples\n",
    "            for task, sample in results:\n",
    "                self.stats[\"total_attempts\"] += 1\n",
    "                \n",
    "                if sample is not None:\n",
    "                    self.generated_samples.append(sample)\n",
    "                    self.stats[\"successful\"] += 1\n",
    "                    self.stats[\"by_dimension\"][task.dimension] += 1\n",
    "                    self.stats[\"by_pii_type\"][task.pii_type] += 1\n",
    "                    self.stats[\"by_locale\"][task.actual_locale] += 1\n",
    "                    samples_since_checkpoint += 1\n",
    "                else:\n",
    "                    self.stats[\"failed\"] += 1\n",
    "                    self.failed_tasks.append(task)\n",
    "            \n",
    "            self.stats[\"batches_processed\"] += 1\n",
    "            \n",
    "            # Calculate metrics for progress display\n",
    "            batch_elapsed = time.time() - batch_start_time\n",
    "            success_rate = self.stats[\"successful\"] / self.stats[\"total_attempts\"]\n",
    "            samples_per_second = len(batch_tasks) / batch_elapsed if batch_elapsed > 0 else 0\n",
    "            \n",
    "            batch_iterator.set_postfix({\n",
    "                \"success\": f\"{self.stats['successful']}/{self.stats['total_attempts']}\",\n",
    "                \"rate\": f\"{success_rate:.1%}\",\n",
    "                \"speed\": f\"{samples_per_second:.1f}/s\",\n",
    "            })\n",
    "            \n",
    "            # Checkpoint if needed\n",
    "            if samples_since_checkpoint >= self.config.samples_per_checkpoint:\n",
    "                cp_num = len(self.generated_samples) // self.config.samples_per_checkpoint\n",
    "                cp_path = self._save_checkpoint(cp_num)\n",
    "                batch_iterator.write(f\"  💾 Checkpoint {cp_num}: {cp_path}\")\n",
    "                samples_since_checkpoint = 0\n",
    "        \n",
    "        # Final checkpoint\n",
    "        self._save_checkpoint(9999)\n",
    "        \n",
    "        return self.generated_samples\n",
    "    \n",
    "    def get_statistics(self) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get comprehensive generation statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with generation metrics and breakdowns\n",
    "        \"\"\"\n",
    "        elapsed = time.time() - self.stats[\"start_time\"] if self.stats[\"start_time\"] else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_generated\": len(self.generated_samples),\n",
    "            \"total_attempts\": self.stats[\"total_attempts\"],\n",
    "            \"total_failed\": self.stats[\"failed\"],\n",
    "            \"success_rate\": (\n",
    "                self.stats[\"successful\"] / self.stats[\"total_attempts\"]\n",
    "                if self.stats[\"total_attempts\"] > 0 else 0\n",
    "            ),\n",
    "            \"batches_processed\": self.stats[\"batches_processed\"],\n",
    "            \"elapsed_seconds\": elapsed,\n",
    "            \"samples_per_second\": len(self.generated_samples) / elapsed if elapsed > 0 else 0,\n",
    "            \"by_dimension\": dict(self.stats[\"by_dimension\"]),\n",
    "            \"by_pii_type\": dict(self.stats[\"by_pii_type\"]),\n",
    "            \"by_locale\": dict(self.stats[\"by_locale\"]),\n",
    "        }\n",
    "    \n",
    "    def save_final_dataset(self, filename: str = \"synthetic_pii_data.json\") -> str:\n",
    "        \"\"\"\n",
    "        Save the complete dataset to a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            filename: Output filename\n",
    "        \n",
    "        Returns:\n",
    "            Path to saved file\n",
    "        \"\"\"\n",
    "        output_path = Path(self.config.output_dir) / filename\n",
    "        \n",
    "        dataset = {\n",
    "            \"metadata\": {\n",
    "                \"generated_at\": datetime.utcnow().isoformat(),\n",
    "                \"total_samples\": len(self.generated_samples),\n",
    "                \"dimensions\": FEATURE_DIMENSIONS,\n",
    "                \"pii_types\": ALL_PII_TYPES,\n",
    "                \"locales\": list(SUPPORTED_LOCALES.keys()),\n",
    "            },\n",
    "            \"statistics\": self.get_statistics(),\n",
    "            \"samples\": [s.model_dump() for s in self.generated_samples],\n",
    "        }\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"✓ Dataset saved to: {output_path}\")\n",
    "        return str(output_path)\n",
    "\n",
    "\n",
    "# Initialize generator with concurrent processing config\n",
    "config = GenerationConfig(\n",
    "    total_samples=11000,\n",
    "    batch_size=40,  # 40 concurrent requests per batch\n",
    "    samples_per_checkpoint=100,\n",
    "    output_dir=\"./data/synthetic\",\n",
    ")\n",
    "\n",
    "generator = SyntheticDataGenerator(\n",
    "    client=grok_client,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"✓ SyntheticDataGenerator initialized with concurrent batch processing\")\n",
    "print(f\"  Target: {config.total_samples} samples\")\n",
    "print(f\"  Batch size: {config.batch_size} concurrent requests\")\n",
    "print(f\"  Per dimension: {config.samples_per_dimension} samples\")\n",
    "print(f\"  Checkpoint every: {config.samples_per_checkpoint} samples\")\n",
    "print(f\"  Output directory: {config.output_dir}\")\n",
    "\n",
    "# Estimate completion time\n",
    "# With 20 concurrent requests, ~3 second batches, that's ~400 requests/min\n",
    "# 11,000 samples / 400 per min ≈ 27.5 minutes (plus overhead)\n",
    "estimated_batches = config.total_samples // config.batch_size\n",
    "estimated_time_minutes = estimated_batches * 3.5 / 60  # 3.5 sec per batch average\n",
    "print(f\"  Estimated completion time: ~{estimated_time_minutes:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b1023",
   "metadata": {},
   "source": [
    "## Quick Test Generation\n",
    "\n",
    "Quick test: Generate a small batch to verify everything works before running the full 11,000 sample generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd6e41bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test generation with 10 samples...\n",
      "Preparing generation tasks...\n",
      "Processing 10 tasks in 1 batches of 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfb6a10a2b6487290a3a16e6c241804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test complete: 10 samples generated\n",
      "\n",
      "First sample:\n",
      "  Dimension: basic\n",
      "  Text: Dear Mr. Rossi,\n",
      "\n",
      "Thank you for registering for the conference. Here is your confirmation:\n",
      "\n",
      "Name: Marco Rossi\n",
      "Email: silvestro28@example.com (work)\n",
      "Pho...\n",
      "  Entities: 4\n",
      "    - EMAIL: 'silvestro28@example.com' [116:139]\n",
      "    - PERSON: 'Marco Rossi' [97:108]\n",
      "    - PHONE: '+39 333 1234567' [154:169]\n",
      "    - ADDRESS: 'Via Roma 45, 20121 Milano' [179:204]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aritr\\AppData\\Local\\Temp\\ipykernel_5200\\1235122109.py:73: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n",
      "C:\\Users\\aritr\\AppData\\Local\\Temp\\ipykernel_5200\\1920566619.py:298: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "async def test_generation(num_samples: int = 10) -> list[SyntheticSample]:\n",
    "    \"\"\"\n",
    "    Run a quick test generation with a small number of samples.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of test samples to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of generated test samples\n",
    "    \"\"\"\n",
    "    print(f\"Running test generation with {num_samples} samples...\")\n",
    "    \n",
    "    test_config = GenerationConfig(\n",
    "        total_samples=num_samples,\n",
    "        samples_per_checkpoint=num_samples + 1,  # No checkpoints for test\n",
    "        output_dir=\"./data/synthetic_test\",\n",
    "    )\n",
    "    \n",
    "    test_generator = SyntheticDataGenerator(\n",
    "        client=grok_client,\n",
    "        config=test_config,\n",
    "    )\n",
    "    \n",
    "    test_samples = await test_generator.generate_batch(progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nTest complete: {len(test_samples)} samples generated\")\n",
    "    \n",
    "    if test_samples:\n",
    "        print(\"\\nFirst sample:\")\n",
    "        s = test_samples[0]\n",
    "        print(f\"  Dimension: {s.feature_dimension.value}\")\n",
    "        print(f\"  Text: {s.text[:150]}...\")\n",
    "        print(f\"  Entities: {len(s.entities)}\")\n",
    "        for e in s.entities:\n",
    "            print(f\"    - {e.label}: '{e.text}' [{e.start}:{e.end}]\")\n",
    "    \n",
    "    return test_samples\n",
    "\n",
    "\n",
    "# Run quick test (comment out for full generation)\n",
    "test_samples = await test_generation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3e2cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SyntheticSample(text='Dear Mr. Rossi,\\n\\nThank you for registering for the conference. Here is your confirmation:\\n\\nName: Marco Rossi\\nEmail: silvestro28@example.com (work)\\nPhone: +39 333 1234567\\nAddress: Via Roma 45, 20121 Milano\\n\\nWe look forward to seeing you!\\n\\nBest regards,\\nConference Team', entities=[EntitySpan(start=116, end=139, label='EMAIL', text='silvestro28@example.com'), EntitySpan(start=97, end=108, label='PERSON', text='Marco Rossi'), EntitySpan(start=154, end=169, label='PHONE', text='+39 333 1234567'), EntitySpan(start=179, end=204, label='ADDRESS', text='Via Roma 45, 20121 Milano')], feature_dimension=<FeatureDimension.BASIC: 'basic'>, seed_pii_type='EMAIL', seed_pii_value='silvestro28@example.com', seed_pii_locale='it_IT', scenario='Email confirmation for conference registration with contact details', type_variant='work', generation_id='basic_EMAIL_e94f71d6', timestamp='2025-12-03T23:09:38.691184'), SyntheticSample(text='DEBUG: User auth failed for IT citizen. ID: U28394359 (Codice Fiscale equiv). Name: Rossi Mario Giuseppe. DOB: 12/05/1985. Address: Via Roma 45, Milano, Lombardia 20121 IT. Check log: \"U28394359\" vs DB mismatch. Phone:+39-02-1234567. Retrying auth...', entities=[EntitySpan(start=44, end=53, label='OTHER_NATIONAL_IDENTITY', text='U28394359'), EntitySpan(start=84, end=104, label='PERSON_FULL_NAME', text='Rossi Mario Giuseppe'), EntitySpan(start=111, end=121, label='DATE_OF_BIRTH', text='12/05/1985'), EntitySpan(start=132, end=171, label='ADDRESS', text='Via Roma 45, Milano, Lombardia 20121 IT'), EntitySpan(start=218, end=232, label='PHONE_NUMBER', text='+39-02-1234567')], feature_dimension=<FeatureDimension.ADVERSARIAL: 'adversarial'>, seed_pii_type='OTHER_NATIONAL_IDENTITY', seed_pii_value='U28394359', seed_pii_locale='it_IT', scenario='Debug log from Italian banking authentication system showing failed user verification with embedded national ID in error message and quoted duplicate for evasion testing', type_variant='standard', generation_id='adversarial_OTHER_NATIONAL_IDENTITY_fe5f544b', timestamp='2025-12-03T23:09:38.691414'), SyntheticSample(text='Hola Juan, gracias por verificar tu identidad para la transferencia a tu wallet de Bitcoin. Tu DNI es I73824384, que coincide con nuestros registros. El código 2FA que enviamos a tu móvil 34612873945 es 927481. Confirma la transacción a la dirección bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh desde tu UPI netflix@paytm para proceder. Saludos del equipo fintech.', entities=[EntitySpan(start=102, end=111, label='OTHER_NATIONAL_IDENTITY', text='I73824384'), EntitySpan(start=188, end=199, label='PHONE_NUMBER', text='34612873945'), EntitySpan(start=203, end=209, label='TWO_FACTOR_CODE', text='927481'), EntitySpan(start=250, end=292, label='CRYPTO_WALLET', text='bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh'), EntitySpan(start=306, end=319, label='UPI_ID', text='netflix@paytm')], feature_dimension=<FeatureDimension.EVOLVING: 'evolving'>, seed_pii_type='OTHER_NATIONAL_IDENTITY', seed_pii_value='I73824384', seed_pii_locale='es_ES', scenario='Customer support email in Spanish fintech app verifying identity for crypto transfer', type_variant='standard', generation_id='evolving_OTHER_NATIONAL_IDENTITY_6fd8ca2a', timestamp='2025-12-03T23:09:38.691525'), SyntheticSample(text='Kontobestätigung: Sehr geehrte Frau Anna Schmidt, Ihr Geburtsdatum ist korrekt als 22/05/1974 erfasst. Name: Anna Schmidt, Adresse: Musterstraße 12, 80331 München, Telefon: +49 89 1234567, Versichertennummer: 1234567890.', entities=[EntitySpan(start=83, end=93, label='DATE_OF_BIRTH', text='22/05/1974'), EntitySpan(start=36, end=48, label='PERSON', text='Anna Schmidt'), EntitySpan(start=132, end=162, label='ADDRESS', text='Musterstraße 12, 80331 München'), EntitySpan(start=173, end=187, label='PHONE', text='+49 89 1234567'), EntitySpan(start=209, end=219, label='INSURANCE', text='1234567890')], feature_dimension=<FeatureDimension.BASIC: 'basic'>, seed_pii_type='DATE_OF_BIRTH', seed_pii_value='22/05/1974', seed_pii_locale='de_DE', scenario='Bank account confirmation email excerpt with personal and contact details', type_variant='standard', generation_id='basic_DATE_OF_BIRTH_987c4faf', timestamp='2025-12-03T23:09:38.691577'), SyntheticSample(text='hey maria, quick chk on ur latest xfer. accnt IT59H9311582875461149318788 didnt go thru. verify w/ cust ID  FR45678901234 and phne  +39-055-1234567. sry 4 delay, systm glitch? pls cnf rm ths ASAP. thx - supprt team (OCR scan frm old recrd)', entities=[EntitySpan(start=46, end=73, label='BANK_ACCOUNT', text='IT59H9311582875461149318788'), EntitySpan(start=108, end=121, label='CUSTOMER_ID', text='FR45678901234'), EntitySpan(start=132, end=147, label='PHONE', text='+39-055-1234567'), EntitySpan(start=4, end=9, label='PERSON', text='maria')], feature_dimension=<FeatureDimension.NOISY: 'noisy'>, seed_pii_type='BANK_ACCOUNT', seed_pii_value='IT59H9311582875461149318788', seed_pii_locale='it_IT', scenario='Hastily typed customer support chat with typos, abbreviations, and OCR artifacts from scanned records', type_variant='domestic', generation_id='noisy_BANK_ACCOUNT_b15a9e38', timestamp='2025-12-03T23:09:38.691623'), SyntheticSample(text='Dear Ms. Murphy, thank you for your visa application to the UK. We have received your details: full name Quinn Murphy (email: qmurphy@example.org), date of birth 15/03/1985, passport number 7054321987, UK National Insurance number QJ 56 12 90 B, and current address Flat 12B, 147 High Street, Birmingham B12 9XY. Your application reference is UKV-2024-56789. Please contact us at +44 20 7946 0000 if needed.', entities=[EntitySpan(start=126, end=145, label='EMAIL', text='qmurphy@example.org'), EntitySpan(start=162, end=172, label='DATE', text='15/03/1985'), EntitySpan(start=190, end=200, label='PASSPORT', text='7054321987'), EntitySpan(start=231, end=244, label='UK_NI_NUMBER', text='QJ 56 12 90 B'), EntitySpan(start=304, end=311, label='UK_POSTCODE', text='B12 9XY')], feature_dimension=<FeatureDimension.MULTILINGUAL: 'multilingual'>, seed_pii_type='EMAIL', seed_pii_value='qmurphy@example.org', seed_pii_locale='en_US', scenario='UK visa application confirmation email to an applicant', type_variant='personal', generation_id='multilingual_EMAIL_3c4016f1', timestamp='2025-12-03T23:09:38.691668'), SyntheticSample(text='Dear Mr. Thompson, thank you for submitting your visa application. Your UK National Insurance number R82345817 has been verified against HMRC records. Please provide your passport number 472910273 and current address: 14 Oakwood Close, Bristol BS9 2JT. Contact us at +44 117 234 5678 if you need to update your date of birth 15/03/1985 or IBAN GB29 RBKC 6016 9733 8392 16 for direct deposit.', entities=[EntitySpan(start=101, end=110, label='OTHER_NATIONAL_IDENTITY', text='R82345817'), EntitySpan(start=187, end=196, label='PASSPORT_NUMBER', text='472910273'), EntitySpan(start=244, end=251, label='UK_POSTCODE', text='BS9 2JT'), EntitySpan(start=267, end=283, label='PHONE_NUMBER', text='+44 117 234 5678'), EntitySpan(start=325, end=335, label='DATE_OF_BIRTH', text='15/03/1985')], feature_dimension=<FeatureDimension.MULTILINGUAL: 'multilingual'>, seed_pii_type='OTHER_NATIONAL_IDENTITY', seed_pii_value='R82345817', seed_pii_locale='en_GB', scenario='UK visa application confirmation email with applicant verification details', type_variant='standard', generation_id='multilingual_OTHER_NATIONAL_IDENTITY_f64302b4', timestamp='2025-12-03T23:09:38.691711'), SyntheticSample(text='hey support, i cant send money to mekhalakumar@paytm  its sayin invalid UPI! my acc no is 1245678912345678 from HDFC, phone 98765 43210. chk my ifsc HDFC0001234 pls fix asap. thx -Rajesh (rajeshpatel91@gma1l.com)', entities=[EntitySpan(start=34, end=52, label='BANK_UPI_ID', text='mekhalakumar@paytm'), EntitySpan(start=90, end=106, label='BANK_ACCOUNT_NUMBER', text='1245678912345678'), EntitySpan(start=149, end=160, label='BANK_IFSC', text='HDFC0001234'), EntitySpan(start=188, end=211, label='EMAIL_ADDRESS', text='rajeshpatel91@gma1l.com')], feature_dimension=<FeatureDimension.NOISY: 'noisy'>, seed_pii_type='BANK_UPI_ID', seed_pii_value='mekhalakumar@paytm', seed_pii_locale='en_IN', scenario='Hastily typed customer support chat message complaining about a UPI transfer issue, with typos, abbreviations, and OCR-like email error', type_variant='standard', generation_id='noisy_BANK_UPI_ID_1da6368c', timestamp='2025-12-03T23:09:38.691746'), SyntheticSample(text='Dear Mr. Karl Lawson,\\n\\nThank you for registering with our service. Your account details are confirmed below:\\n\\nFull Name: Mr. Karl Lawson\\nEmail: karl.lawson@email.com\\nPhone: (555) 123-4567\\nAddress: 123 Oak Street, Springfield, IL 62701\\n\\nBest regards,\\nCustomer Service Team', entities=[EntitySpan(start=9, end=20, label='NAME', text='Karl Lawson'), EntitySpan(start=144, end=165, label='EMAIL', text='karl.lawson@email.com'), EntitySpan(start=173, end=187, label='PHONE', text='(555) 123-4567'), EntitySpan(start=197, end=234, label='ADDRESS', text='123 Oak Street, Springfield, IL 62701')], feature_dimension=<FeatureDimension.BASIC: 'basic'>, seed_pii_type='NAME', seed_pii_value='Karl Lawson', seed_pii_locale='en_US', scenario='Customer registration confirmation email with contact details', type_variant='formal with title', generation_id='basic_NAME_0ac79856', timestamp='2025-12-03T23:09:38.691775'), SyntheticSample(text='Policy Confirmation Details:\\n\\nPolicyholder: Johnathan R. Mitchell\\nInsurance Number: POL-7745702107\\nDate of Birth: 15/08/1972\\nPhone: +61 412 345 678\\nAddress: 123 Koala Crescent, Sydney NSW 2000\\n\\nThank you for your recent policy update. Please keep this reference for future claims.', entities=[EntitySpan(start=84, end=98, label='INSURANCE_NUMBER', text='POL-7745702107'), EntitySpan(start=44, end=65, label='NAME', text='Johnathan R. Mitchell'), EntitySpan(start=114, end=124, label='DATE_OF_BIRTH', text='15/08/1972'), EntitySpan(start=132, end=147, label='PHONE_NUMBER', text='+61 412 345 678')], feature_dimension=<FeatureDimension.BASIC: 'basic'>, seed_pii_type='INSURANCE_NUMBER', seed_pii_value='POL-7745702107', seed_pii_locale='en_AU', scenario='Insurance policy confirmation email excerpt with holder details', type_variant='standard', generation_id='basic_INSURANCE_NUMBER_9e218750', timestamp='2025-12-03T23:09:38.691817')]\n"
     ]
    }
   ],
   "source": [
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c180d5f0",
   "metadata": {},
   "source": [
    "## Run Generation\n",
    "\n",
    "Main execution cell - runs the full generation process.\n",
    "\n",
    "WARNING: This will make ~11,000+ API calls to xAI. Ensure you have:\n",
    "\n",
    "1. Sufficient API credits\n",
    "2. Stable internet connection\n",
    "3. Time for completion\n",
    "\n",
    "To resume from a checkpoint, uncomment the load_checkpoint line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07cb6262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYNTHETIC PII DATA GENERATION\n",
      "============================================================\n",
      "Start time: 2025-12-03T15:09:58.386096\n",
      "Target samples: 11000\n",
      "\n",
      "Preparing generation tasks...\n",
      "Processing 10944 tasks in 274 batches of 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e74e6450cc431684bdca183b8abf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aritr\\AppData\\Local\\Temp\\ipykernel_5200\\1235122109.py:73: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n",
      "C:\\Users\\aritr\\AppData\\Local\\Temp\\ipykernel_5200\\1920566619.py:298: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  💾 Checkpoint 1: data\\synthetic\\synthetic_checkpoint_0001.json\n",
      "  💾 Checkpoint 2: data\\synthetic\\synthetic_checkpoint_0002.json\n",
      "  💾 Checkpoint 3: data\\synthetic\\synthetic_checkpoint_0003.json\n",
      "  💾 Checkpoint 4: data\\synthetic\\synthetic_checkpoint_0004.json\n",
      "  💾 Checkpoint 5: data\\synthetic\\synthetic_checkpoint_0005.json\n",
      "  💾 Checkpoint 7: data\\synthetic\\synthetic_checkpoint_0007.json\n",
      "  💾 Checkpoint 8: data\\synthetic\\synthetic_checkpoint_0008.json\n",
      "  💾 Checkpoint 9: data\\synthetic\\synthetic_checkpoint_0009.json\n",
      "  💾 Checkpoint 10: data\\synthetic\\synthetic_checkpoint_0010.json\n",
      "  💾 Checkpoint 11: data\\synthetic\\synthetic_checkpoint_0011.json\n",
      "  💾 Checkpoint 13: data\\synthetic\\synthetic_checkpoint_0013.json\n",
      "  💾 Checkpoint 14: data\\synthetic\\synthetic_checkpoint_0014.json\n",
      "  💾 Checkpoint 15: data\\synthetic\\synthetic_checkpoint_0015.json\n",
      "  💾 Checkpoint 16: data\\synthetic\\synthetic_checkpoint_0016.json\n",
      "  💾 Checkpoint 17: data\\synthetic\\synthetic_checkpoint_0017.json\n",
      "  💾 Checkpoint 19: data\\synthetic\\synthetic_checkpoint_0019.json\n",
      "  💾 Checkpoint 20: data\\synthetic\\synthetic_checkpoint_0020.json\n",
      "  💾 Checkpoint 21: data\\synthetic\\synthetic_checkpoint_0021.json\n",
      "  💾 Checkpoint 22: data\\synthetic\\synthetic_checkpoint_0022.json\n",
      "  💾 Checkpoint 23: data\\synthetic\\synthetic_checkpoint_0023.json\n",
      "  💾 Checkpoint 25: data\\synthetic\\synthetic_checkpoint_0025.json\n",
      "  💾 Checkpoint 26: data\\synthetic\\synthetic_checkpoint_0026.json\n",
      "  💾 Checkpoint 27: data\\synthetic\\synthetic_checkpoint_0027.json\n",
      "  💾 Checkpoint 28: data\\synthetic\\synthetic_checkpoint_0028.json\n",
      "  💾 Checkpoint 29: data\\synthetic\\synthetic_checkpoint_0029.json\n",
      "  💾 Checkpoint 31: data\\synthetic\\synthetic_checkpoint_0031.json\n",
      "  💾 Checkpoint 32: data\\synthetic\\synthetic_checkpoint_0032.json\n",
      "  💾 Checkpoint 33: data\\synthetic\\synthetic_checkpoint_0033.json\n",
      "  💾 Checkpoint 34: data\\synthetic\\synthetic_checkpoint_0034.json\n",
      "  💾 Checkpoint 35: data\\synthetic\\synthetic_checkpoint_0035.json\n",
      "  💾 Checkpoint 36: data\\synthetic\\synthetic_checkpoint_0036.json\n",
      "  💾 Checkpoint 38: data\\synthetic\\synthetic_checkpoint_0038.json\n",
      "  💾 Checkpoint 39: data\\synthetic\\synthetic_checkpoint_0039.json\n",
      "  💾 Checkpoint 40: data\\synthetic\\synthetic_checkpoint_0040.json\n",
      "  💾 Checkpoint 41: data\\synthetic\\synthetic_checkpoint_0041.json\n",
      "  💾 Checkpoint 42: data\\synthetic\\synthetic_checkpoint_0042.json\n",
      "  💾 Checkpoint 44: data\\synthetic\\synthetic_checkpoint_0044.json\n",
      "  💾 Checkpoint 45: data\\synthetic\\synthetic_checkpoint_0045.json\n",
      "  💾 Checkpoint 46: data\\synthetic\\synthetic_checkpoint_0046.json\n",
      "  💾 Checkpoint 47: data\\synthetic\\synthetic_checkpoint_0047.json\n",
      "  💾 Checkpoint 48: data\\synthetic\\synthetic_checkpoint_0048.json\n",
      "  💾 Checkpoint 49: data\\synthetic\\synthetic_checkpoint_0049.json\n",
      "  💾 Checkpoint 51: data\\synthetic\\synthetic_checkpoint_0051.json\n",
      "  💾 Checkpoint 52: data\\synthetic\\synthetic_checkpoint_0052.json\n",
      "  💾 Checkpoint 53: data\\synthetic\\synthetic_checkpoint_0053.json\n",
      "  💾 Checkpoint 54: data\\synthetic\\synthetic_checkpoint_0054.json\n",
      "  💾 Checkpoint 55: data\\synthetic\\synthetic_checkpoint_0055.json\n",
      "  💾 Checkpoint 57: data\\synthetic\\synthetic_checkpoint_0057.json\n",
      "  💾 Checkpoint 58: data\\synthetic\\synthetic_checkpoint_0058.json\n",
      "  💾 Checkpoint 59: data\\synthetic\\synthetic_checkpoint_0059.json\n",
      "  💾 Checkpoint 60: data\\synthetic\\synthetic_checkpoint_0060.json\n",
      "  💾 Checkpoint 61: data\\synthetic\\synthetic_checkpoint_0061.json\n",
      "  💾 Checkpoint 63: data\\synthetic\\synthetic_checkpoint_0063.json\n",
      "  💾 Checkpoint 64: data\\synthetic\\synthetic_checkpoint_0064.json\n",
      "  💾 Checkpoint 65: data\\synthetic\\synthetic_checkpoint_0065.json\n",
      "  💾 Checkpoint 66: data\\synthetic\\synthetic_checkpoint_0066.json\n",
      "  💾 Checkpoint 67: data\\synthetic\\synthetic_checkpoint_0067.json\n",
      "  💾 Checkpoint 69: data\\synthetic\\synthetic_checkpoint_0069.json\n",
      "  💾 Checkpoint 70: data\\synthetic\\synthetic_checkpoint_0070.json\n",
      "  💾 Checkpoint 71: data\\synthetic\\synthetic_checkpoint_0071.json\n",
      "  💾 Checkpoint 72: data\\synthetic\\synthetic_checkpoint_0072.json\n",
      "  💾 Checkpoint 73: data\\synthetic\\synthetic_checkpoint_0073.json\n",
      "  💾 Checkpoint 74: data\\synthetic\\synthetic_checkpoint_0074.json\n",
      "  💾 Checkpoint 76: data\\synthetic\\synthetic_checkpoint_0076.json\n",
      "  💾 Checkpoint 77: data\\synthetic\\synthetic_checkpoint_0077.json\n",
      "  💾 Checkpoint 78: data\\synthetic\\synthetic_checkpoint_0078.json\n",
      "  💾 Checkpoint 79: data\\synthetic\\synthetic_checkpoint_0079.json\n",
      "Generation error: AioRpcError: <AioRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.RESOURCE_EXHAUSTED\n",
      "\tdetails = \"Too many requests for team 9f03f76f-d9af-4210-9ffb-90f7a35a5943 and model grok-4-1-fast-non-reasoning. Your team's rate limit is — Requests per Second (actual/limit): 8/8, Requests per Minute (actual/limit): 0/480. You can view your team rate limits at https://console.x.ai.\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Too many requests for team 9f03f76f-d9af-4210-9ffb-90f7a35a5943 and model grok-4-1-fast-non-reasoning. Your team\\'s rate limit is \\xe2\\x80\\x94 Requests per Second (actual/limit): 8/8, Requests per Minute (actual/limit): 0/480. You can view your team rate limits at https://console.x.ai.\", grpc_status:8}\"\n",
      ">\n",
      "  💾 Checkpoint 80: data\\synthetic\\synthetic_checkpoint_0080.json\n",
      "  💾 Checkpoint 82: data\\synthetic\\synthetic_checkpoint_0082.json\n",
      "  💾 Checkpoint 83: data\\synthetic\\synthetic_checkpoint_0083.json\n",
      "  💾 Checkpoint 84: data\\synthetic\\synthetic_checkpoint_0084.json\n",
      "  💾 Checkpoint 85: data\\synthetic\\synthetic_checkpoint_0085.json\n",
      "  💾 Checkpoint 86: data\\synthetic\\synthetic_checkpoint_0086.json\n",
      "  💾 Checkpoint 87: data\\synthetic\\synthetic_checkpoint_0087.json\n",
      "  💾 Checkpoint 89: data\\synthetic\\synthetic_checkpoint_0089.json\n",
      "  💾 Checkpoint 90: data\\synthetic\\synthetic_checkpoint_0090.json\n",
      "  💾 Checkpoint 91: data\\synthetic\\synthetic_checkpoint_0091.json\n",
      "  💾 Checkpoint 92: data\\synthetic\\synthetic_checkpoint_0092.json\n",
      "  💾 Checkpoint 93: data\\synthetic\\synthetic_checkpoint_0093.json\n",
      "  💾 Checkpoint 95: data\\synthetic\\synthetic_checkpoint_0095.json\n",
      "  💾 Checkpoint 96: data\\synthetic\\synthetic_checkpoint_0096.json\n",
      "  💾 Checkpoint 97: data\\synthetic\\synthetic_checkpoint_0097.json\n",
      "  💾 Checkpoint 98: data\\synthetic\\synthetic_checkpoint_0098.json\n",
      "Generation error: AioRpcError: <AioRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.RESOURCE_EXHAUSTED\n",
      "\tdetails = \"Too many requests for team 9f03f76f-d9af-4210-9ffb-90f7a35a5943 and model grok-4-1-fast-non-reasoning. Your team's rate limit is — Requests per Second (actual/limit): 8/8, Requests per Minute (actual/limit): 0/480. You can view your team rate limits at https://console.x.ai.\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Too many requests for team 9f03f76f-d9af-4210-9ffb-90f7a35a5943 and model grok-4-1-fast-non-reasoning. Your team\\'s rate limit is \\xe2\\x80\\x94 Requests per Second (actual/limit): 8/8, Requests per Minute (actual/limit): 0/480. You can view your team rate limits at https://console.x.ai.\", grpc_status:8}\"\n",
      ">\n",
      "  💾 Checkpoint 99: data\\synthetic\\synthetic_checkpoint_0099.json\n",
      "  💾 Checkpoint 101: data\\synthetic\\synthetic_checkpoint_0101.json\n",
      "  💾 Checkpoint 102: data\\synthetic\\synthetic_checkpoint_0102.json\n",
      "  💾 Checkpoint 103: data\\synthetic\\synthetic_checkpoint_0103.json\n",
      "  💾 Checkpoint 104: data\\synthetic\\synthetic_checkpoint_0104.json\n",
      "  💾 Checkpoint 105: data\\synthetic\\synthetic_checkpoint_0105.json\n",
      "  💾 Checkpoint 106: data\\synthetic\\synthetic_checkpoint_0106.json\n",
      "  💾 Checkpoint 108: data\\synthetic\\synthetic_checkpoint_0108.json\n",
      "\n",
      "============================================================\n",
      "GENERATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Total generated: 10836\n",
      "Success rate: 99.0%\n",
      "\n",
      "By dimension:\n",
      "  evolving: 1813\n",
      "  multilingual: 1819\n",
      "  basic: 1818\n",
      "  noisy: 1801\n",
      "  contextual: 1819\n",
      "  adversarial: 1766\n",
      "\n",
      "By PII type:\n",
      "  PASSPORT_NUMBER: 684\n",
      "  DATE_OF_BIRTH: 684\n",
      "  POSTAL_CODE: 682\n",
      "  NAMES_OF_PLACES_OR_NOUNS: 682\n",
      "  PHONE: 682\n",
      "  NATIONAL_IDENTITY_SSN_AADHAR: 681\n",
      "  VEHICLE_REGISTRATION: 680\n",
      "  BANK_UPI_ID: 680\n",
      "  OTHER_NATIONAL_IDENTITY: 679\n",
      "  TAX_IDENTIFICATION: 677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aritr\\AppData\\Local\\Temp\\ipykernel_5200\\1920566619.py:483: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"generated_at\": datetime.utcnow().isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset saved to: data\\synthetic\\synthetic_pii_data.json\n"
     ]
    }
   ],
   "source": [
    "async def run_generation():\n",
    "    \"\"\"Execute the full synthetic data generation pipeline.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"SYNTHETIC PII DATA GENERATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Start time: {datetime.now().isoformat()}\")\n",
    "    print(f\"Target samples: {generator.config.total_samples}\")\n",
    "    print()\n",
    "    \n",
    "    # Uncomment to resume from checkpoint:\n",
    "    # generator.load_checkpoint(\"./data/synthetic/synthetic_checkpoint_0050.json\")\n",
    "    # start_idx = len(generator.generated_samples)\n",
    "    start_idx = 0\n",
    "    \n",
    "    try:\n",
    "        samples = await generator.generate_batch(\n",
    "            start_from=start_idx,\n",
    "            progress_bar=True,\n",
    "        )\n",
    "        \n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(\"GENERATION COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = generator.get_statistics()\n",
    "        print(f\"\\nTotal generated: {stats['total_generated']}\")\n",
    "        print(f\"Success rate: {stats['success_rate']:.1%}\")\n",
    "        \n",
    "        print(\"\\nBy dimension:\")\n",
    "        for dim, count in stats[\"by_dimension\"].items():\n",
    "            print(f\"  {dim}: {count}\")\n",
    "        \n",
    "        print(\"\\nBy PII type:\")\n",
    "        for pii_type, count in sorted(stats[\"by_pii_type\"].items(), key=lambda x: -x[1])[:10]:\n",
    "            print(f\"  {pii_type}: {count}\")\n",
    "        \n",
    "        # Save final dataset\n",
    "        output_path = generator.save_final_dataset(\"synthetic_pii_data.json\")\n",
    "        \n",
    "        return samples\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nGeneration interrupted! Saving checkpoint...\")\n",
    "        generator._save_checkpoint(9998)\n",
    "        print(\"Checkpoint saved. Run again to resume.\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        await grok_client.close()\n",
    "\n",
    "\n",
    "# Run the generation\n",
    "# In Jupyter, use: await run_generation()\n",
    "# In script, use: asyncio.run(run_generation())\n",
    "\n",
    "# For Jupyter notebooks:\n",
    "samples = await run_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9444e",
   "metadata": {},
   "source": [
    "## Post-Generation Analysis and Export\n",
    "\n",
    "Analyze the generated dataset and prepare for the next notebook (validation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd839f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Total samples: 10836\n",
      "\n",
      "Distribution by dimension:\n",
      "  basic          :  1818 ( 16.8%)\n",
      "  contextual     :  1819 ( 16.8%)\n",
      "  noisy          :  1801 ( 16.6%)\n",
      "  evolving       :  1813 ( 16.7%)\n",
      "  multilingual   :  1819 ( 16.8%)\n",
      "  adversarial    :  1766 ( 16.3%)\n",
      "\n",
      "Distribution by PII type:\n",
      "  BANK_ACCOUNT                  :  671 (  6.2%)\n",
      "  BANK_UPI_ID                   :  680 (  6.3%)\n",
      "  CREDIT_CARD                   :  670 (  6.2%)\n",
      "  DATE_OF_BIRTH                 :  684 (  6.3%)\n",
      "  DRIVER_LICENSE                :  677 (  6.2%)\n",
      "  EMAIL                         :  672 (  6.2%)\n",
      "  INSURANCE_NUMBER              :  673 (  6.2%)\n",
      "  NAME                          :  662 (  6.1%)\n",
      "  NAMES_OF_PLACES_OR_NOUNS      :  682 (  6.3%)\n",
      "  NATIONAL_IDENTITY_SSN_AADHAR  :  681 (  6.3%)\n",
      "  OTHER_NATIONAL_IDENTITY       :  679 (  6.3%)\n",
      "  PASSPORT_NUMBER               :  684 (  6.3%)\n",
      "  PHONE                         :  682 (  6.3%)\n",
      "  POSTAL_CODE                   :  682 (  6.3%)\n",
      "  TAX_IDENTIFICATION            :  677 (  6.2%)\n",
      "  VEHICLE_REGISTRATION          :  680 (  6.3%)\n",
      "\n",
      "Distribution by locale:\n",
      "  de_DE     :  979 (  9.0%)\n",
      "  en_AU     : 1033 (  9.5%)\n",
      "  en_CA     : 1020 (  9.4%)\n",
      "  en_GB     : 1006 (  9.3%)\n",
      "  en_IN     : 1669 ( 15.4%)\n",
      "  en_US     : 1053 (  9.7%)\n",
      "  es_ES     :  998 (  9.2%)\n",
      "  fr_FR     : 1029 (  9.5%)\n",
      "  it_IT     : 1023 (  9.4%)\n",
      "  nl_NL     : 1026 (  9.5%)\n",
      "\n",
      "Entity statistics:\n",
      "  Total entities: 49180\n",
      "  Average per sample: 4.54\n",
      "\n",
      "Text length statistics:\n",
      "  Min: 140\n",
      "  Max: 557\n",
      "  Mean: 301.1\n",
      "\n",
      "============================================================\n",
      "SAMPLE EXAMPLES (one per dimension)\n",
      "============================================================\n",
      "\n",
      "[EVOLVING]\n",
      "Text: Guten Tag Herr Müller, vielen Dank für Ihre Fintech-App-Anmeldung. Wir haben Ihren deutschen Reisepass C41531085 verifiziert. Bitte bestätigen Sie die UPI-ID netflixuser@paytm und Ihr Ethereum-Wallet ...\n",
      "Entities: 4\n",
      "  - [103:112] PASSPORT_NUMBER: 'C41531085'\n",
      "  - [158:175] UPI_ID: 'netflixuser@paytm'\n",
      "  - [200:242] CRYPTO_WALLET: '0x742d35Cc6634C0532925a3b8D87e061Df4B15f62'\n",
      "\n",
      "[MULTILINGUAL]\n",
      "Text: Dear Mr. Patel, thank you for submitting your employment application to our Toronto office. We've received your Canadian SIN V30358510 and verified it against government records. Please provide your f...\n",
      "Entities: 4\n",
      "  - [125:134] OTHER_NATIONAL_IDENTITY: 'V30358510'\n",
      "  - [229:236] POSTAL_CODE: 'M5V 2T6'\n",
      "  - [254:269] PHONE_NUMBER: '+1-416-555-0198'\n",
      "\n",
      "[BASIC]\n",
      "Text: Passport Application Confirmation\n",
      "\n",
      "Dear Mr. Hans Müller,\n",
      "\n",
      "Your passport application has been processed successfully.\n",
      "Passport Number: C82371706\n",
      "Date of Birth: 15.03.1985\n",
      "Place of Birth: Berlin\n",
      "Address...\n",
      "Entities: 4\n",
      "  - [134:143] PASSPORT_NUMBER: 'C82371706'\n",
      "  - [44:55] PERSON_NAME: 'Hans Müller'\n",
      "  - [159:169] DATE_OF_BIRTH: '15.03.1985'\n",
      "\n",
      "[NOISY]\n",
      "Text: hey sally, quick chk on that custm acct. SSN is 247-67-9732 for Johnathan R0berts DOB 04/15/78 ph 555-867-5309. need to verfy addr too 123 Oak St Apt 4B Anyt0wn US 90210. thx! sent frm mobl...\n",
      "Entities: 5\n",
      "  - [48:59] NATIONAL_IDENTITY_SSN_AADHAR: '247-67-9732'\n",
      "  - [64:73] PERSON_FIRST_NAME: 'Johnathan'\n",
      "  - [74:81] PERSON_LAST_NAME: 'R0berts'\n",
      "\n",
      "[CONTEXTUAL]\n",
      "Text: Hola equipo, por favor contacta a Raúl Pintor en pintoraul@example.org para confirmar el pedido del producto PINTOR-456 del almacén Madrid. Su número de cliente es 478392104 y el envío va a Calle Pint...\n",
      "Entities: 5\n",
      "  - [49:70] EMAIL: 'pintoraul@example.org'\n",
      "  - [34:45] PERSON: 'Raúl Pintor'\n",
      "  - [164:173] CUSTOMER_ID: '478392104'\n",
      "\n",
      "[ADVERSARIAL]\n",
      "Text: Debug log from user auth module:\n",
      "Error: Failed to verify token for charlesmeyer@example.net\n",
      "User ID:  AU123456789 (ABN-linked)\n",
      "Phone: +61 412 345 678\n",
      "Full name: Charles T Meyer\n",
      "IP: 203.0.113.45\n",
      "Retry ...\n",
      "Entities: 4\n",
      "  - [67:91] EMAIL: 'charlesmeyer@example.net'\n",
      "  - [102:113] ID_NUMBER: 'AU123456789'\n",
      "  - [134:149] PHONE: '+61 412 345 678'\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataset(samples: list[SyntheticSample]) -> None:\n",
    "    \"\"\"Print comprehensive analysis of generated dataset.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic counts\n",
    "    print(f\"\\nTotal samples: {len(samples)}\")\n",
    "    \n",
    "    # Dimension distribution\n",
    "    dim_counts = defaultdict(int)\n",
    "    for s in samples:\n",
    "        dim_counts[s.feature_dimension.value] += 1\n",
    "    \n",
    "    print(\"\\nDistribution by dimension:\")\n",
    "    for dim in FEATURE_DIMENSIONS:\n",
    "        count = dim_counts.get(dim, 0)\n",
    "        pct = count / len(samples) * 100 if samples else 0\n",
    "        print(f\"  {dim:15s}: {count:5d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # PII type distribution\n",
    "    pii_counts = defaultdict(int)\n",
    "    for s in samples:\n",
    "        pii_counts[s.seed_pii_type] += 1\n",
    "    \n",
    "    print(\"\\nDistribution by PII type:\")\n",
    "    for pii_type in sorted(pii_counts.keys()):\n",
    "        count = pii_counts[pii_type]\n",
    "        pct = count / len(samples) * 100 if samples else 0\n",
    "        print(f\"  {pii_type:30s}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Locale distribution\n",
    "    locale_counts = defaultdict(int)\n",
    "    for s in samples:\n",
    "        if s.seed_pii_locale:\n",
    "            locale_counts[s.seed_pii_locale] += 1\n",
    "    \n",
    "    print(\"\\nDistribution by locale:\")\n",
    "    for locale in sorted(locale_counts.keys()):\n",
    "        count = locale_counts[locale]\n",
    "        pct = count / len(samples) * 100 if samples else 0\n",
    "        print(f\"  {locale:10s}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Entity statistics\n",
    "    total_entities = sum(len(s.entities) for s in samples)\n",
    "    avg_entities = total_entities / len(samples) if samples else 0\n",
    "    \n",
    "    print(f\"\\nEntity statistics:\")\n",
    "    print(f\"  Total entities: {total_entities}\")\n",
    "    print(f\"  Average per sample: {avg_entities:.2f}\")\n",
    "    \n",
    "    # Text length statistics\n",
    "    text_lengths = [len(s.text) for s in samples]\n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(f\"  Min: {min(text_lengths) if text_lengths else 0}\")\n",
    "    print(f\"  Max: {max(text_lengths) if text_lengths else 0}\")\n",
    "    print(f\"  Mean: {sum(text_lengths) / len(text_lengths) if text_lengths else 0:.1f}\")\n",
    "    \n",
    "    # Sample examples\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE EXAMPLES (one per dimension)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    shown_dims = set()\n",
    "    for sample in samples:\n",
    "        if sample.feature_dimension.value not in shown_dims:\n",
    "            shown_dims.add(sample.feature_dimension.value)\n",
    "            print(f\"\\n[{sample.feature_dimension.value.upper()}]\")\n",
    "            print(f\"Text: {sample.text[:200]}...\")\n",
    "            print(f\"Entities: {len(sample.entities)}\")\n",
    "            for ent in sample.entities[:3]:\n",
    "                print(f\"  - [{ent.start}:{ent.end}] {ent.label}: '{ent.text}'\")\n",
    "            \n",
    "            if len(shown_dims) >= 6:\n",
    "                break\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "if 'samples' in dir() and samples:\n",
    "    analyze_dataset(samples)\n",
    "else:\n",
    "    print(\"No samples generated yet. Run the generation cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d7338",
   "metadata": {},
   "source": [
    "## Export to CSV/Parquet\n",
    "\n",
    "Export the generated data in formats suitable for later validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "449ace09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported JSONL: data\\synthetic\\synthetic_samples.jsonl\n",
      "✓ Exported CSV: data\\synthetic\\synthetic_samples.csv\n",
      "✓ Exported Parquet: data\\synthetic\\synthetic_samples.parquet\n",
      "✓ Exported dimension-specific JSON files\n",
      "\n",
      "Total files exported: 9\n",
      "\n",
      "Exported files:\n",
      "  jsonl: data\\synthetic\\synthetic_samples.jsonl\n",
      "  csv: data\\synthetic\\synthetic_samples.csv\n",
      "  parquet: data\\synthetic\\synthetic_samples.parquet\n",
      "  json_basic: data\\synthetic\\synthetic_basic.json\n",
      "  json_contextual: data\\synthetic\\synthetic_contextual.json\n",
      "  json_noisy: data\\synthetic\\synthetic_noisy.json\n",
      "  json_evolving: data\\synthetic\\synthetic_evolving.json\n",
      "  json_multilingual: data\\synthetic\\synthetic_multilingual.json\n",
      "  json_adversarial: data\\synthetic\\synthetic_adversarial.json\n"
     ]
    }
   ],
   "source": [
    "def export_for_validation(\n",
    "    samples: list[SyntheticSample],\n",
    "    output_dir: str = \"./data/synthetic\",\n",
    ") -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Export samples in multiple formats for downstream processing.\n",
    "    \n",
    "    Args:\n",
    "        samples: List of generated samples\n",
    "        output_dir: Output directory path\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping format names to file paths\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    exported_files = {}\n",
    "    \n",
    "    # 1. JSON Lines format (one sample per line, for streaming)\n",
    "    jsonl_path = output_dir / \"synthetic_samples.jsonl\"\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sample in samples:\n",
    "            f.write(json.dumps(sample.model_dump(), ensure_ascii=False) + \"\\n\")\n",
    "    exported_files[\"jsonl\"] = str(jsonl_path)\n",
    "    print(f\"✓ Exported JSONL: {jsonl_path}\")\n",
    "    \n",
    "    # 2. CSV format (flattened, for quick inspection)\n",
    "    csv_data = []\n",
    "    for sample in samples:\n",
    "        csv_data.append({\n",
    "            \"generation_id\": sample.generation_id,\n",
    "            \"text\": sample.text,\n",
    "            \"feature_dimension\": sample.feature_dimension.value,\n",
    "            \"seed_pii_type\": sample.seed_pii_type,\n",
    "            \"seed_pii_value\": sample.seed_pii_value,\n",
    "            \"seed_pii_locale\": sample.seed_pii_locale,\n",
    "            \"scenario\": sample.scenario,\n",
    "            \"type_variant\": sample.type_variant,\n",
    "            \"num_entities\": len(sample.entities),\n",
    "            \"entities_json\": json.dumps([e.model_dump() for e in sample.entities]),\n",
    "            \"timestamp\": sample.timestamp,\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_path = output_dir / \"synthetic_samples.csv\"\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    exported_files[\"csv\"] = str(csv_path)\n",
    "    print(f\"✓ Exported CSV: {csv_path}\")\n",
    "    \n",
    "    # 3. Parquet format (efficient for large datasets)\n",
    "    try:\n",
    "        parquet_path = output_dir / \"synthetic_samples.parquet\"\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        exported_files[\"parquet\"] = str(parquet_path)\n",
    "        print(f\"✓ Exported Parquet: {parquet_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Parquet export skipped ({e})\")\n",
    "    \n",
    "    # 4. Dimension-specific JSON files (for targeted validation)\n",
    "    for dimension in FEATURE_DIMENSIONS:\n",
    "        dim_samples = [s for s in samples if s.feature_dimension.value == dimension]\n",
    "        if dim_samples:\n",
    "            dim_path = output_dir / f\"synthetic_{dimension}.json\"\n",
    "            with open(dim_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    [s.model_dump() for s in dim_samples],\n",
    "                    f,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "            exported_files[f\"json_{dimension}\"] = str(dim_path)\n",
    "    print(f\"✓ Exported dimension-specific JSON files\")\n",
    "    \n",
    "    print(f\"\\nTotal files exported: {len(exported_files)}\")\n",
    "    return exported_files\n",
    "\n",
    "\n",
    "# Export\n",
    "if 'samples' in dir() and samples:\n",
    "    exported = export_for_validation(samples)\n",
    "    print(\"\\nExported files:\")\n",
    "    for fmt, path in exported.items():\n",
    "        print(f\"  {fmt}: {path}\")\n",
    "else:\n",
    "    print(\"No samples to export. Run generation first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
