{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8053fec5",
   "metadata": {},
   "source": [
    "# Dataset Splitting and Merging\n",
    "\n",
    "# Dataset Splitting and Merging\n",
    "\n",
    "Split validated synthetic PII data into 80-20 train/test sets and combine with the original ai4privacy English-only\n",
    "dataset splits.\n",
    "\n",
    "Following HuggingFace Dataset Upload Guide: https://huggingface.co/docs/hub/en/datasets-upload-guide-llm\n",
    "\n",
    "Output structure: - Combined train: ai4privacy train + 80% synthetic - Combined test: ai4privacy validation + 20%\n",
    "synthetic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdd9c1",
   "metadata": {},
   "source": [
    "## Imports and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "425f2566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  ai4privacy dataset: data\\ai4privacy_english_only\n",
      "  Synthetic validated: data\\validation\\validated_samples.jsonl\n",
      "  Output directory: data\\combined_dataset\n",
      "  HuggingFace repo: Ari-S-123/better-english-pii-anonymizer\n",
      "  Synthetic split ratio: 80% train / 20% test\n",
      "  Random seed: 69\n",
      "  Max shard size: 500MB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_from_disk,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "AI4PRIVACY_PATH: Path = Path(\"./data/ai4privacy_english_only\")\n",
    "SYNTHETIC_VALIDATED_PATH: Path = Path(\"./data/validation/validated_samples.jsonl\")\n",
    "OUTPUT_PATH: Path = Path(\"./data/combined_dataset\")\n",
    "\n",
    "# HuggingFace Hub configuration (set to None if not pushing to Hub)\n",
    "HF_REPO_ID: str | None = \"Ari-S-123/better-english-pii-anonymizer\"  # e.g., \"your-username/pii-detection-combined\"\n",
    "HF_PRIVATE: bool = False  # Set to False for public datasets\n",
    "\n",
    "# Split configuration\n",
    "SYNTHETIC_TRAIN_RATIO: float = 0.80\n",
    "SYNTHETIC_TEST_RATIO: float = 0.20\n",
    "RANDOM_SEED: int = 69\n",
    "\n",
    "# Memory-efficient upload settings (per HF guide recommendations)\n",
    "MAX_SHARD_SIZE: str = \"500MB\"  # Recommended for large datasets\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  ai4privacy dataset: {AI4PRIVACY_PATH}\")\n",
    "print(f\"  Synthetic validated: {SYNTHETIC_VALIDATED_PATH}\")\n",
    "print(f\"  Output directory: {OUTPUT_PATH}\")\n",
    "print(f\"  HuggingFace repo: {HF_REPO_ID or 'Not configured (local only)'}\")\n",
    "print(f\"  Synthetic split ratio: {SYNTHETIC_TRAIN_RATIO:.0%} train / {SYNTHETIC_TEST_RATIO:.0%} test\")\n",
    "print(f\"  Random seed: {RANDOM_SEED}\")\n",
    "print(f\"  Max shard size: {MAX_SHARD_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71c13b",
   "metadata": {},
   "source": [
    "## Define Explicit Features Schema\n",
    "\n",
    "Define explicit Features schema for the unified dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc107407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Features schema defined:\n",
      "  source_text: Value('string')\n",
      "  privacy_mask: {'label': List(Value('string')), 'start': List(Value('int64')), 'end': List(Value('int64')), 'value': List(Value('string')), 'label_index': List(Value('int64'))}\n",
      "  feature_dimension: Value('string')\n",
      "  seed_pii_type: Value('string')\n",
      "  seed_pii_value: Value('string')\n",
      "  seed_pii_locale: Value('string')\n",
      "  scenario: Value('string')\n",
      "  type_variant: Value('string')\n",
      "  generation_id: Value('string')\n",
      "  data_source: Value('string')\n",
      "  language: Value('string')\n",
      "  region: Value('string')\n",
      "  script: Value('string')\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for privacy_mask entries (nested structure)\n",
    "# This matches the ai4privacy format: {label, start, end, value, label_index}\n",
    "PRIVACY_MASK_FEATURE = {\n",
    "    \"label\": Value(\"string\"),\n",
    "    \"start\": Value(\"int64\"),\n",
    "    \"end\": Value(\"int64\"),\n",
    "    \"value\": Value(\"string\"),\n",
    "    \"label_index\": Value(\"int64\"),\n",
    "}\n",
    "\n",
    "# Define the complete unified schema\n",
    "UNIFIED_FEATURES = Features({\n",
    "    # Core fields (required for NER training)\n",
    "    \"source_text\": Value(\"string\"),\n",
    "    \"privacy_mask\": Sequence(PRIVACY_MASK_FEATURE),\n",
    "    \n",
    "    # Synthetic-specific metadata (nullable for ai4privacy records)\n",
    "    \"feature_dimension\": Value(\"string\"),\n",
    "    \"seed_pii_type\": Value(\"string\"),\n",
    "    \"seed_pii_value\": Value(\"string\"),\n",
    "    \"seed_pii_locale\": Value(\"string\"),\n",
    "    \"scenario\": Value(\"string\"),\n",
    "    \"type_variant\": Value(\"string\"),\n",
    "    \"generation_id\": Value(\"string\"),\n",
    "    \n",
    "    # Source identification\n",
    "    \"data_source\": Value(\"string\"),\n",
    "    \n",
    "    # Language/locale metadata\n",
    "    \"language\": Value(\"string\"),\n",
    "    \"region\": Value(\"string\"),\n",
    "    \"script\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "print(\"Unified Features schema defined:\")\n",
    "for feature_name, feature_type in UNIFIED_FEATURES.items():\n",
    "    print(f\"  {feature_name}: {feature_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f87b9e",
   "metadata": {},
   "source": [
    "## Load ai4privacy English-Only Dataset\n",
    "\n",
    "Load the pre-filtered English-only ai4privacy dataset from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2004f5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai4privacy English-only dataset loaded:\n",
      "  Train split: 120,533 examples\n",
      "  Validation split: 30,160 examples\n",
      "\n",
      "Original features: ['source_text', 'masked_text', 'privacy_mask', 'split', 'uid', 'language', 'region', 'script', 'mbert_tokens', 'mbert_token_classes']\n",
      "\n",
      "Sample ai4privacy record keys: ['source_text', 'masked_text', 'privacy_mask', 'split', 'uid', 'language', 'region', 'script', 'mbert_tokens', 'mbert_token_classes']\n"
     ]
    }
   ],
   "source": [
    "ai4privacy_dataset: DatasetDict = load_from_disk(str(AI4PRIVACY_PATH))\n",
    "\n",
    "print(\"ai4privacy English-only dataset loaded:\")\n",
    "print(f\"  Train split: {len(ai4privacy_dataset['train']):,} examples\")\n",
    "print(f\"  Validation split: {len(ai4privacy_dataset['validation']):,} examples\")\n",
    "print(f\"\\nOriginal features: {list(ai4privacy_dataset['train'].features.keys())}\")\n",
    "\n",
    "# Display sample record structure\n",
    "sample_ai4: dict[str, Any] = ai4privacy_dataset[\"train\"][0]\n",
    "print(f\"\\nSample ai4privacy record keys: {list(sample_ai4.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42040f",
   "metadata": {},
   "source": [
    "## Load Validated Synthetic Samples\n",
    "\n",
    "Load the GPT-5.1 validated synthetic samples from JSONL format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbb50848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic validated samples loaded: 6,002 examples\n",
      "\n",
      "Distribution by feature dimension:\n",
      "  adversarial: 580 (9.7%)\n",
      "  basic: 1,237 (20.6%)\n",
      "  contextual: 1,001 (16.7%)\n",
      "  evolving: 943 (15.7%)\n",
      "  multilingual: 1,146 (19.1%)\n",
      "  noisy: 1,095 (18.2%)\n",
      "\n",
      "Sample synthetic record keys: ['text', 'entities', 'feature_dimension', 'seed_pii_type', 'seed_pii_value', 'seed_pii_locale', 'scenario', 'type_variant', 'generation_id', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "synthetic_samples: list[dict[str, Any]] = []\n",
    "\n",
    "with open(SYNTHETIC_VALIDATED_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            synthetic_samples.append(json.loads(line))\n",
    "\n",
    "print(f\"Synthetic validated samples loaded: {len(synthetic_samples):,} examples\")\n",
    "\n",
    "# Display distribution by feature dimension\n",
    "dimension_counts: Counter[str] = Counter(\n",
    "    s.get(\"feature_dimension\", \"unknown\") for s in synthetic_samples\n",
    ")\n",
    "print(\"\\nDistribution by feature dimension:\")\n",
    "for dim, count in sorted(dimension_counts.items()):\n",
    "    pct = count / len(synthetic_samples) * 100\n",
    "    print(f\"  {dim}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Display sample record structure\n",
    "print(f\"\\nSample synthetic record keys: {list(synthetic_samples[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83cb18",
   "metadata": {},
   "source": [
    "## Stratified 80-20 Split of Synthetic Data\n",
    "\n",
    "Perform stratified 80-20 train/test split on synthetic data. Stratification ensures balanced representation of each\n",
    "feature dimension in both splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fab890b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data split complete:\n",
      "  Train: 4,801 samples (80.0%)\n",
      "  Test: 1,201 samples (20.0%)\n",
      "\n",
      "Verify stratification (train split):\n",
      "  adversarial: 464 (9.7%)\n",
      "  basic: 989 (20.6%)\n",
      "  contextual: 801 (16.7%)\n",
      "  evolving: 754 (15.7%)\n",
      "  multilingual: 917 (19.1%)\n",
      "  noisy: 876 (18.2%)\n",
      "\n",
      "Verify stratification (test split):\n",
      "  adversarial: 116 (9.7%)\n",
      "  basic: 248 (20.6%)\n",
      "  contextual: 200 (16.7%)\n",
      "  evolving: 189 (15.7%)\n",
      "  multilingual: 229 (19.1%)\n",
      "  noisy: 219 (18.2%)\n"
     ]
    }
   ],
   "source": [
    "# Extract feature dimensions for stratification\n",
    "stratify_labels: list[str] = [\n",
    "    s.get(\"feature_dimension\", \"unknown\") for s in synthetic_samples\n",
    "]\n",
    "\n",
    "# Create indices for splitting\n",
    "indices: list[int] = list(range(len(synthetic_samples)))\n",
    "\n",
    "# Perform stratified split\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    train_size=SYNTHETIC_TRAIN_RATIO,\n",
    "    test_size=SYNTHETIC_TEST_RATIO,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=stratify_labels,\n",
    ")\n",
    "\n",
    "# Split the samples\n",
    "synthetic_train: list[dict[str, Any]] = [synthetic_samples[i] for i in train_indices]\n",
    "synthetic_test: list[dict[str, Any]] = [synthetic_samples[i] for i in test_indices]\n",
    "\n",
    "print(f\"Synthetic data split complete:\")\n",
    "print(f\"  Train: {len(synthetic_train):,} samples ({len(synthetic_train)/len(synthetic_samples)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(synthetic_test):,} samples ({len(synthetic_test)/len(synthetic_samples)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification preserved dimension distribution\n",
    "print(\"\\nVerify stratification (train split):\")\n",
    "train_dims: Counter[str] = Counter(s.get(\"feature_dimension\") for s in synthetic_train)\n",
    "for dim, count in sorted(train_dims.items()):\n",
    "    pct = count / len(synthetic_train) * 100\n",
    "    print(f\"  {dim}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nVerify stratification (test split):\")\n",
    "test_dims: Counter[str] = Counter(s.get(\"feature_dimension\") for s in synthetic_test)\n",
    "for dim, count in sorted(test_dims.items()):\n",
    "    pct = count / len(synthetic_test) * 100\n",
    "    print(f\"  {dim}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91664d",
   "metadata": {},
   "source": [
    "## Schema Conversion Functions\n",
    "\n",
    "Convert between synthetic and ai4privacy dataset schemas.\n",
    "\n",
    "The ai4privacy schema uses: - source_text: Original text with PII - privacy_mask: List of {label, start, end, value,\n",
    "label_index} - mbert_tokens/mbert_token_classes: Pre-tokenized for mBERT\n",
    "\n",
    "The synthetic schema uses: - text: Original text with PII - entities: List of {start, end, label, text}\n",
    "\n",
    "We'll create a unified schema that preserves all information from both sources while maintaining compatibility for\n",
    "downstream tokenization with DeBERTaV3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48c1259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing schema conversion functions...\n",
      "\n",
      "Synthetic → Unified:\n",
      "  source_text (preview): Dear Mr. Raj Patel,\n",
      "\n",
      "Thank you for your visa application to Canada. Here are you...\n",
      "  privacy_mask count: 5\n",
      "  data_source: synthetic\n",
      "\n",
      "ai4privacy → Unified:\n",
      "  source_text (preview): To-do list for 4th August 1942: meet with Brandy Haroon at 10:17 to discuss the ...\n",
      "  privacy_mask count: 4\n",
      "  data_source: ai4privacy\n"
     ]
    }
   ],
   "source": [
    "def convert_synthetic_to_unified(sample: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert a synthetic sample to the unified schema format.\n",
    "    \n",
    "    Args:\n",
    "        sample: A synthetic sample dictionary with keys like 'text', 'entities',\n",
    "                'feature_dimension', etc.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary in unified schema format compatible with ai4privacy records.\n",
    "    \"\"\"\n",
    "    # Convert entity spans from synthetic format to ai4privacy-compatible format\n",
    "    privacy_mask: list[dict[str, Any]] = []\n",
    "    \n",
    "    entities = sample.get(\"entities\", [])\n",
    "    label_counters: Counter[str] = Counter()\n",
    "    \n",
    "    for entity in entities:\n",
    "        label: str = entity.get(\"label\", \"UNKNOWN\")\n",
    "        label_counters[label] += 1\n",
    "        \n",
    "        privacy_mask.append({\n",
    "            \"label\": label,\n",
    "            \"start\": int(entity.get(\"start\", 0)),\n",
    "            \"end\": int(entity.get(\"end\", 0)),\n",
    "            \"value\": str(entity.get(\"text\", \"\")),\n",
    "            \"label_index\": int(label_counters[label]),\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        # Core fields\n",
    "        \"source_text\": str(sample.get(\"text\", \"\")),\n",
    "        \"privacy_mask\": privacy_mask,\n",
    "        \n",
    "        # Synthetic-specific metadata (ensure string types, handle None)\n",
    "        \"feature_dimension\": str(sample.get(\"feature_dimension\", \"\")),\n",
    "        \"seed_pii_type\": str(sample.get(\"seed_pii_type\", \"\")),\n",
    "        \"seed_pii_value\": str(sample.get(\"seed_pii_value\", \"\")),\n",
    "        \"seed_pii_locale\": str(sample.get(\"seed_pii_locale\") or \"\"),\n",
    "        \"scenario\": str(sample.get(\"scenario\", \"\")),\n",
    "        \"type_variant\": str(sample.get(\"type_variant\", \"\")),\n",
    "        \"generation_id\": str(sample.get(\"generation_id\", \"\")),\n",
    "        \n",
    "        # Source identification\n",
    "        \"data_source\": \"synthetic\",\n",
    "        \n",
    "        # Language metadata\n",
    "        \"language\": \"en\",\n",
    "        \"region\": \"\",\n",
    "        \"script\": \"Latn\",\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_ai4privacy_to_unified(sample: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert an ai4privacy sample to the unified schema format.\n",
    "    \n",
    "    Args:\n",
    "        sample: An ai4privacy sample dictionary with keys like 'source_text',\n",
    "                'privacy_mask', 'mbert_tokens', etc.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary in unified schema format.\n",
    "    \"\"\"\n",
    "    # Ensure privacy_mask entries have correct types\n",
    "    privacy_mask: list[dict[str, Any]] = []\n",
    "    for entry in sample.get(\"privacy_mask\", []):\n",
    "        privacy_mask.append({\n",
    "            \"label\": str(entry.get(\"label\", \"\")),\n",
    "            \"start\": int(entry.get(\"start\", 0)),\n",
    "            \"end\": int(entry.get(\"end\", 0)),\n",
    "            \"value\": str(entry.get(\"value\", \"\")),\n",
    "            \"label_index\": int(entry.get(\"label_index\", 0)),\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        # Core fields\n",
    "        \"source_text\": str(sample.get(\"source_text\", \"\")),\n",
    "        \"privacy_mask\": privacy_mask,\n",
    "        \n",
    "        # Synthetic-specific metadata (empty for ai4privacy)\n",
    "        \"feature_dimension\": \"\",\n",
    "        \"seed_pii_type\": \"\",\n",
    "        \"seed_pii_value\": \"\",\n",
    "        \"seed_pii_locale\": \"\",\n",
    "        \"scenario\": \"\",\n",
    "        \"type_variant\": \"\",\n",
    "        \"generation_id\": \"\",\n",
    "        \n",
    "        # Source identification\n",
    "        \"data_source\": \"ai4privacy\",\n",
    "        \n",
    "        # Language metadata\n",
    "        \"language\": str(sample.get(\"language\", \"en\")),\n",
    "        \"region\": str(sample.get(\"region\") or \"\"),\n",
    "        \"script\": str(sample.get(\"script\") or \"\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the conversion functions\n",
    "print(\"Testing schema conversion functions...\")\n",
    "\n",
    "test_synthetic = synthetic_samples[0]\n",
    "converted_synthetic = convert_synthetic_to_unified(test_synthetic)\n",
    "print(f\"\\nSynthetic → Unified:\")\n",
    "print(f\"  source_text (preview): {converted_synthetic['source_text'][:80]}...\")\n",
    "print(f\"  privacy_mask count: {len(converted_synthetic['privacy_mask'])}\")\n",
    "print(f\"  data_source: {converted_synthetic['data_source']}\")\n",
    "\n",
    "test_ai4 = ai4privacy_dataset[\"train\"][0]\n",
    "converted_ai4 = convert_ai4privacy_to_unified(test_ai4)\n",
    "print(f\"\\nai4privacy → Unified:\")\n",
    "print(f\"  source_text (preview): {converted_ai4['source_text'][:80]}...\")\n",
    "print(f\"  privacy_mask count: {len(converted_ai4['privacy_mask'])}\")\n",
    "print(f\"  data_source: {converted_ai4['data_source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c916f",
   "metadata": {},
   "source": [
    "## Convert All Samples to Unified Format\n",
    "\n",
    "Convert all samples from both sources to the unified schema format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85febc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting synthetic train samples to unified format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21d9fb57bc64ac9b10eeb88d4dd1b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Synthetic train:   0%|          | 0/4801 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting synthetic test samples to unified format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496e04f836d64182adad59a813ae168f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Synthetic test:   0%|          | 0/1201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting ai4privacy samples to unified format...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8468eca0110f4d8bafeeea68eee5d516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ai4privacy train:   0%|          | 0/120533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394ca3ab000f4f52841b958115c73494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ai4privacy validation:   0%|          | 0/30160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversion complete:\n",
      "  Synthetic train (unified): 4,801\n",
      "  Synthetic test (unified): 1,201\n",
      "  ai4privacy train (unified): 120,533\n",
      "  ai4privacy validation (unified): 30,160\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Convert synthetic samples\n",
    "print(\"Converting synthetic train samples to unified format...\")\n",
    "synthetic_train_unified: list[dict[str, Any]] = [\n",
    "    convert_synthetic_to_unified(s) for s in tqdm(synthetic_train, desc=\"Synthetic train\")\n",
    "]\n",
    "\n",
    "print(\"Converting synthetic test samples to unified format...\")\n",
    "synthetic_test_unified: list[dict[str, Any]] = [\n",
    "    convert_synthetic_to_unified(s) for s in tqdm(synthetic_test, desc=\"Synthetic test\")\n",
    "]\n",
    "\n",
    "# Convert ai4privacy samples\n",
    "print(\"\\nConverting ai4privacy samples to unified format...\")\n",
    "\n",
    "ai4_train_unified: list[dict[str, Any]] = [\n",
    "    convert_ai4privacy_to_unified(sample) \n",
    "    for sample in tqdm(ai4privacy_dataset[\"train\"], desc=\"ai4privacy train\")\n",
    "]\n",
    "\n",
    "ai4_val_unified: list[dict[str, Any]] = [\n",
    "    convert_ai4privacy_to_unified(sample) \n",
    "    for sample in tqdm(ai4privacy_dataset[\"validation\"], desc=\"ai4privacy validation\")\n",
    "]\n",
    "\n",
    "print(\"\\nConversion complete:\")\n",
    "print(f\"  Synthetic train (unified): {len(synthetic_train_unified):,}\")\n",
    "print(f\"  Synthetic test (unified): {len(synthetic_test_unified):,}\")\n",
    "print(f\"  ai4privacy train (unified): {len(ai4_train_unified):,}\")\n",
    "print(f\"  ai4privacy validation (unified): {len(ai4_val_unified):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee942f",
   "metadata": {},
   "source": [
    "## Combine and Create Dataset with Explicit Features\n",
    "\n",
    "Combine datasets and create HuggingFace Dataset:\n",
    "\n",
    "-   Combined train = ai4privacy train + synthetic train\n",
    "-   Combined test = ai4privacy validation + synthetic test\n",
    "\n",
    "Note: We let the datasets library infer the schema automatically for the nested privacy_mask structure, then verify the\n",
    "inferred types match expectations. This avoids encoding issues with complex nested Sequence features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b5991b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset statistics:\n",
      "\n",
      "  TRAIN SET:\n",
      "    ai4privacy: 120,533\n",
      "    synthetic: 4,801\n",
      "    combined: 125,334\n",
      "\n",
      "  TEST SET:\n",
      "    ai4privacy: 30,160\n",
      "    synthetic: 1,201\n",
      "    combined: 31,361\n",
      "\n",
      "Creating HuggingFace Datasets (schema will be inferred)...\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source_text', 'privacy_mask', 'feature_dimension', 'seed_pii_type', 'seed_pii_value', 'seed_pii_locale', 'scenario', 'type_variant', 'generation_id', 'data_source', 'language', 'region', 'script'],\n",
      "        num_rows: 125334\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source_text', 'privacy_mask', 'feature_dimension', 'seed_pii_type', 'seed_pii_value', 'seed_pii_locale', 'scenario', 'type_variant', 'generation_id', 'data_source', 'language', 'region', 'script'],\n",
      "        num_rows: 31361\n",
      "    })\n",
      "})\n",
      "\n",
      "Inferred Features (verify these match expectations):\n",
      "  source_text: Value('string')\n",
      "  privacy_mask: List({'end': Value('int64'), 'label': Value('string'), 'label_index': Value('int64'), 'start': Value('int64'), 'value': Value('string')})\n",
      "  feature_dimension: Value('string')\n",
      "  seed_pii_type: Value('string')\n",
      "  seed_pii_value: Value('string')\n",
      "  seed_pii_locale: Value('string')\n",
      "  scenario: Value('string')\n",
      "  type_variant: Value('string')\n",
      "  generation_id: Value('string')\n",
      "  data_source: Value('string')\n",
      "  language: Value('string')\n",
      "  region: Value('string')\n",
      "  script: Value('string')\n",
      "\n",
      "✓ Schema validation passed\n"
     ]
    }
   ],
   "source": [
    "# Combine train sets\n",
    "combined_train_records: list[dict[str, Any]] = ai4_train_unified + synthetic_train_unified\n",
    "\n",
    "# Combine test sets\n",
    "combined_test_records: list[dict[str, Any]] = ai4_val_unified + synthetic_test_unified\n",
    "\n",
    "print(\"Combined dataset statistics:\")\n",
    "print(f\"\\n  TRAIN SET:\")\n",
    "print(f\"    ai4privacy: {len(ai4_train_unified):,}\")\n",
    "print(f\"    synthetic: {len(synthetic_train_unified):,}\")\n",
    "print(f\"    combined: {len(combined_train_records):,}\")\n",
    "\n",
    "print(f\"\\n  TEST SET:\")\n",
    "print(f\"    ai4privacy: {len(ai4_val_unified):,}\")\n",
    "print(f\"    synthetic: {len(synthetic_test_unified):,}\")\n",
    "print(f\"    combined: {len(combined_test_records):,}\")\n",
    "\n",
    "# Create Dataset objects WITHOUT explicit features (let datasets infer schema)\n",
    "# This handles nested structures like privacy_mask more reliably\n",
    "print(\"\\nCreating HuggingFace Datasets (schema will be inferred)...\")\n",
    "\n",
    "combined_train_dataset: Dataset = Dataset.from_list(combined_train_records)\n",
    "combined_test_dataset: Dataset = Dataset.from_list(combined_test_records)\n",
    "\n",
    "# Create DatasetDict\n",
    "combined_dataset: DatasetDict = DatasetDict({\n",
    "    \"train\": combined_train_dataset,\n",
    "    \"test\": combined_test_dataset,\n",
    "})\n",
    "\n",
    "print(f\"\\n{combined_dataset}\")\n",
    "\n",
    "# Verify inferred schema matches expectations\n",
    "print(f\"\\nInferred Features (verify these match expectations):\")\n",
    "for feature_name, feature_type in combined_dataset[\"train\"].features.items():\n",
    "    print(f\"  {feature_name}: {feature_type}\")\n",
    "\n",
    "# Validate critical fields have correct types\n",
    "train_features = combined_dataset[\"train\"].features\n",
    "assert \"source_text\" in train_features, \"Missing source_text field\"\n",
    "assert \"privacy_mask\" in train_features, \"Missing privacy_mask field\"\n",
    "assert \"data_source\" in train_features, \"Missing data_source field\"\n",
    "print(\"\\n✓ Schema validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e506d",
   "metadata": {},
   "source": [
    "## Pre-Upload Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4eed0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PRE-UPLOAD VALIDATION\n",
      "============================================================\n",
      "\n",
      "1. Testing first example loads correctly...\n",
      "   ✓ Train example loaded successfully\n",
      "     - source_text length: 137 chars\n",
      "     - privacy_mask entries: 4\n",
      "     - data_source: ai4privacy\n",
      "   ✓ Test example loaded successfully\n",
      "\n",
      "2. Checking dataset sizes...\n",
      "   Train: 125,334 examples\n",
      "   Test: 31,361 examples\n",
      "\n",
      "3. Verifying features schema...\n",
      "   ✓ All 13 features present\n",
      "\n",
      "4. Validating data source distribution...\n",
      "   Train: {'ai4privacy': 120533, 'synthetic': 4801}\n",
      "   Test: {'ai4privacy': 30160, 'synthetic': 1201}\n",
      "\n",
      "5. Checking for empty critical fields...\n",
      "   ⚠ Empty source_text: 7 train, 0 test\n",
      "\n",
      "============================================================\n",
      "Validation complete. Review any warnings above before upload.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRE-UPLOAD VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test first example from each split\n",
    "print(\"\\n1. Testing first example loads correctly...\")\n",
    "try:\n",
    "    train_example = combined_dataset[\"train\"][0]\n",
    "    print(f\"   ✓ Train example loaded successfully\")\n",
    "    print(f\"     - source_text length: {len(train_example['source_text'])} chars\")\n",
    "    print(f\"     - privacy_mask entries: {len(train_example['privacy_mask'])}\")\n",
    "    print(f\"     - data_source: {train_example['data_source']}\")\n",
    "    \n",
    "    test_example = combined_dataset[\"test\"][0]\n",
    "    print(f\"   ✓ Test example loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Error loading example: {e}\")\n",
    "\n",
    "# Check dataset sizes\n",
    "print(\"\\n2. Checking dataset sizes...\")\n",
    "print(f\"   Train: {len(combined_dataset['train']):,} examples\")\n",
    "print(f\"   Test: {len(combined_dataset['test']):,} examples\")\n",
    "\n",
    "# Verify features are preserved\n",
    "print(\"\\n3. Verifying features schema...\")\n",
    "expected_features = set(UNIFIED_FEATURES.keys())\n",
    "actual_features = set(combined_dataset[\"train\"].features.keys())\n",
    "if expected_features == actual_features:\n",
    "    print(f\"   ✓ All {len(expected_features)} features present\")\n",
    "else:\n",
    "    missing = expected_features - actual_features\n",
    "    extra = actual_features - expected_features\n",
    "    if missing:\n",
    "        print(f\"   ✗ Missing features: {missing}\")\n",
    "    if extra:\n",
    "        print(f\"   ⚠ Extra features: {extra}\")\n",
    "\n",
    "# Validate data source distribution\n",
    "print(\"\\n4. Validating data source distribution...\")\n",
    "train_sources: Counter[str] = Counter(\n",
    "    combined_dataset[\"train\"][\"data_source\"]\n",
    ")\n",
    "test_sources: Counter[str] = Counter(\n",
    "    combined_dataset[\"test\"][\"data_source\"]\n",
    ")\n",
    "print(f\"   Train: {dict(train_sources)}\")\n",
    "print(f\"   Test: {dict(test_sources)}\")\n",
    "\n",
    "# Check for empty/null values in critical fields\n",
    "print(\"\\n5. Checking for empty critical fields...\")\n",
    "empty_source_text_train = sum(\n",
    "    1 for x in combined_dataset[\"train\"][\"source_text\"] if not x.strip()\n",
    ")\n",
    "empty_source_text_test = sum(\n",
    "    1 for x in combined_dataset[\"test\"][\"source_text\"] if not x.strip()\n",
    ")\n",
    "if empty_source_text_train == 0 and empty_source_text_test == 0:\n",
    "    print(f\"   ✓ No empty source_text fields\")\n",
    "else:\n",
    "    print(f\"   ⚠ Empty source_text: {empty_source_text_train} train, {empty_source_text_test} test\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Validation complete. Review any warnings above before upload.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14a683",
   "metadata": {},
   "source": [
    "## Investigate and Clean Empty Records\n",
    "\n",
    "Empty text samples provide no training signal and can cause issues with tokenization and loss computation during\n",
    "training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b9f88ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 empty source_text records in train set\n",
      "\n",
      "Investigating empty records:\n",
      "------------------------------------------------------------\n",
      "\n",
      "  Index 1894:\n",
      "    data_source: ai4privacy\n",
      "    source_text: '' (length: 0)\n",
      "    privacy_mask entries: 0\n",
      "    language: en\n",
      "    region: GB\n",
      "\n",
      "  Index 14629:\n",
      "    data_source: ai4privacy\n",
      "    source_text: '' (length: 0)\n",
      "    privacy_mask entries: 0\n",
      "    language: en\n",
      "    region: US\n",
      "\n",
      "  Index 20189:\n",
      "    data_source: ai4privacy\n",
      "    source_text: '' (length: 0)\n",
      "    privacy_mask entries: 0\n",
      "    language: en\n",
      "    region: CA\n",
      "\n",
      "  Index 45145:\n",
      "    data_source: ai4privacy\n",
      "    source_text: '' (length: 0)\n",
      "    privacy_mask entries: 0\n",
      "    language: en\n",
      "    region: IN\n",
      "\n",
      "  Index 62063:\n",
      "    data_source: ai4privacy\n",
      "    source_text: '' (length: 0)\n",
      "    privacy_mask entries: 0\n",
      "    language: en\n",
      "    region: US\n",
      "\n",
      "  Index 108065:\n",
      "    data_source: ai4privacy\n",
      "    source_text: '' (length: 0)\n",
      "    privacy_mask entries: 0\n",
      "    language: en\n",
      "    region: US\n",
      "\n",
      "  Index 108066:\n",
      "    data_source: ai4privacy\n",
      "    source_text: '' (length: 0)\n",
      "    privacy_mask entries: 0\n",
      "    language: en\n",
      "    region: US\n",
      "\n",
      "Empty records by source: {'ai4privacy': 7}\n",
      "\n",
      "------------------------------------------------------------\n",
      "Removing empty records...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b579f7950ede4f6fb0bfdfe531957ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering empty source_text:   0%|          | 0/125334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: 125,334 → 125,327 (7 removed)\n",
      "✓ All empty records removed from train set\n",
      "✓ Test set has 0 empty records (expected: 0)\n"
     ]
    }
   ],
   "source": [
    "# Find indices of empty source_text in train set\n",
    "empty_train_indices: list[int] = [\n",
    "    i for i, text in enumerate(combined_dataset[\"train\"][\"source_text\"])\n",
    "    if not text.strip()\n",
    "]\n",
    "\n",
    "print(f\"Found {len(empty_train_indices)} empty source_text records in train set\")\n",
    "\n",
    "# Investigate the empty records\n",
    "if empty_train_indices:\n",
    "    print(\"\\nInvestigating empty records:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for idx in empty_train_indices:\n",
    "        record = combined_dataset[\"train\"][idx]\n",
    "        print(f\"\\n  Index {idx}:\")\n",
    "        print(f\"    data_source: {record['data_source']}\")\n",
    "        print(f\"    source_text: '{record['source_text']}' (length: {len(record['source_text'])})\")\n",
    "        print(f\"    privacy_mask entries: {len(record['privacy_mask'])}\")\n",
    "        \n",
    "        # Show additional context for synthetic records\n",
    "        if record[\"data_source\"] == \"synthetic\":\n",
    "            print(f\"    feature_dimension: {record['feature_dimension']}\")\n",
    "            print(f\"    generation_id: {record['generation_id']}\")\n",
    "        else:\n",
    "            print(f\"    language: {record['language']}\")\n",
    "            print(f\"    region: {record['region']}\")\n",
    "\n",
    "# Count by source\n",
    "empty_by_source: Counter[str] = Counter(\n",
    "    combined_dataset[\"train\"][i][\"data_source\"] for i in empty_train_indices\n",
    ")\n",
    "print(f\"\\nEmpty records by source: {dict(empty_by_source)}\")\n",
    "\n",
    "# Remove empty records from train set\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Removing empty records...\")\n",
    "\n",
    "# Create mask for non-empty records\n",
    "non_empty_mask: list[bool] = [\n",
    "    bool(text.strip()) for text in combined_dataset[\"train\"][\"source_text\"]\n",
    "]\n",
    "\n",
    "# Filter train set\n",
    "train_before = len(combined_dataset[\"train\"])\n",
    "combined_dataset[\"train\"] = combined_dataset[\"train\"].filter(\n",
    "    lambda example: bool(example[\"source_text\"].strip()),\n",
    "    desc=\"Filtering empty source_text\",\n",
    ")\n",
    "train_after = len(combined_dataset[\"train\"])\n",
    "\n",
    "print(f\"\\nTrain set: {train_before:,} → {train_after:,} ({train_before - train_after} removed)\")\n",
    "\n",
    "# Verify no empty records remain\n",
    "remaining_empty = sum(\n",
    "    1 for text in combined_dataset[\"train\"][\"source_text\"] if not text.strip()\n",
    ")\n",
    "if remaining_empty == 0:\n",
    "    print(\"✓ All empty records removed from train set\")\n",
    "else:\n",
    "    print(f\"⚠ Still have {remaining_empty} empty records (unexpected)\")\n",
    "\n",
    "# Also check test set (should be 0 based on earlier validation)\n",
    "test_empty = sum(\n",
    "    1 for text in combined_dataset[\"test\"][\"source_text\"] if not text.strip()\n",
    ")\n",
    "print(f\"✓ Test set has {test_empty} empty records (expected: 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6fd20",
   "metadata": {},
   "source": [
    "## Save Locally as Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7f3aaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset as Parquet (recommended format)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04170056ce74669a887b52764b474d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Train: data\\combined_dataset\\train.parquet (14.9 MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79169386d3ce4eaba5d6e0c07301c8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Test: data\\combined_dataset\\test.parquet (3.7 MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6235f74a08e244e99df6bc29a31375e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/125327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f391082fe64b659d1e736d19db625d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/31361 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Arrow format: data\\combined_dataset\\arrow\n",
      "\n",
      "To reload locally:\n",
      "  # From Parquet:\n",
      "  from datasets import load_dataset\n",
      "  dataset = load_dataset('parquet', data_files={'train': 'data\\combined_dataset\\train.parquet', 'test': 'data\\combined_dataset\\test.parquet'})\n",
      "\n",
      "  # From Arrow:\n",
      "  from datasets import load_from_disk\n",
      "  dataset = load_from_disk('data\\combined_dataset\\arrow')\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as Parquet files (recommended by HF guide)\n",
    "parquet_train_path = OUTPUT_PATH / \"train.parquet\"\n",
    "parquet_test_path = OUTPUT_PATH / \"test.parquet\"\n",
    "\n",
    "print(\"Saving dataset as Parquet (recommended format)...\")\n",
    "\n",
    "combined_dataset[\"train\"].to_parquet(str(parquet_train_path))\n",
    "print(f\"  ✓ Train: {parquet_train_path} ({parquet_train_path.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "\n",
    "combined_dataset[\"test\"].to_parquet(str(parquet_test_path))\n",
    "print(f\"  ✓ Test: {parquet_test_path} ({parquet_test_path.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "\n",
    "# Also save DatasetDict for convenience (Arrow format)\n",
    "combined_dataset.save_to_disk(str(OUTPUT_PATH / \"arrow\"))\n",
    "print(f\"  ✓ Arrow format: {OUTPUT_PATH / 'arrow'}\")\n",
    "\n",
    "print(\"\\nTo reload locally:\")\n",
    "print(f\"  # From Parquet:\")\n",
    "print(f\"  from datasets import load_dataset\")\n",
    "print(f\"  dataset = load_dataset('parquet', data_files={{'train': '{parquet_train_path}', 'test': '{parquet_test_path}'}})\")\n",
    "print(f\"\\n  # From Arrow:\")\n",
    "print(f\"  from datasets import load_from_disk\")\n",
    "print(f\"  dataset = load_from_disk('{OUTPUT_PATH / 'arrow'}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e24e7",
   "metadata": {},
   "source": [
    "## Generate Dataset Card\n",
    "\n",
    "Generate Dataset Card (README.md) for the combined dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8baa7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset card saved: data\\combined_dataset\\README.md\n",
      "\n",
      "Preview:\n",
      "------------------------------------------------------------\n",
      "---\n",
      "license: mit\n",
      "task_categories:\n",
      "  - token-classification\n",
      "language:\n",
      "  - en\n",
      "tags:\n",
      "  - pii\n",
      "  - ner\n",
      "  - privacy\n",
      "  - synthetic-data\n",
      "size_categories:\n",
      "  - 100K<n<1M\n",
      "configs:\n",
      "  - config_name: default\n",
      "    data_files:\n",
      "      - split: train\n",
      "        path: \"train.parquet\"\n",
      "      - split: test\n",
      "        path: \"test.parquet\"\n",
      "---\n",
      "\n",
      "# PII Detection Combined Dataset\n",
      "\n",
      "Combined dataset for PII (Personally Identifiable Information) detection,\n",
      "merging the ai4privacy English-only subset with synthetically generated\n",
      "challenging examples targeting NER failure modes.\n",
      "\n",
      "## Dataset Description\n",
      "\n",
      "This dataset combines two sources:\n",
      "\n",
      "1. **ai4privacy/open-pii-masking-500k** (English subset): 120,533 train / 30,160 test examples\n",
      "2. **Synthetic data** (GPT-5.1 validated): 4,801 train / 1,201 test examples\n",
      "\n",
      "**Total**: 125,334 train / 31,361 test examples\n",
      "\n",
      "### Synthetic Data Feature Dimensions\n",
      "\n",
      "The synthetic data specifically targets six NER failure mode dimensions\n",
      "from Singh & Narayanan (2025) \"Unmasking the Reality of PII Masking Models\":\n",
      "\n",
      "| Dimension | Train Count | Description |\n",
      "|-----------|-------------|-------------|\n",
      "| adversarial | 464 | Intentionally deceptive patterns |\n",
      "| basic | 989 | Standard, well-formatted entities |\n",
      "| contextual | 801 | Ambiguous entities requiring context |\n",
      "| evolving | 754 | Modern/emerging PII formats |\n",
      "| multilingual | 917 | International formats in English |\n",
      "| noisy | 876 | Real-world text imperfections |\n",
      "\n",
      "## Dataset Schema\n",
      "\n",
      "| Field | Type | Description |\n",
      "|-------|------|--------...\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "DATASET_CARD_TEMPLATE = '''---\n",
    "license: mit\n",
    "task_categories:\n",
    "  - token-classification\n",
    "language:\n",
    "  - en\n",
    "tags:\n",
    "  - pii\n",
    "  - ner\n",
    "  - privacy\n",
    "  - synthetic-data\n",
    "size_categories:\n",
    "  - 100K<n<1M\n",
    "configs:\n",
    "  - config_name: default\n",
    "    data_files:\n",
    "      - split: train\n",
    "        path: \"train.parquet\"\n",
    "      - split: test\n",
    "        path: \"test.parquet\"\n",
    "---\n",
    "\n",
    "# PII Detection Combined Dataset\n",
    "\n",
    "Combined dataset for PII (Personally Identifiable Information) detection,\n",
    "merging the ai4privacy English-only subset with synthetically generated\n",
    "challenging examples targeting NER failure modes.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "This dataset combines two sources:\n",
    "\n",
    "1. **ai4privacy/open-pii-masking-500k** (English subset): {ai4_train:,} train / {ai4_test:,} test examples\n",
    "2. **Synthetic data** (GPT-5.1 validated): {syn_train:,} train / {syn_test:,} test examples\n",
    "\n",
    "**Total**: {total_train:,} train / {total_test:,} test examples\n",
    "\n",
    "### Synthetic Data Feature Dimensions\n",
    "\n",
    "The synthetic data specifically targets six NER failure mode dimensions\n",
    "from Singh & Narayanan (2025) \"Unmasking the Reality of PII Masking Models\":\n",
    "\n",
    "| Dimension | Train Count | Description |\n",
    "|-----------|-------------|-------------|\n",
    "{dimension_table}\n",
    "\n",
    "## Dataset Schema\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `source_text` | string | Original text containing PII entities |\n",
    "| `privacy_mask` | list | Entity annotations with label, start, end, value |\n",
    "| `data_source` | string | Either \"ai4privacy\" or \"synthetic\" |\n",
    "| `feature_dimension` | string | NER challenge dimension (synthetic only) |\n",
    "| `language` | string | Language code (always \"en\") |\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load from HuggingFace Hub\n",
    "dataset = load_dataset(\"{repo_id}\")\n",
    "\n",
    "# Or load from local Parquet files\n",
    "dataset = load_dataset(\"parquet\", data_files={{\n",
    "    \"train\": \"train.parquet\",\n",
    "    \"test\": \"test.parquet\"\n",
    "}})\n",
    "\n",
    "# Access examples\n",
    "print(dataset[\"train\"][0])\n",
    "```\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this dataset, please cite:\n",
    "```bibtex\n",
    "@misc{{pii_combined_dataset_{year},\n",
    "  title={{PII Detection Combined Dataset}},\n",
    "  year={{{year}}},\n",
    "  publisher={{Hugging Face}},\n",
    "  note={{Combines ai4privacy English subset with synthetic challenging examples}}\n",
    "}}\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "MIT License\n",
    "\n",
    "## Dataset Creation\n",
    "\n",
    "- **Created**: {date}\n",
    "- **ai4privacy source**: ai4privacy/open-pii-masking-500k-ai4privacy\n",
    "- **Synthetic validation**: OpenAI GPT-5.1\n",
    "- **Split strategy**: 80/20 stratified split on synthetic data\n",
    "'''\n",
    "\n",
    "# Generate dimension table\n",
    "dimension_table_rows: list[str] = []\n",
    "for dim, count in sorted(train_dims.items()):\n",
    "    descriptions = {\n",
    "        \"basic\": \"Standard, well-formatted entities\",\n",
    "        \"contextual\": \"Ambiguous entities requiring context\",\n",
    "        \"noisy\": \"Real-world text imperfections\",\n",
    "        \"evolving\": \"Modern/emerging PII formats\",\n",
    "        \"multilingual\": \"International formats in English\",\n",
    "        \"adversarial\": \"Intentionally deceptive patterns\",\n",
    "    }\n",
    "    desc = descriptions.get(dim, \"Unknown dimension\")\n",
    "    dimension_table_rows.append(f\"| {dim} | {count:,} | {desc} |\")\n",
    "\n",
    "dimension_table = \"\\n\".join(dimension_table_rows)\n",
    "\n",
    "# Fill in template\n",
    "readme_content = DATASET_CARD_TEMPLATE.format(\n",
    "    ai4_train=len(ai4_train_unified),\n",
    "    ai4_test=len(ai4_val_unified),\n",
    "    syn_train=len(synthetic_train_unified),\n",
    "    syn_test=len(synthetic_test_unified),\n",
    "    total_train=len(combined_train_records),\n",
    "    total_test=len(combined_test_records),\n",
    "    dimension_table=dimension_table,\n",
    "    repo_id=HF_REPO_ID or \"your-username/pii-detection-combined\",\n",
    "    year=datetime.now().year,\n",
    "    date=datetime.now().strftime(\"%Y-%m-%d\"),\n",
    ")\n",
    "\n",
    "# Save README.md\n",
    "readme_path = OUTPUT_PATH / \"README.md\"\n",
    "readme_path.write_text(readme_content, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Dataset card saved: {readme_path}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(\"-\" * 60)\n",
    "print(readme_content[:1500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cecc1",
   "metadata": {},
   "source": [
    "## Push to HuggingFace Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa34801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing dataset to HuggingFace Hub: Ari-S-123/better-english-pii-anonymizer\n",
      "  Private: False\n",
      "  Max shard size: 500MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3166e0b8eb46db919fecda6e06c945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ad72ad803e4f1ba60d50de0027cbca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae20d98d96e647a686e3421fed0236e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554c96ef78d74a139969b4d834eb980c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66fee42bf4942af875f7672a7f0227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60ab22875e94b3e9e900f1569adbb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d237e72ef00b49d7a5729e51d3438cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d982d37591b4c069c567125602ae6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset uploaded successfully!\n",
      "  View at: https://huggingface.co/datasets/Ari-S-123/better-english-pii-anonymizer\n",
      "\n",
      "To load from Hub:\n",
      "  from datasets import load_dataset\n",
      "  dataset = load_dataset('Ari-S-123/better-english-pii-anonymizer')\n"
     ]
    }
   ],
   "source": [
    "if HF_REPO_ID:\n",
    "    print(f\"Pushing dataset to HuggingFace Hub: {HF_REPO_ID}\")\n",
    "    print(f\"  Private: {HF_PRIVATE}\")\n",
    "    print(f\"  Max shard size: {MAX_SHARD_SIZE}\")\n",
    "    \n",
    "    # Push with memory-efficient settings\n",
    "    combined_dataset.push_to_hub(\n",
    "        HF_REPO_ID,\n",
    "        private=HF_PRIVATE,\n",
    "        max_shard_size=MAX_SHARD_SIZE,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Dataset uploaded successfully!\")\n",
    "    print(f\"  View at: https://huggingface.co/datasets/{HF_REPO_ID}\")\n",
    "    print(f\"\\nTo load from Hub:\")\n",
    "    print(f\"  from datasets import load_dataset\")\n",
    "    print(f\"  dataset = load_dataset('{HF_REPO_ID}')\")\n",
    "else:\n",
    "    print(\"HF_REPO_ID not configured. Skipping Hub upload.\")\n",
    "    print(\"\\nTo upload later, set HF_REPO_ID and run:\")\n",
    "    print(\"  combined_dataset.push_to_hub('your-username/dataset-name')\")\n",
    "    print(\"\\nOr upload the Parquet files directly via the Hub UI:\")\n",
    "    print(f\"  {OUTPUT_PATH / 'train.parquet'}\")\n",
    "    print(f\"  {OUTPUT_PATH / 'test.parquet'}\")\n",
    "    print(f\"  {OUTPUT_PATH / 'README.md'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776b924",
   "metadata": {},
   "source": [
    "## Final Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f27706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET COMBINATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 INPUT DATA:\n",
      "   ai4privacy train: 120,533\n",
      "   ai4privacy validation: 30,160\n",
      "   synthetic validated: 6,002\n",
      "\n",
      "🔀 SYNTHETIC SPLIT (stratified by dimension):\n",
      "   Train (80%): 4,801\n",
      "   Test (20%): 1,201\n",
      "\n",
      "✅ COMBINED OUTPUT:\n",
      "   Train total: 125,334\n",
      "     ├─ ai4privacy: 120,533 (96.2%)\n",
      "     └─ synthetic: 4,801 (3.8%)\n",
      "   Test total: 31,361\n",
      "     ├─ ai4privacy: 30,160 (96.2%)\n",
      "     └─ synthetic: 1,201 (3.8%)\n",
      "\n",
      "📁 OUTPUT FILES:\n",
      "   Directory: data\\combined_dataset\n",
      "   ├─ train.parquet (recommended format)\n",
      "   ├─ test.parquet\n",
      "   ├─ README.md (dataset card)\n",
      "   └─ arrow/ (DatasetDict format)\n",
      "\n",
      "📈 SYNTHETIC DIMENSION DISTRIBUTION IN TRAIN:\n",
      "   adversarial: 464\n",
      "   basic: 989\n",
      "   contextual: 801\n",
      "   evolving: 754\n",
      "   multilingual: 917\n",
      "   noisy: 876\n",
      "\n",
      "🌐 HUGGINGFACE HUB:\n",
      "   Repository: https://huggingface.co/datasets/Ari-S-123/better-english-pii-anonymizer\n",
      "\n",
      "================================================================================\n",
      "✓ Dataset combination complete. Ready for tokenization and training.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET COMBINATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📊 INPUT DATA:\")\n",
    "print(f\"   ai4privacy train: {len(ai4privacy_dataset['train']):,}\")\n",
    "print(f\"   ai4privacy validation: {len(ai4privacy_dataset['validation']):,}\")\n",
    "print(f\"   synthetic validated: {len(synthetic_samples):,}\")\n",
    "\n",
    "print(\"\\n🔀 SYNTHETIC SPLIT (stratified by dimension):\")\n",
    "print(f\"   Train ({SYNTHETIC_TRAIN_RATIO:.0%}): {len(synthetic_train):,}\")\n",
    "print(f\"   Test ({SYNTHETIC_TEST_RATIO:.0%}): {len(synthetic_test):,}\")\n",
    "\n",
    "print(\"\\n✅ COMBINED OUTPUT:\")\n",
    "print(f\"   Train total: {len(combined_train_records):,}\")\n",
    "print(f\"     ├─ ai4privacy: {len(ai4_train_unified):,} ({len(ai4_train_unified)/len(combined_train_records)*100:.1f}%)\")\n",
    "print(f\"     └─ synthetic: {len(synthetic_train_unified):,} ({len(synthetic_train_unified)/len(combined_train_records)*100:.1f}%)\")\n",
    "print(f\"   Test total: {len(combined_test_records):,}\")\n",
    "print(f\"     ├─ ai4privacy: {len(ai4_val_unified):,} ({len(ai4_val_unified)/len(combined_test_records)*100:.1f}%)\")\n",
    "print(f\"     └─ synthetic: {len(synthetic_test_unified):,} ({len(synthetic_test_unified)/len(combined_test_records)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n📁 OUTPUT FILES:\")\n",
    "print(f\"   Directory: {OUTPUT_PATH}\")\n",
    "print(f\"   ├─ train.parquet (recommended format)\")\n",
    "print(f\"   ├─ test.parquet\")\n",
    "print(f\"   ├─ README.md (dataset card)\")\n",
    "print(f\"   └─ arrow/ (DatasetDict format)\")\n",
    "\n",
    "print(\"\\n📈 SYNTHETIC DIMENSION DISTRIBUTION IN TRAIN:\")\n",
    "for dim, count in sorted(train_dims.items()):\n",
    "    print(f\"   {dim}: {count:,}\")\n",
    "\n",
    "if HF_REPO_ID:\n",
    "    print(f\"\\n🌐 HUGGINGFACE HUB:\")\n",
    "    print(f\"   Repository: https://huggingface.co/datasets/{HF_REPO_ID}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Dataset combination complete. Ready for tokenization and training.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
