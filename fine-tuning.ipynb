{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890c3d39",
   "metadata": {},
   "source": [
    "# Modal Fine-Tuning Script for DeBERTa-v3-large with DoRA\n",
    "\n",
    "Fine-tunes microsoft/deberta-v3-large for PII Named Entity Recognition using Weight-Decomposed Low-Rank Adaptation\n",
    "(DoRA) on an H100 GPU.\n",
    "\n",
    "Dataset: Ari-S-123/better-english-pii-anonymizer Base Model: microsoft/deberta-v3-large Adapter Method: DoRA (via PEFT\n",
    "library)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd9b0a",
   "metadata": {},
   "source": [
    "## Set up Modal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33464ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: modal in c:\\users\\ari\\anaconda3\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (3.11.10)\n",
      "Requirement already satisfied: cbor2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (5.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (2025.4.26)\n",
      "Requirement already satisfied: click~=8.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (8.1.8)\n",
      "Requirement already satisfied: grpclib<0.4.9,>=0.4.7 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.4.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<7.0,>=3.19 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (6.33.1)\n",
      "Requirement already satisfied: rich>=12.0.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (13.9.4)\n",
      "Requirement already satisfied: synchronicity~=0.10.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.10.5)\n",
      "Requirement already satisfied: toml in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.10.2)\n",
      "Requirement already satisfied: typer>=0.9 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.9.0)\n",
      "Requirement already satisfied: types-certifi in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (2021.10.8.3)\n",
      "Requirement already satisfied: types-toml in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.10.8.20240310)\n",
      "Requirement already satisfied: watchfiles in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (1.1.1)\n",
      "Requirement already satisfied: typing_extensions~=4.6 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ari\\anaconda3\\lib\\site-packages (from click~=8.1->modal) (0.4.6)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from grpclib<0.4.9,>=0.4.7->modal) (4.3.0)\n",
      "Requirement already satisfied: multidict in c:\\users\\ari\\anaconda3\\lib\\site-packages (from grpclib<0.4.9,>=0.4.7->modal) (6.1.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from h2<5,>=3.1.0->grpclib<0.4.9,>=0.4.7->modal) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from h2<5,>=3.1.0->grpclib<0.4.9,>=0.4.7->modal) (4.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from rich>=12.0.0->modal) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from rich>=12.0.0->modal) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->modal) (0.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->modal) (3.7)\n",
      "Requirement already satisfied: anyio>=3.0.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from watchfiles->modal) (4.7.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from anyio>=3.0.0->watchfiles->modal) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f295688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The web browser should have opened for you to authenticate and get an API \n",
      "token.\n",
      "If it didn't, please copy this URL into your web browser manually:\n",
      "\n",
      "â ‹ Waiting for authentication in the web browser\n",
      "https://modal.com/token-flow/tf-pi96WS5f36mD1Ka9nKvlRi\n",
      "\n",
      "â ‹ Waiting for authentication in the web browser\n",
      "â ‹ Waiting for authentication in the web browser\n",
      "\n",
      "â ‹ Waiting for token flow to complete...\n",
      "â ™ Waiting for token flow to complete...\n",
      "â ¹ Waiting for token flow to complete...\n",
      "â ¸ Waiting for token flow to complete...\n",
      "â ¼ Waiting for token flow to complete...\n",
      "â ´ Waiting for token flow to complete...\n",
      "â § Waiting for token flow to complete...\n",
      "â ‡ Waiting for token flow to complete...\n",
      "â  Waiting for token flow to complete...\n",
      "â ‹ Waiting for token flow to complete...\n",
      "â ™ Waiting for token flow to complete...\n",
      "â ¹ Waiting for token flow to complete...\n",
      "â ¼ Waiting for token flow to complete...\n",
      "â ´ Waiting for token flow to complete...\n",
      "â ¦ Waiting for token flow to complete...\n",
      "â § Waiting for token flow to complete...\n",
      "â ‡ Waiting for token flow to complete...\n",
      "\n",
      "Web authentication finished successfully!\n",
      "Token is connected to the ari-s-123 workspace.\n",
      "Verifying token against https://api.modal.com\n",
      "Token verified successfully!\n",
      "â ‹ Storing token\n",
      "\n",
      "Token written to C:\\Users\\Ari/.modal.toml in profile ari-s-123.\n"
     ]
    }
   ],
   "source": [
    "!python -m modal setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb3fc0",
   "metadata": {},
   "source": [
    "## Imports and Modal App Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c16c2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Modal app configured\n",
      "  App name: pii-deberta-dora-finetune\n",
      "  Volume mount: /checkpoints\n"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "\n",
    "APP_NAME: str = \"pii-deberta-dora-finetune\"\n",
    "\n",
    "# Define the Modal App\n",
    "app = modal.App(name=APP_NAME)\n",
    "\n",
    "# HuggingFace Hub configuration\n",
    "# Create the secret first by running in a terminal:\n",
    "#   modal secret create huggingface HF_TOKEN=hf_your_token_here\n",
    "HF_SECRET = modal.Secret.from_name(\"huggingface\")\n",
    "\n",
    "# Create a persistent volume to store checkpoints and final model\n",
    "# This persists between runs so you don't lose progress if something fails\n",
    "volume = modal.Volume.from_name(\"pii-model-checkpoints\", create_if_missing=True)\n",
    "VOLUME_MOUNT_PATH: str = \"/checkpoints\"\n",
    "\n",
    "print(\"âœ“ Modal app configured\")\n",
    "print(f\"  App name: {APP_NAME}\")\n",
    "print(f\"  Volume mount: {VOLUME_MOUNT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed616d",
   "metadata": {},
   "source": [
    "## Docker Image Definition\n",
    "\n",
    "Build a custom image with all required dependencies for DoRA fine-tuning. The image is cached, so subsequent runs are\n",
    "fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45cd1d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training image defined\n",
      "  Base: debian_slim (Python 3.13)\n",
      "  PyTorch: >=2.4.0 (CUDA 12.1)\n",
      "  Key packages: transformers, peft, datasets, seqeval\n"
     ]
    }
   ],
   "source": [
    "training_image = (\n",
    "    modal.Image.debian_slim(python_version=\"3.13\")\n",
    "    # Install system dependencies\n",
    "    .apt_install(\"git\", \"curl\", \"build-essential\")\n",
    "    # Install PyTorch with CUDA 12.1 support (H100 compatible)\n",
    "    .pip_install(\n",
    "        \"torch>=2.4.0\",\n",
    "        \"torchvision\",\n",
    "        \"torchaudio\",\n",
    "        extra_index_url=\"https://download.pytorch.org/whl/cu121\",\n",
    "    )\n",
    "    # Install core ML dependencies\n",
    "    # Let pip resolve compatible versions by using minimum version constraints\n",
    "    # rather than exact pins, which cause conflicts\n",
    "    .pip_install(\n",
    "        # Transformers ecosystem - use compatible ranges\n",
    "        \"transformers>=4.46.0\",\n",
    "        \"peft>=0.14.0\",\n",
    "        \"accelerate>=1.0.0\",\n",
    "        \"datasets>=3.0.0\",\n",
    "        \"huggingface_hub>=0.26.0\",  # Don't pin exact version - let pip resolve\n",
    "        # Training utilities\n",
    "        \"scikit-learn>=1.5.0\",\n",
    "        \"seqeval>=1.2.2\",\n",
    "        \"numpy>=1.26.0\",\n",
    "        \"tqdm>=4.66.0\",\n",
    "        # Sentencepiece for DeBERTa tokenizer\n",
    "        \"sentencepiece>=0.2.0\",\n",
    "        \"protobuf>=4.25.0\",\n",
    "        # BitsAndBytes for potential quantization\n",
    "        \"bitsandbytes>=0.44.0\",\n",
    "        gpu=\"H100\",  # Build with H100 to ensure CUDA compatibility\n",
    "    )\n",
    "    # Set environment variables\n",
    "    .env({\n",
    "        \"HF_HOME\": \"/root/.cache/huggingface\",\n",
    "        \"TRANSFORMERS_CACHE\": \"/root/.cache/huggingface/hub\",\n",
    "        \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "    })\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training image defined\")\n",
    "print(\"  Base: debian_slim (Python 3.13)\")\n",
    "print(\"  PyTorch: >=2.4.0 (CUDA 12.1)\")\n",
    "print(\"  Key packages: transformers, peft, datasets, seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262aae08",
   "metadata": {},
   "source": [
    "## Training Configuration Dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f11a5814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ TrainingConfig dataclass defined\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Configuration for DeBERTa-v3-large DoRA fine-tuning on PII detection.\n",
    "\n",
    "    This configuration is optimized for an H100 GPU (80GB VRAM) and targets\n",
    "    the specific modules recommended for DeBERTa attention layers.\n",
    "\n",
    "    Attributes:\n",
    "        model_name: HuggingFace model identifier for the base model.\n",
    "        dataset_name: HuggingFace dataset identifier for training data.\n",
    "        output_dir: Local directory for saving checkpoints during training.\n",
    "        hub_model_id: HuggingFace Hub repository ID for pushing the final adapter.\n",
    "        max_length: Maximum sequence length for tokenization (DeBERTa supports 512).\n",
    "        learning_rate: AdamW learning rate. DoRA typically needs slightly lower LR.\n",
    "        num_train_epochs: Maximum number of training epochs.\n",
    "        per_device_train_batch_size: Batch size per GPU for training.\n",
    "        per_device_eval_batch_size: Batch size per GPU for evaluation.\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients.\n",
    "        warmup_ratio: Proportion of training steps for learning rate warmup.\n",
    "        weight_decay: L2 regularization coefficient.\n",
    "        lora_r: LoRA/DoRA rank (higher = more parameters, better capacity).\n",
    "        lora_alpha: LoRA/DoRA scaling factor (typically 2x rank).\n",
    "        lora_dropout: Dropout probability for LoRA layers.\n",
    "        target_modules: DeBERTa attention modules to apply DoRA to.\n",
    "        early_stopping_patience: Number of evaluations without improvement before stopping.\n",
    "        eval_steps: Evaluate every N steps.\n",
    "        save_steps: Save checkpoint every N steps.\n",
    "        logging_steps: Log metrics every N steps.\n",
    "        seed: Random seed for reproducibility.\n",
    "        bf16: Use bfloat16 mixed precision (recommended for H100).\n",
    "        dataloader_num_workers: Number of workers for data loading.\n",
    "        push_to_hub: Whether to push the final model to HuggingFace Hub.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model and dataset\n",
    "    model_name: str = \"microsoft/deberta-v3-large\"\n",
    "    dataset_name: str = \"Ari-S-123/better-english-pii-anonymizer\"\n",
    "    output_dir: str = \"/checkpoints/pii-deberta-dora\"\n",
    "    hub_model_id: str = \"Ari-S-123/deberta-v3-large-pii-dora\"\n",
    "\n",
    "    # Tokenization\n",
    "    max_length: int = 512\n",
    "\n",
    "    # Training hyperparameters\n",
    "    learning_rate: float = 2e-5\n",
    "    num_train_epochs: int = 5\n",
    "    per_device_train_batch_size: int = 16\n",
    "    per_device_eval_batch_size: int = 32\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "\n",
    "    # DoRA configuration (per PLAN.md recommendations)\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: list[str] = field(default_factory=lambda: [\n",
    "        \"query_proj\",\n",
    "        \"key_proj\",\n",
    "        \"value_proj\",\n",
    "        \"dense\",\n",
    "    ])\n",
    "\n",
    "    # Early stopping and evaluation\n",
    "    early_stopping_patience: int = 3\n",
    "    eval_steps: int = 500\n",
    "    save_steps: int = 500\n",
    "    logging_steps: int = 100\n",
    "\n",
    "    # Reproducibility and performance\n",
    "    seed: int = 69\n",
    "    bf16: bool = True  # H100 has excellent bf16 support\n",
    "    dataloader_num_workers: int = 4\n",
    "\n",
    "    # Hub configuration\n",
    "    push_to_hub: bool = True\n",
    "\n",
    "\n",
    "print(\"âœ“ TrainingConfig dataclass defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee12799d",
   "metadata": {},
   "source": [
    "## Label Mapping and Tokenization Functions\n",
    "\n",
    "These functions are defined at module level so they can be serialized and sent to the remote Modal container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d42ef2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Helper functions defined: build_label_vocabulary, tokenize_and_align_labels\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "def build_label_vocabulary(dataset: Any) -> tuple[list[str], dict[str, int], dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Build a complete BIO label vocabulary from the dataset's privacy_mask annotations.\n",
    "\n",
    "    Scans all unique entity labels in the dataset and creates a BIO-formatted\n",
    "    label vocabulary with \"O\" (Outside) plus B-/I- prefixed entity labels.\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace Dataset or DatasetDict containing privacy_mask field.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - label_list: Ordered list of all BIO labels (e.g., [\"O\", \"B-EMAIL\", \"I-EMAIL\", ...])\n",
    "            - label_to_id: Mapping from label string to integer ID\n",
    "            - id_to_label: Mapping from integer ID to label string\n",
    "    \"\"\"\n",
    "    unique_labels: set[str] = set()\n",
    "\n",
    "    # Handle both Dataset and DatasetDict\n",
    "    splits = dataset.keys() if hasattr(dataset, \"keys\") else [\"train\"]\n",
    "\n",
    "    for split in splits:\n",
    "        split_data = dataset[split] if hasattr(dataset, \"keys\") else dataset\n",
    "        for example in split_data:\n",
    "            privacy_mask = example.get(\"privacy_mask\", {})\n",
    "            labels = privacy_mask.get(\"label\", [])\n",
    "            if isinstance(labels, list):\n",
    "                unique_labels.update(labels)\n",
    "\n",
    "    # Sort for deterministic ordering\n",
    "    sorted_labels = sorted(unique_labels)\n",
    "\n",
    "    # Build BIO vocabulary: O + B-X/I-X for each entity type\n",
    "    label_list: list[str] = [\"O\"]\n",
    "    for label in sorted_labels:\n",
    "        label_list.append(f\"B-{label}\")\n",
    "        label_list.append(f\"I-{label}\")\n",
    "\n",
    "    label_to_id: dict[str, int] = {label: i for i, label in enumerate(label_list)}\n",
    "    id_to_label: dict[int, str] = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "    return label_list, label_to_id, id_to_label\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(\n",
    "    examples: dict[str, Any],\n",
    "    tokenizer: Any,\n",
    "    label_to_id: dict[str, int],\n",
    "    max_length: int = 512,\n",
    ") -> dict[str, list]:\n",
    "    \"\"\"\n",
    "    Tokenize text and align span annotations to subword token boundaries.\n",
    "\n",
    "    This function handles the critical alignment between character-level entity\n",
    "    spans (from privacy_mask) and subword token-level BIO labels required for\n",
    "    DeBERTa's token classification head.\n",
    "\n",
    "    The alignment strategy:\n",
    "        1. Tokenize with offset_mapping to get characterâ†’token correspondence\n",
    "        2. For each entity span, find overlapping tokens\n",
    "        3. Assign B- to first overlapping token, I- to subsequent tokens\n",
    "        4. Special tokens ([CLS], [SEP], [PAD]) get label -100 (ignored in loss)\n",
    "\n",
    "    Args:\n",
    "        examples: Batch of examples with \"source_text\" and \"privacy_mask\" fields.\n",
    "        tokenizer: HuggingFace tokenizer instance (DeBERTa tokenizer).\n",
    "        label_to_id: Mapping from BIO label strings to integer IDs.\n",
    "        max_length: Maximum sequence length for truncation.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with keys:\n",
    "            - input_ids: Tokenized input sequences\n",
    "            - attention_mask: Attention masks (1 for real tokens, 0 for padding)\n",
    "            - labels: Aligned BIO label IDs (-100 for special tokens)\n",
    "    \"\"\"\n",
    "    # Tokenize all texts in the batch\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"source_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    all_labels: list[list[int]] = []\n",
    "\n",
    "    for batch_idx, offset_mapping in enumerate(tokenized[\"offset_mapping\"]):\n",
    "        # Initialize all labels as \"O\" (Outside)\n",
    "        labels: list[int] = [label_to_id[\"O\"]] * len(offset_mapping)\n",
    "\n",
    "        # Get entity spans for this example\n",
    "        privacy_mask = examples[\"privacy_mask\"][batch_idx]\n",
    "\n",
    "        # Handle both dict and list-of-dicts formats\n",
    "        if isinstance(privacy_mask, dict):\n",
    "            entity_labels = privacy_mask.get(\"label\", [])\n",
    "            entity_starts = privacy_mask.get(\"start\", [])\n",
    "            entity_ends = privacy_mask.get(\"end\", [])\n",
    "        else:\n",
    "            # Shouldn't happen with this dataset, but handle gracefully\n",
    "            entity_labels, entity_starts, entity_ends = [], [], []\n",
    "\n",
    "        # Process each entity span\n",
    "        for ent_label, ent_start, ent_end in zip(entity_labels, entity_starts, entity_ends):\n",
    "            is_first_token = True\n",
    "\n",
    "            for token_idx, (tok_start, tok_end) in enumerate(offset_mapping):\n",
    "                # Skip special tokens (offset is (0, 0) for [CLS], [SEP], [PAD])\n",
    "                if tok_start == tok_end == 0:\n",
    "                    labels[token_idx] = -100  # Ignored in loss calculation\n",
    "                    continue\n",
    "\n",
    "                # Check if this token overlaps with the entity span\n",
    "                if tok_start < ent_end and tok_end > ent_start:\n",
    "                    bio_label = f\"B-{ent_label}\" if is_first_token else f\"I-{ent_label}\"\n",
    "\n",
    "                    # Only assign if the label exists in our vocabulary\n",
    "                    if bio_label in label_to_id:\n",
    "                        labels[token_idx] = label_to_id[bio_label]\n",
    "                        is_first_token = False\n",
    "\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Remove offset_mapping from output (not needed for training)\n",
    "    tokenized.pop(\"offset_mapping\")\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"âœ“ Helper functions defined: build_label_vocabulary, tokenize_and_align_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c63395a",
   "metadata": {},
   "source": [
    "## Metrics Computation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88702e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Metrics function defined: create_compute_metrics\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_compute_metrics(id_to_label: dict[int, str]) -> callable:\n",
    "    \"\"\"\n",
    "    Create a metrics computation function for NER evaluation.\n",
    "\n",
    "    Uses seqeval library for proper NER metrics (precision, recall, F1) that\n",
    "    account for entity boundaries, not just individual token labels.\n",
    "\n",
    "    Args:\n",
    "        id_to_label: Mapping from integer label IDs to BIO label strings.\n",
    "\n",
    "    Returns:\n",
    "        Callable that computes metrics from EvalPrediction object.\n",
    "    \"\"\"\n",
    "    from seqeval.metrics import (\n",
    "        classification_report,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred: Any) -> dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute NER evaluation metrics from model predictions.\n",
    "\n",
    "        Args:\n",
    "            eval_pred: EvalPrediction object with predictions and label_ids.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing precision, recall, and F1 scores.\n",
    "        \"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Convert IDs to label strings, filtering out special tokens (-100)\n",
    "        true_predictions: list[list[str]] = []\n",
    "        true_labels: list[list[str]] = []\n",
    "\n",
    "        for prediction, label in zip(predictions, labels):\n",
    "            pred_tags: list[str] = []\n",
    "            true_tags: list[str] = []\n",
    "\n",
    "            for pred_id, label_id in zip(prediction, label):\n",
    "                # Skip special tokens (label_id == -100)\n",
    "                if label_id == -100:\n",
    "                    continue\n",
    "\n",
    "                pred_tags.append(id_to_label.get(pred_id, \"O\"))\n",
    "                true_tags.append(id_to_label.get(label_id, \"O\"))\n",
    "\n",
    "            true_predictions.append(pred_tags)\n",
    "            true_labels.append(true_tags)\n",
    "\n",
    "        # Compute seqeval metrics (entity-level, not token-level)\n",
    "        metrics = {\n",
    "            \"precision\": precision_score(true_labels, true_predictions),\n",
    "            \"recall\": recall_score(true_labels, true_predictions),\n",
    "            \"f1\": f1_score(true_labels, true_predictions),\n",
    "        }\n",
    "\n",
    "        # Log detailed classification report to console\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ENTITY-LEVEL CLASSIFICATION REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(classification_report(true_labels, true_predictions))\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "print(\"âœ“ Metrics function defined: create_compute_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dda4f9",
   "metadata": {},
   "source": [
    "## Main Training Function (Modal Remote Execution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27f2cbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Remote training function defined: train_deberta_dora\n",
      "  GPU: H100\n",
      "  Timeout: 6 hours\n",
      "  Memory: 64GB\n"
     ]
    }
   ],
   "source": [
    "@app.function(\n",
    "    image=training_image,\n",
    "    gpu=\"H100\",\n",
    "    timeout=60 * 60 * 6,  # 6 hour timeout for full training\n",
    "    secrets=[HF_SECRET],\n",
    "    volumes={VOLUME_MOUNT_PATH: volume},\n",
    "    memory=65536,  # 64GB RAM\n",
    ")\n",
    "def train_deberta_dora(config_dict: dict[str, Any] | None = None) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fine-tune DeBERTa-v3-large with DoRA for PII Named Entity Recognition.\n",
    "\n",
    "    This function runs REMOTELY on a Modal H100 GPU and performs:\n",
    "        1. Dataset loading from HuggingFace Hub\n",
    "        2. Tokenization and label alignment\n",
    "        3. DoRA adapter initialization on attention modules\n",
    "        4. Training with early stopping\n",
    "        5. Final model saving and Hub upload\n",
    "\n",
    "    Args:\n",
    "        config_dict: Training configuration as a dictionary (dataclasses don't\n",
    "                     serialize well across Modal's boundary). Uses defaults if None.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing training metrics and final model location.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "    from transformers import (\n",
    "        AutoModelForTokenClassification,\n",
    "        AutoTokenizer,\n",
    "        DataCollatorForTokenClassification,\n",
    "        EarlyStoppingCallback,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "    )\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    # =========================================================================\n",
    "    # Reconstruct config from dict (dataclasses don't serialize across Modal)\n",
    "    # =========================================================================\n",
    "    if config_dict is None:\n",
    "        config_dict = {}\n",
    "\n",
    "    # Default values\n",
    "    model_name = config_dict.get(\"model_name\", \"microsoft/deberta-v3-large\")\n",
    "    dataset_name = config_dict.get(\"dataset_name\", \"Ari-S-123/better-english-pii-anonymizer\")\n",
    "    output_dir = config_dict.get(\"output_dir\", \"/checkpoints/pii-deberta-dora\")\n",
    "    hub_model_id = config_dict.get(\"hub_model_id\", \"Ari-S-123/deberta-v3-large-pii-dora\")\n",
    "    max_length = config_dict.get(\"max_length\", 512)\n",
    "    learning_rate = config_dict.get(\"learning_rate\", 2e-5)\n",
    "    num_train_epochs = config_dict.get(\"num_train_epochs\", 5)\n",
    "    per_device_train_batch_size = config_dict.get(\"per_device_train_batch_size\", 16)\n",
    "    per_device_eval_batch_size = config_dict.get(\"per_device_eval_batch_size\", 32)\n",
    "    gradient_accumulation_steps = config_dict.get(\"gradient_accumulation_steps\", 2)\n",
    "    warmup_ratio = config_dict.get(\"warmup_ratio\", 0.1)\n",
    "    weight_decay = config_dict.get(\"weight_decay\", 0.01)\n",
    "    lora_r = config_dict.get(\"lora_r\", 16)\n",
    "    lora_alpha = config_dict.get(\"lora_alpha\", 32)\n",
    "    lora_dropout = config_dict.get(\"lora_dropout\", 0.05)\n",
    "    target_modules = config_dict.get(\"target_modules\", [\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"])\n",
    "    early_stopping_patience = config_dict.get(\"early_stopping_patience\", 3)\n",
    "    eval_steps = config_dict.get(\"eval_steps\", 500)\n",
    "    save_steps = config_dict.get(\"save_steps\", 500)\n",
    "    logging_steps = config_dict.get(\"logging_steps\", 100)\n",
    "    seed = config_dict.get(\"seed\", 42)\n",
    "    bf16 = config_dict.get(\"bf16\", True)\n",
    "    dataloader_num_workers = config_dict.get(\"dataloader_num_workers\", 4)\n",
    "    push_to_hub = config_dict.get(\"push_to_hub\", True)\n",
    "\n",
    "    # Authenticate with HuggingFace Hub\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"âœ“ Authenticated with HuggingFace Hub\")\n",
    "    else:\n",
    "        print(\"âš  No HF_TOKEN found. Hub operations may fail.\")\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DeBERTa-v3-large DoRA Fine-Tuning for PII Detection\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"DoRA Rank: {lora_r}, Alpha: {lora_alpha}\")\n",
    "    print(f\"Target Modules: {target_modules}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 1: Load Dataset\n",
    "    # =========================================================================\n",
    "    print(\"ðŸ“¦ Loading dataset from HuggingFace Hub...\")\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    print(f\"   Train samples: {len(dataset['train']):,}\")\n",
    "    print(f\"   Test samples: {len(dataset['test']):,}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 2: Build Label Vocabulary\n",
    "    # =========================================================================\n",
    "    print(\"\\nðŸ·ï¸  Building label vocabulary...\")\n",
    "    label_list, label_to_id, id_to_label = build_label_vocabulary(dataset)\n",
    "    num_labels = len(label_list)\n",
    "    print(f\"   Total BIO labels: {num_labels}\")\n",
    "    print(f\"   Entity types: {(num_labels - 1) // 2}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 3: Load Tokenizer\n",
    "    # =========================================================================\n",
    "    print(f\"\\nðŸ“ Loading tokenizer: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        add_prefix_space=True,  # Important for consistent tokenization\n",
    "    )\n",
    "    print(f\"   Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 4: Tokenize Dataset\n",
    "    # =========================================================================\n",
    "    print(\"\\nâš™ï¸  Tokenizing and aligning labels...\")\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_and_align_labels(\n",
    "            examples,\n",
    "            tokenizer=tokenizer,\n",
    "            label_to_id=label_to_id,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        desc=\"Tokenizing\",\n",
    "        num_proc=dataloader_num_workers,\n",
    "    )\n",
    "\n",
    "    print(f\"   Train tokens: {len(tokenized_dataset['train']):,} examples\")\n",
    "    print(f\"   Test tokens: {len(tokenized_dataset['test']):,} examples\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 5: Load Base Model\n",
    "    # =========================================================================\n",
    "    print(f\"\\nðŸ¤– Loading base model: {model_name}\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id_to_label,\n",
    "        label2id=label_to_id,\n",
    "        torch_dtype=torch.bfloat16 if bf16 else torch.float32,\n",
    "    )\n",
    "    print(f\"   Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 6: Configure DoRA Adapter\n",
    "    # =========================================================================\n",
    "    print(\"\\nðŸ”§ Configuring DoRA adapter...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.TOKEN_CLS,\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=target_modules,\n",
    "        use_dora=True,  # Enable Weight-Decomposed Low-Rank Adaptation\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 7: Setup Training Arguments\n",
    "    # =========================================================================\n",
    "    print(\"\\nðŸ“‹ Configuring training arguments...\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        # Training hyperparameters\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=weight_decay,\n",
    "        # Precision\n",
    "        bf16=bf16,\n",
    "        # Evaluation and saving\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        # Logging\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=logging_steps,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        # Performance\n",
    "        dataloader_num_workers=dataloader_num_workers,\n",
    "        dataloader_pin_memory=True,\n",
    "        # Hub (adapter will be pushed separately)\n",
    "        push_to_hub=False,\n",
    "        # Reproducibility\n",
    "        seed=seed,\n",
    "        data_seed=seed,\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 8: Initialize Trainer\n",
    "    # =========================================================================\n",
    "    print(\"\\nðŸ‹ï¸ Initializing Trainer...\")\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        label_pad_token_id=-100,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=create_compute_metrics(id_to_label),\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=early_stopping_patience,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 9: Train!\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸš€ STARTING TRAINING\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ“ TRAINING COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Total steps: {train_result.global_step}\")\n",
    "    print(f\"   Training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 10: Final Evaluation\n",
    "    # =========================================================================\n",
    "    print(\"\\nðŸ“Š Running final evaluation...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"   Final F1: {eval_results.get('eval_f1', 'N/A'):.4f}\")\n",
    "    print(f\"   Final Precision: {eval_results.get('eval_precision', 'N/A'):.4f}\")\n",
    "    print(f\"   Final Recall: {eval_results.get('eval_recall', 'N/A'):.4f}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 11: Save Model Locally\n",
    "    # =========================================================================\n",
    "    final_output_dir = f\"{output_dir}/final\"\n",
    "    print(f\"\\nðŸ’¾ Saving model to {final_output_dir}...\")\n",
    "\n",
    "    # Save adapter weights\n",
    "    model.save_pretrained(final_output_dir)\n",
    "    tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "    # Commit volume to persist checkpoint\n",
    "    volume.commit()\n",
    "    print(\"   âœ“ Checkpoint persisted to Modal volume\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 12: Push to HuggingFace Hub\n",
    "    # =========================================================================\n",
    "    if push_to_hub and hf_token:\n",
    "        print(f\"\\nâ˜ï¸  Pushing adapter to HuggingFace Hub: {hub_model_id}\")\n",
    "        model.push_to_hub(\n",
    "            hub_model_id,\n",
    "            use_auth_token=hf_token,\n",
    "            commit_message=\"DoRA fine-tuned DeBERTa-v3-large for PII detection\",\n",
    "        )\n",
    "        tokenizer.push_to_hub(\n",
    "            hub_model_id,\n",
    "            use_auth_token=hf_token,\n",
    "        )\n",
    "        print(f\"   âœ“ Adapter available at: https://huggingface.co/{hub_model_id}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 13: Return Results\n",
    "    # =========================================================================\n",
    "    results = {\n",
    "        \"training_loss\": train_result.training_loss,\n",
    "        \"global_step\": train_result.global_step,\n",
    "        \"eval_f1\": eval_results.get(\"eval_f1\"),\n",
    "        \"eval_precision\": eval_results.get(\"eval_precision\"),\n",
    "        \"eval_recall\": eval_results.get(\"eval_recall\"),\n",
    "        \"model_path\": final_output_dir,\n",
    "        \"hub_model_id\": hub_model_id if push_to_hub else None,\n",
    "        \"num_labels\": num_labels,\n",
    "        \"trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸŽ‰ FINE-TUNING PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    for key, value in results.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ“ Remote training function defined: train_deberta_dora\")\n",
    "print(\"  GPU: H100\")\n",
    "print(\"  Timeout: 6 hours\")\n",
    "print(\"  Memory: 64GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2013a",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "This cell dispatches the training job to Modal's cloud infrastructure. This local machine just orchestrates - all heavy\n",
    "computation happens remotely.\n",
    "\n",
    "In a notebook, we must wrap the .remote() call inside app.run(). This tells Modal to \"start\" the app, hydrate the\n",
    "function metadata, andestablish the connection to Modal's infrastructure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e05c5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸš€ DISPATCHING TRAINING TO MODAL H100 GPU\n",
      "======================================================================\n",
      "Model: microsoft/deberta-v3-large\n",
      "Dataset: Ari-S-123/better-english-pii-anonymizer\n",
      "DoRA Rank: 16, Alpha: 32\n",
      "Learning Rate: 2e-05\n",
      "Epochs: 5\n",
      "Batch Size: 16 (effective: 32)\n",
      "======================================================================\n",
      "\n",
      "This will:\n",
      "  1. Build the Docker image (cached after first run)\n",
      "  2. Spin up an H100 GPU on Modal\n",
      "  3. Download dataset and model from HuggingFace\n",
      "  4. Train with DoRA for up to 5 epochs (early stopping enabled)\n",
      "  5. Push final adapter to HuggingFace Hub\n",
      "\n",
      "Logs will stream below. This may take 1-3 hours depending on dataset size.\n",
      "======================================================================\n",
      "\n",
      "\u001b[2K\u001b[32mâœ“\u001b[0m Initialized. \u001b[37mView run at \u001b[0m\n",
      "\u001b[4;37mhttps://modal.com/apps/ari-s-123/main/ap-ykyVX5Cnd5OuokEgq1VWZC\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m Initializing...\n",
      "\u001b[2K\u001b[33mBuilding image im-LFDfNwKQ1aaFgTJo1KVpYs\n",
      "\u001b[2K\u001b[33m|\u001b[0m Creating objects...ts...\u001b[0m\n",
      "=> Step 0: FROM base\n",
      "\u001b[2K\u001b[33m|\u001b[0m\u001b[33m Creating objects...\u001b[0m\n",
      "=> Step 1: RUN python -m pip install 'torch>=2.4.0' torchaudio torchvision --extra-index-url https://download.pytorch.org/whl/cu121\n",
      "\u001b[2K\u001b[33mLooking in indexes: http://pypi-mirror.modal.local:5555/simple, https://download.pytorch.org/whl/cu121\n",
      "\u001b[2K\u001b[33mCollecting torch>=2.4.0ects...\u001b[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/torch/torch-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "\u001b[2K\u001b[33mCollecting torchaudios...ts...\u001b[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/torchaudio/torchaudio-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "\u001b[2K\u001b[33mCollecting torchvisionjects...\u001b[0m\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/torchvision/torchvision-0.24.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "\u001b[2K\u001b[33mCollecting filelock (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/filelock/filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "\u001b[2K\u001b[33mCollecting typing-extensions>=4.10.0 (from torch>=2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "\u001b[2K\u001b[33mCollecting setuptools (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/setuptools/setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "\u001b[2K\u001b[33mCollecting sympy>=1.13.3 (from torch>=2.4.0)\n",
      "\u001b[2K\u001b[33m  Downloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "\u001b[2K\u001b[33mCollecting networkx>=2.5.1 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/networkx/networkx-3.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "\u001b[2K\u001b[33mCollecting jinja2 (from torch>=2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "\u001b[2K\u001b[33mCollecting fsspec>=0.8.5 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/fsspec/fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cublas-cu12/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cufft-cu12/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-curand-cu12/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-nccl-cu12==2.27.5 (from torch>=2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-nvshmem-cu12==3.3.20 (from torch>=2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cufile-cu12/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting triton==3.5.1 (from torch>=2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.5.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting numpy (from torchvision)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/numpy/numpy-2.3.5-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K\u001b[33mCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/pillow/pillow-12.0.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "\u001b[2K\u001b[33mCollecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.4.0)\n",
      "\u001b[2K\u001b[33m  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K\u001b[33m     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 536.2/536.2 kB 178.5 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mCollecting MarkupSafe>=2.0 (from jinja2->torch>=2.4.0)\n",
      "  Downloading http://pypi-mirror.modal.local:5555/simple/markupsafe/markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/torch/torch-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 899.7/899.7 MB 155.7 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cublas-cu12/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 594.3/594.3 MB 408.4 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10.2/10.2 MB 472.5 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 88.0/88.0 MB 395.3 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 954.8/954.8 kB 748.7 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading https://download.pytorch.org/whl/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 706.8/706.8 MB 112.8 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cufft-cu12/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 193.1/193.1 MB 422.6 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cufile-cu12/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.2/1.2 MB 284.8 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-curand-cu12/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 63.6/63.6 MB 410.1 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 267.5/267.5 MB 383.8 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 288.2/288.2 MB 427.7 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 287.2/287.2 MB 130.1 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 322.3/322.3 MB 121.0 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 39.3/39.3 MB 413.0 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 124.7/124.7 MB 117.2 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K\u001b[33mDownloading https://download.pytorch.org/whl/triton-3.5.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 170.5/170.5 MB 73.5 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/torchaudio/torchaudio-2.9.1-cp313-cp313-manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.1/2.1 MB 254.1 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/torchvision/torchvision-0.24.1-cp313-cp313-manylinux_2_28_x86_64.whl (8.0 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8.0/8.0 MB 194.9 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/fsspec/fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/networkx/networkx-3.6-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.1/2.1 MB 468.3 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/pillow/pillow-12.0.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7.0/7.0 MB 246.2 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6.3/6.3 MB 155.0 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/filelock/filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/markupsafe/markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/numpy/numpy-2.3.5-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 16.6/16.6 MB 260.6 MB/s eta 0:00:00\n",
      "Downloading http://pypi-mirror.modal.local:5555/simple/setuptools/setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.2/1.2 MB 821.1 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mInstalling collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "\u001b[2K\u001b[33m-\u001b[0m Creating objects...ts...\u001b[0m\n",
      "Successfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.12.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6 numpy-2.3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pillow-12.0.0 setuptools-80.9.0 sympy-1.14.0 torch-2.9.1 torchaudio-2.9.1 torchvision-0.24.1 triton-3.5.1 typing-extensions-4.15.0\n",
      "\u001b[2K\u001b[33m-\u001b[0m\u001b[33m Creating objects...\u001b[0m\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[2K\u001b[33mSaving image...objects...ts...\u001b[0m\n",
      "\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m \u001b[0m\u001b[30m---------------------------------\u001b[0m \u001b[32m0.1/7.1 GB\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-\u001b[0m\u001b[30m \u001b[0m\u001b[30m--------------------------------\u001b[0m \u001b[32m0.3/7.1 GB\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-\u001b[0m\u001b[91m \u001b[0m\u001b[30m--------------------------------\u001b[0m \u001b[32m0.4/7.1 GB\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m---\u001b[0m\u001b[30m \u001b[0m\u001b[30m------------------------------\u001b[0m \u001b[32m0.7/7.1 GB\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-------\u001b[0m\u001b[91m \u001b[0m\u001b[30m--------------------------\u001b[0m \u001b[32m1.6/7.1 GB\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m---------\u001b[0m\u001b[30m \u001b[0m\u001b[30m------------------------\u001b[0m \u001b[32m2.0/7.1 GB\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------------\u001b[0m \u001b[32m2.9/7.1 GB\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------------\u001b[0m \u001b[32m3.0/7.1 GB\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m--------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------------------\u001b[0m \u001b[32m3.0/7.1 GB\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-----------------\u001b[0m \u001b[32m3.4/7.1 GB\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m---------------\u001b[0m \u001b[32m3.8/7.1 GB\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-----------\u001b[0m \u001b[32m4.7/7.1 GB\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-----------\u001b[0m \u001b[32m4.7/7.1 GB\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----------------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m-----------\u001b[0m \u001b[32m4.7/7.1 GB\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----------------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m-----------\u001b[0m \u001b[32m4.8/7.1 GB\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-------------------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m--------\u001b[0m \u001b[32m5.3/7.1 GB\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-----------------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m----\u001b[0m \u001b[32m6.1/7.1 GB\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[33mImage saved, took 5.75sobjects...\n",
      "\u001b[2K\u001b[33m/\u001b[0m\u001b[33m Creating objects...\u001b[0m\n",
      "Built image im-LFDfNwKQ1aaFgTJo1KVpYs in 98.25s\n",
      "\n",
      "\n",
      "\u001b[2K\u001b[33mBuilding image im-x0KZvm50mKJZgvoKK9runm\n",
      "\u001b[2K\u001b[33m|\u001b[0m Creating objects...ts...\u001b[0m\n",
      "=> Step 0: FROM base\n",
      "\u001b[2K\u001b[33m/\u001b[0m\u001b[33m Creating objects...\u001b[0m\n",
      "=> Step 1: RUN python -m pip install 'accelerate>=1.0.0' 'bitsandbytes>=0.44.0' 'datasets>=3.0.0' 'huggingface_hub>=0.26.0' 'numpy>=1.26.0' 'peft>=0.14.0' 'protobuf>=4.25.0' 'scikit-learn>=1.5.0' 'sentencepiece>=0.2.0' 'seqeval>=1.2.2' 'tqdm>=4.66.0' 'transformers>=4.46.0'\n",
      "\u001b[2K\u001b[33mLooking in indexes: http://pypi-mirror.modal.local:5555/simple\n",
      "\u001b[2K\u001b[33mCollecting accelerate>=1.0.0..\u001b[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/accelerate/accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "\u001b[2K\u001b[33mCollecting bitsandbytes>=0.44.0[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/bitsandbytes/bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "\u001b[2K\u001b[33mCollecting datasets>=3.0.0s...\u001b[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/datasets/datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "\u001b[2K\u001b[33mCollecting huggingface_hub>=0.26.0\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/huggingface-hub/huggingface_hub-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "\u001b[2K\u001b[33mRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.13/site-packages (2.3.5)\n",
      "\u001b[2K\u001b[33mCollecting peft>=0.14.0ects...\u001b[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/peft/peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "\u001b[2K\u001b[33mCollecting protobuf>=4.25.0...\u001b[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/protobuf/protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "\u001b[2K\u001b[33mCollecting scikit-learn>=1.5.0\u001b[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/scikit-learn/scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "\u001b[2K\u001b[33mCollecting sentencepiece>=0.2.0[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/sentencepiece/sentencepiece-0.2.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "\u001b[2K\u001b[33mCollecting seqeval>=1.2.2ts...\u001b[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/seqeval/seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K\u001b[33m  Preparing metadata (setup.py): started\n",
      "\u001b[2K\u001b[33m  Preparing metadata (setup.py): finished with status 'done'\n",
      "\u001b[2K\u001b[33mCollecting tqdm>=4.66.0ects...\u001b[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/tqdm/tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K\u001b[33mCollecting transformers>=4.46.0[0m\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/transformers/transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K\u001b[33mCollecting packaging>=20.0 (from accelerate>=1.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/packaging/packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "\u001b[2K\u001b[33mCollecting psutil (from accelerate>=1.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/psutil/psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
      "\u001b[2K\u001b[33mCollecting pyyaml (from accelerate>=1.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/pyyaml/pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.13/site-packages (from accelerate>=1.0.0) (2.9.1)\n",
      "\u001b[2K\u001b[33mCollecting safetensors>=0.4.3 (from accelerate>=1.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/safetensors/safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "\u001b[2K\u001b[33mRequirement already satisfied: filelock in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (2025.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (12.8.90)\n",
      "\u001b[2K\u001b[33mRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (12.8.90)\n",
      "\u001b[2K\u001b[33mRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.13/site-packages (from torch>=2.0.0->accelerate>=1.0.0) (3.5.1)\n",
      "\u001b[2K\u001b[33mCollecting pyarrow>=21.0.0 (from datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/pyarrow/pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "\u001b[2K\u001b[33mCollecting dill<0.4.1,>=0.3.0 (from datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/dill/dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "\u001b[2K\u001b[33mCollecting pandas (from datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/pandas/pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K\u001b[33mCollecting requests>=2.32.2 (from datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/requests/requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "\u001b[2K\u001b[33mCollecting httpx<1.0.0 (from datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/httpx/httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "\u001b[2K\u001b[33mCollecting xxhash (from datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/xxhash/xxhash-3.6.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "\u001b[2K\u001b[33mCollecting multiprocess<0.70.19 (from datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/multiprocess/multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "\u001b[2K\u001b[33mCollecting fsspec>=0.8.5 (from torch>=2.0.0->accelerate>=1.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/fsspec/fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "\u001b[2K\u001b[33mCollecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub>=0.26.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/hf-xet/hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "\u001b[2K\u001b[33mCollecting shellingham (from huggingface_hub>=0.26.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/shellingham/shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "\u001b[2K\u001b[33mCollecting typer-slim (from huggingface_hub>=0.26.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/typer-slim/typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "\u001b[2K\u001b[33mCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/aiohttp/aiohttp-3.13.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "\u001b[2K\u001b[33mCollecting anyio (from httpx<1.0.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/anyio/anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "\u001b[2K\u001b[33mCollecting certifi (from httpx<1.0.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/certifi/certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "\u001b[2K\u001b[33mCollecting httpcore==1.* (from httpx<1.0.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/httpcore/httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "\u001b[2K\u001b[33mCollecting idna (from httpx<1.0.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/idna/idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "\u001b[2K\u001b[33mCollecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/h11/h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "\u001b[2K\u001b[33mCollecting scipy>=1.8.0 (from scikit-learn>=1.5.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/scipy/scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K\u001b[33mCollecting joblib>=1.2.0 (from scikit-learn>=1.5.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/joblib/joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "\u001b[2K\u001b[33mCollecting threadpoolctl>=3.1.0 (from scikit-learn>=1.5.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/threadpoolctl/threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "\u001b[2K\u001b[33mCollecting huggingface_hub>=0.26.0\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/huggingface-hub/huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "\u001b[2K\u001b[33mCollecting regex!=2019.12.17 (from transformers>=4.46.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/regex/regex-2025.11.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K\u001b[33mCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.46.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/tokenizers/tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "\u001b[2K\u001b[33mCollecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/aiohappyeyeballs/aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "\u001b[2K\u001b[33mCollecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/aiosignal/aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/attrs/attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "\u001b[2K\u001b[33mCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/frozenlist/frozenlist-1.8.0-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "\u001b[2K\u001b[33mCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/multidict/multidict-6.7.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "\u001b[2K\u001b[33mCollecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/propcache/propcache-0.4.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "\u001b[2K\u001b[33mCollecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/yarl/yarl-1.22.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "\u001b[2K\u001b[33mCollecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/charset-normalizer/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "\u001b[2K\u001b[33mCollecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/urllib3/urllib3-2.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "\u001b[2K\u001b[33mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.0.0) (1.3.0)\n",
      "\u001b[2K\u001b[33mRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.0.0) (3.0.3)\n",
      "\u001b[2K\u001b[33mCollecting python-dateutil>=2.8.2 (from pandas->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/python-dateutil/python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "\u001b[2K\u001b[33mCollecting pytz>=2020.1 (from pandas->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/pytz/pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "\u001b[2K\u001b[33mCollecting tzdata>=2022.7 (from pandas->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/tzdata/tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "\u001b[2K\u001b[33mCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/six/six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "\u001b[2K\u001b[33mCollecting click>=8.0.0 (from typer-slim->huggingface_hub>=0.26.0)\n",
      "\u001b[2K\u001b[33m  Downloading http://pypi-mirror.modal.local:5555/simple/click/click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/accelerate/accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/bitsandbytes/bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 59.4/59.4 MB 221.1 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/datasets/datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/dill/dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/fsspec/fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/hf-xet/hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 MB 249.1 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/httpx/httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/httpcore/httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/multiprocess/multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/peft/peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 556.4/556.4 kB 332.0 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/protobuf/protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/scikit-learn/scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.4 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9.4/9.4 MB 209.6 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/sentencepiece/sentencepiece-0.2.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.4/1.4 MB 228.8 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/tqdm/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/transformers/transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12.0/12.0 MB 206.1 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/huggingface-hub/huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 566.1/566.1 kB 376.1 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/tokenizers/tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 3.3/3.3 MB 241.0 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/aiohttp/aiohttp-3.13.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.7/1.7 MB 269.9 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/multidict/multidict-6.7.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (254 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/yarl/yarl-1.22.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/aiohappyeyeballs/aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/aiosignal/aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/attrs/attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/frozenlist/frozenlist-1.8.0-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (234 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/h11/h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/idna/idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/joblib/joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/packaging/packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/propcache/propcache-0.4.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (204 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/pyarrow/pyarrow-22.0.0-cp313-cp313-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 47.7/47.7 MB 234.1 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/pyyaml/pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 801.6/801.6 kB 239.7 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/regex/regex-2025.11.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 803.5/803.5 kB 240.7 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/requests/requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/charset-normalizer/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/urllib3/urllib3-2.6.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/certifi/certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/safetensors/safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/scipy/scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 35.7/35.7 MB 195.0 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/threadpoolctl/threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/anyio/anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/pandas/pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
      "\u001b[2K\u001b[33m   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12.3/12.3 MB 234.6 MB/s eta 0:00:00\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/python-dateutil/python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/pytz/pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/six/six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/tzdata/tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/psutil/psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)\n",
      "\u001b[2K\u001b[33mDownloading http://pypi-mirror.modal.local:5555/simple/xxhash/xxhash-3.6.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K\u001b[33mBuilding wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "\u001b[2K\u001b[33m  DEPRECATION: Building 'seqeval' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'seqeval'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "\u001b[2K\u001b[33m  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16250 sha256=4f9b15349070ce62c9878f159417dc8b792744b34ed23d11dad3c7f782504f0d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8pl2o_i2/wheels/df/74/24/ece46ce10c3a5acf4af8011cdc470c2a26d23fb141aaa41416\n",
      "Successfully built seqeval\n",
      "\u001b[2K\u001b[33mInstalling collected packages: pytz, xxhash, urllib3, tzdata, tqdm, threadpoolctl, six, sentencepiece, scipy, safetensors, regex, pyyaml, pyarrow, psutil, protobuf, propcache, packaging, multidict, joblib, idna, hf-xet, h11, fsspec, frozenlist, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scikit-learn, requests, python-dateutil, multiprocess, httpcore, anyio, aiosignal, seqeval, pandas, huggingface_hub, httpx, aiohttp, tokenizers, bitsandbytes, accelerate, transformers, datasets, peft\n",
      "\u001b[2K\u001b[33m  Attempting uninstall: fsspec\u001b[0m\n",
      "    Found existing installation: fsspec 2025.12.0\n",
      "\u001b[2K\u001b[33m    Uninstalling fsspec-2025.12.0:\n",
      "\u001b[2K\u001b[33m      Successfully uninstalled fsspec-2025.12.0\n",
      "\u001b[2K\u001b[33m|\u001b[0m Creating objects...ts...\u001b[0m\n",
      "\u001b[2K\u001b[33mSuccessfully installed accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 attrs-25.4.0 bitsandbytes-0.48.2 certifi-2025.11.12 charset_normalizer-3.4.4 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-0.36.0 idna-3.11 joblib-1.5.2 multidict-6.7.0 multiprocess-0.70.18 packaging-25.0 pandas-2.3.3 peft-0.18.0 propcache-0.4.1 protobuf-6.33.1 psutil-7.1.3 pyarrow-22.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 scikit-learn-1.7.2 scipy-1.16.3 sentencepiece-0.2.1 seqeval-1.2.2 six-1.17.0 threadpoolctl-3.6.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 tzdata-2025.2 urllib3-2.6.0 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[2K\u001b[33m-\u001b[0m\u001b[33m Creating objects...\u001b[0m\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[2K\u001b[33mSaving image...objects...ts...\u001b[0m\n",
      "\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----\u001b[0m\u001b[91m \u001b[0m\u001b[30m-------------------------\u001b[0m \u001b[32m123.4/807.7 MB\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m------\u001b[0m\u001b[91m \u001b[0m\u001b[30m-----------------------\u001b[0m \u001b[32m185.5/807.7 MB\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m--------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m---------------\u001b[0m \u001b[32m396.1/807.7 MB\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m--------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m---------\u001b[0m \u001b[32m545.7/807.7 MB\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m--------------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m---------\u001b[0m \u001b[32m554.9/807.7 MB\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-------\u001b[0m \u001b[32m594.5/807.7 MB\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----------------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m-------\u001b[0m \u001b[32m616.9/807.7 MB\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-----------------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m------\u001b[0m \u001b[32m638.6/807.7 MB\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m------------------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m-----\u001b[0m \u001b[32m661.4/807.7 MB\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-------------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m----\u001b[0m \u001b[32m683.1/807.7 MB\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m--------------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m---\u001b[0m \u001b[32m705.0/807.7 MB\u001b[0m \u001b[33m0:00:02\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m--------------------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m---\u001b[0m \u001b[32m724.0/807.7 MB\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m---------------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m--\u001b[0m \u001b[32m739.1/807.7 MB\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----------------------------\u001b[0m\u001b[30m \u001b[0m\u001b[30m-\u001b[0m \u001b[32m762.0/807.7 MB\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m----------------------------\u001b[0m\u001b[91m \u001b[0m\u001b[30m-\u001b[0m \u001b[32m774.8/807.7 MB\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-----------------------------\u001b[0m\u001b[30m \u001b[0m \u001b[32m790.2/807.7 MB\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-----------------------------\u001b[0m\u001b[91m \u001b[0m \u001b[32m802.1/807.7 MB\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mUploading image snapshotâ€¦\u001b[0m \u001b[91m-----------------------------\u001b[0m\u001b[91m \u001b[0m \u001b[32m807.7/807.7 MB\u001b[0m \u001b[33m0:00:04\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33mImage saved, took 5.31s\n",
      "\u001b[0m\u001b[33mUploading image snapshotâ€¦\u001b[0m\u001b[33m \u001b[0m\u001b[91m-----------------------------\u001b[0m\u001b[91m \u001b[0m\u001b[33m \u001b[0m\u001b[32m807.7/807.7 MB\u001b[0m\u001b[33m \u001b[0m\u001b[33m0:00:04\u001b[0m\u001b[33m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[33m3m Creating objects...\u001b[0m\n",
      "Built image im-x0KZvm50mKJZgvoKK9runm in 42.90s\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33mUploading image snapshotâ€¦\u001b[0m\u001b[33m \u001b[0m\u001b[91m-----------------------------\u001b[0m\u001b[91m \u001b[0m\u001b[33m \u001b[0m\u001b[32m807.7/807.7 MB\u001b[0m\u001b[33m \u001b[0m\u001b[33m0:00:04\u001b[0m\u001b[33m\n",
      "\u001b[2K\u001b[33mBuilding image im-UN5sL4SvLswnEb7ykzl410\n",
      "\u001b[2K\u001b[33m-\u001b[0m Creating objects...ts...\u001b[0m\n",
      "=> Step 0: FROM base\n",
      "\u001b[2K\u001b[33m\\\u001b[0m\u001b[33m Creating objects...\u001b[0m\n",
      "=> Step 1: ENV HF_HOME=/root/.cache/huggingface\n",
      "\u001b[2K\u001b[33m\\\u001b[0m\u001b[33m Creating objects...\u001b[0m\n",
      "=> Step 2: ENV TRANSFORMERS_CACHE=/root/.cache/huggingface/hub\n",
      "\u001b[2K\u001b[33m\\\u001b[0m\u001b[33m Creating objects...\u001b[0m\n",
      "=> Step 3: ENV TOKENIZERS_PARALLELISM=false\n",
      "\u001b[2K\u001b[33mSaving image...objects...ts...\u001b[0m\n",
      "\u001b[2K\u001b[33mImage saved, took 809.80mss...\u001b[0m\n",
      "\u001b[2K\u001b[33m/\u001b[0m\u001b[33m Creating objects...\u001b[0m\n",
      "Built image im-UN5sL4SvLswnEb7ykzl410 in 2.97s\n",
      "\n",
      "\n",
      "\u001b[2K\u001b[34m|\u001b[0m Creating objects...ts...\u001b[0m\n",
      "\u001b[90mâ””â”€â”€ \u001b[0mðŸ”¨ Created function train_deberta_dora.\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32mâœ“\u001b[0m Created objects.\n",
      "\u001b[90mâ””â”€â”€ \u001b[0mðŸ”¨ Created function train_deberta_dora.\n",
      "\u001b[?25l\u001b[34m-\u001b[0m \u001b[34mRunning (3 containers finished)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (3 containers finished)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (3 containers finished)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (3 containers finished)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (3 containers finished)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mWorker assigned...\u001b[0m \u001b[37mView app at \u001b[0m0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (3 containers finished)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (3 containers finished)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m/usr/local/lib/python3.13/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34mâœ“ Authenticated with HuggingFace Hub5Cnd5OuokEgq1VWZC\u001b[0m\n",
      "\u001b[2K\u001b[31mNote: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.0m\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34mdal.com/apps/ari-s-123/main/ap-ykyVX5Cnd5OuokEgq1VWZC\u001b[0m\n",
      "======================================================================\n",
      "DeBERTa-v3-large DoRA Fine-Tuning for PII Detection\n",
      "======================================================================\n",
      "Model: microsoft/deberta-v3-large\n",
      "Dataset: Ari-S-123/better-english-pii-anonymizer\n",
      "DoRA Rank: 16, Alpha: 32\n",
      "Target Modules: ['query_proj', 'key_proj', 'value_proj', 'dense']\n",
      "\u001b[2K\u001b[34mGPU: NVIDIA H100 80GB HBM3g (1/1 containers active)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "VRAM: 85.0 GB\n",
      "======================================================================\n",
      "\n",
      "ðŸ“¦ Loading dataset from HuggingFace Hub...\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31mdal.com/apps/ari-s-123/main/ap-ykyVX5Cnd5OuokEgq1VWZC\u001b[0m\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AGenerating train split:   0%|          | 0/125327 [00:00<?, ? examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125327/125327 [00:00<00:00, 527307.90 examples/s]Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125327/125327 [00:00<00:00, 525196.84 examples/s]\n",
      "\n",
      "\u001b[2K\u001b[31m\u001b[1AGenerating test split:   0%|          | 0/31361 [00:00<?, ? examples/s]Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31361/31361 [00:00<00:00, 538918.32 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m   Train samples: 125,327in/ap-ykyVX5Cnd5OuokEgq1VWZC\u001b[0m\n",
      "   Test samples: 31,361\n",
      "\n",
      "ðŸ·ï¸  Building label vocabulary...\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[4;37mhttps://modal.com/apps/ari-s-123/main/ap-ykyVX5Cnd5OuokEgq1VWZC\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[31mTraceback (most recent call last):\n",
      "  File \"/pkg/modal/_runtime/container_io_manager.py\", line 946, in handle_input_exception\n",
      "    yield\n",
      "  File \"/pkg/modal/_container_entrypoint.py\", line 254, in run_input_sync\n",
      "    values = io_context.call_function_sync()\n",
      "  File \"/pkg/modal/_runtime/container_io_manager.py\", line 225, in call_function_sync\n",
      "    expected_value_or_values = self.finalized_function.callable(*args, **kwargs)\n",
      "  File \"C:\\Users\\Ari\\AppData\\Local\\Temp\\ipykernel_4260\\1605310723.py\", line 113, in train_deberta_dora\n",
      "  File \"C:\\Users\\Ari\\AppData\\Local\\Temp\\ipykernel_4260\\2108201250.py\", line 29, in build_label_vocabulary\n",
      "AttributeError: 'list' object has no attribute 'get'\n",
      "\u001b[0m\u001b[33mStopping app - uncaught exception raised in remote container: AttributeError(\"'list' object has no attribute 'get'\").\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m modal\u001b[38;5;241m.\u001b[39menable_output():\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m app\u001b[38;5;241m.\u001b[39mrun():\n\u001b[1;32m---> 67\u001b[0m         results \u001b[38;5;241m=\u001b[39m train_deberta_dora\u001b[38;5;241m.\u001b[39mremote(config_dict)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Display final results (this runs after training completes)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_object.py:339\u001b[0m, in \u001b[0;36mlive_method.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhydrate()\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_functions.py:1760\u001b[0m, in \u001b[0;36m_Function.remote\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_generator:\n\u001b[0;32m   1756\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidError(\n\u001b[0;32m   1757\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA generator function cannot be called with `.remote(...)`. Use `.remote_gen(...)` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1758\u001b[0m     )\n\u001b[1;32m-> 1760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_function(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_functions.py:1704\u001b[0m, in \u001b[0;36m_Function._call_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m     invocation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _Invocation\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m   1697\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1698\u001b[0m         args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1701\u001b[0m         function_call_invocation_type\u001b[38;5;241m=\u001b[39mapi_pb2\u001b[38;5;241m.\u001b[39mFUNCTION_CALL_INVOCATION_TYPE_SYNC,\n\u001b[0;32m   1702\u001b[0m     )\n\u001b[1;32m-> 1704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m invocation\u001b[38;5;241m.\u001b[39mrun_function()\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_functions.py:291\u001b[0m, in \u001b[0;36m_Invocation.run_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m ctx\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mretry_policy\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39msync_client_retries_enabled\n\u001b[0;32m    289\u001b[0m ):\n\u001b[0;32m    290\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_output()\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _process_result(item\u001b[38;5;241m.\u001b[39mresult, item\u001b[38;5;241m.\u001b[39mdata_format, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# User errors including timeouts are managed by the user specified retry policy.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m user_retry_manager \u001b[38;5;241m=\u001b[39m RetryManager(ctx\u001b[38;5;241m.\u001b[39mretry_policy)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_utils\\function_utils.py:533\u001b[0m, in \u001b[0;36m_process_result\u001b[1;34m(result, data_format, stub, client)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_with_hints(exc)\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteError(result\u001b[38;5;241m.\u001b[39mexception)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m<ta-01KBR676XGSYD4E5EZ51R6GQTJ>:C:\\Users\\Ari\\AppData\\Local\\Temp\\ipykernel_4260\\1605310723.py:113\u001b[0m, in \u001b[0;36mtrain_deberta_dora\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KBR676XGSYD4E5EZ51R6GQTJ>:C:\\Users\\Ari\\AppData\\Local\\Temp\\ipykernel_4260\\2108201250.py:29\u001b[0m, in \u001b[0;36mbuild_label_vocabulary\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "# Create your training configuration\n",
    "# Modify these values as needed before running\n",
    "config = TrainingConfig(\n",
    "    # Model settings\n",
    "    model_name=\"microsoft/deberta-v3-large\",\n",
    "    dataset_name=\"Ari-S-123/better-english-pii-anonymizer\",\n",
    "    hub_model_id=\"Ari-S-123/deberta-v3-large-pii-dora\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # DoRA settings (per PLAN.md)\n",
    "    lora_r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"],\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_patience=3,\n",
    "    \n",
    "    # Hub upload\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸš€ DISPATCHING TRAINING TO MODAL H100 GPU\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Dataset: {config.dataset_name}\")\n",
    "print(f\"DoRA Rank: {config.lora_r}, Alpha: {config.lora_alpha}\")\n",
    "print(f\"Learning Rate: {config.learning_rate}\")\n",
    "print(f\"Epochs: {config.num_train_epochs}\")\n",
    "print(f\"Batch Size: {config.per_device_train_batch_size} (effective: {config.per_device_train_batch_size * config.gradient_accumulation_steps})\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nThis will:\")\n",
    "print(\"  1. Build the Docker image (cached after first run)\")\n",
    "print(\"  2. Spin up an H100 GPU on Modal\")\n",
    "print(\"  3. Download dataset and model from HuggingFace\")\n",
    "print(\"  4. Train with DoRA for up to 5 epochs (early stopping enabled)\")\n",
    "print(\"  5. Push final adapter to HuggingFace Hub\")\n",
    "print(\"\\nLogs will stream below. This may take 1-3 hours depending on dataset size.\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Convert dataclass to dict for serialization across Modal boundary\n",
    "config_dict = asdict(config)\n",
    "\n",
    "# =============================================================================\n",
    "# THE KEY FIX: Wrap .remote() calls inside app.run() context manager\n",
    "# =============================================================================\n",
    "# When running Modal from a notebook (not via `modal run`), you must explicitly\n",
    "# \"run\" the app. The context manager:\n",
    "#   1. Connects to Modal's API\n",
    "#   2. Registers all functions and their metadata (hydration)\n",
    "#   3. Builds/fetches the Docker image if needed\n",
    "#   4. Keeps the connection alive while your code runs\n",
    "#\n",
    "# modal.enable_output() ensures logs stream back to your notebook in real-time.\n",
    "\n",
    "with modal.enable_output():\n",
    "    with app.run():\n",
    "        results = train_deberta_dora.remote(config_dict)\n",
    "\n",
    "# Display final results (this runs after training completes)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… TRAINING COMPLETE - RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227f224",
   "metadata": {},
   "source": [
    "## How to Use the Trained Model\n",
    "\n",
    "After training completes, use the adapter like this:\n",
    "\n",
    "```python\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Load base model and apply your DoRA adapter\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-large\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"Ari-S-123/deberta-v3-large-pii-dora\"  # Your trained adapter\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Ari-S-123/deberta-v3-large-pii-dora\"\n",
    ")\n",
    "\n",
    "# Create NER pipeline for easy inference\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Test it!\n",
    "text = \"Contact John Smith at john.smith@email.com or call 555-123-4567\"\n",
    "entities = ner(text)\n",
    "\n",
    "for ent in entities:\n",
    "    print(f\"  {ent['entity_group']}: '{ent['word']}' (confidence: {ent['score']:.2%})\")\n",
    "```\n",
    "\n",
    "The model will be available at: https://huggingface.co/Ari-S-123/deberta-v3-large-pii-dora\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
