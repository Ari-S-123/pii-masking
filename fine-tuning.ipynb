{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890c3d39",
   "metadata": {},
   "source": [
    "# Modal Fine-Tuning Script for DeBERTa-v3-large with DoRA\n",
    "\n",
    "Fine-tunes microsoft/deberta-v3-large for PII Named Entity Recognition using Weight-Decomposed Low-Rank Adaptation\n",
    "(DoRA) on an H100 GPU.\n",
    "\n",
    "Dataset: Ari-S-123/better-english-pii-anonymizer Base Model: microsoft/deberta-v3-large Adapter Method: DoRA (via PEFT\n",
    "library)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd9b0a",
   "metadata": {},
   "source": [
    "## Set up Modal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33464ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: modal in c:\\users\\ari\\anaconda3\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (3.11.10)\n",
      "Requirement already satisfied: cbor2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (5.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (2025.4.26)\n",
      "Requirement already satisfied: click~=8.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (8.1.8)\n",
      "Requirement already satisfied: grpclib<0.4.9,>=0.4.7 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.4.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<7.0,>=3.19 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (6.33.1)\n",
      "Requirement already satisfied: rich>=12.0.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (13.9.4)\n",
      "Requirement already satisfied: synchronicity~=0.10.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.10.5)\n",
      "Requirement already satisfied: toml in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.10.2)\n",
      "Requirement already satisfied: typer>=0.9 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.9.0)\n",
      "Requirement already satisfied: types-certifi in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (2021.10.8.3)\n",
      "Requirement already satisfied: types-toml in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (0.10.8.20240310)\n",
      "Requirement already satisfied: watchfiles in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (1.1.1)\n",
      "Requirement already satisfied: typing_extensions~=4.6 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from modal) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ari\\anaconda3\\lib\\site-packages (from click~=8.1->modal) (0.4.6)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from grpclib<0.4.9,>=0.4.7->modal) (4.3.0)\n",
      "Requirement already satisfied: multidict in c:\\users\\ari\\anaconda3\\lib\\site-packages (from grpclib<0.4.9,>=0.4.7->modal) (6.1.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from h2<5,>=3.1.0->grpclib<0.4.9,>=0.4.7->modal) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from h2<5,>=3.1.0->grpclib<0.4.9,>=0.4.7->modal) (4.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from rich>=12.0.0->modal) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from rich>=12.0.0->modal) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->modal) (0.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from aiohttp->modal) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->modal) (3.7)\n",
      "Requirement already satisfied: anyio>=3.0.0 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from watchfiles->modal) (4.7.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ari\\anaconda3\\lib\\site-packages (from anyio>=3.0.0->watchfiles->modal) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f295688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The web browser should have opened for you to authenticate and get an API \n",
      "token.\n",
      "If it didn't, please copy this URL into your web browser manually:\n",
      "\n",
      "‚†ã Waiting for authentication in the web browser\n",
      "https://modal.com/token-flow/tf-Z8JxPJBG8wQFuCqXReNSQa\n",
      "\n",
      "‚†ã Waiting for authentication in the web browser\n",
      "‚†ã Waiting for authentication in the web browser\n",
      "\n",
      "‚†ã Waiting for token flow to complete...\n",
      "‚†ô Waiting for token flow to complete...\n",
      "‚†π Waiting for token flow to complete...\n",
      "‚†∏ Waiting for token flow to complete...\n",
      "‚†º Waiting for token flow to complete...\n",
      "‚†¥ Waiting for token flow to complete...\n",
      "‚†¶ Waiting for token flow to complete...\n",
      "‚†ß Waiting for token flow to complete...\n",
      "‚†á Waiting for token flow to complete...\n",
      "‚†è Waiting for token flow to complete...\n",
      "‚†ã Waiting for token flow to complete...\n",
      "‚†ô Waiting for token flow to complete...\n",
      "‚†π Waiting for token flow to complete...\n",
      "‚†∏ Waiting for token flow to complete...\n",
      "‚†º Waiting for token flow to complete...\n",
      "‚†¶ Waiting for token flow to complete...\n",
      "‚†ß Waiting for token flow to complete...\n",
      "‚†á Waiting for token flow to complete...\n",
      "‚†è Waiting for token flow to complete...\n",
      "‚†ã Waiting for token flow to complete...\n",
      "‚†ô Waiting for token flow to complete...\n",
      "‚†π Waiting for token flow to complete...\n",
      "‚†∏ Waiting for token flow to complete...\n",
      "‚†∏ Waiting for token flow to complete...\n",
      "\n",
      "Web authentication finished successfully!\n",
      "Token is connected to the ari-s-123 workspace.\n",
      "Verifying token against https://api.modal.com\n",
      "Token verified successfully!\n",
      "‚†ã Storing token\n",
      "\n",
      "Token written to C:\\Users\\Ari/.modal.toml in profile ari-s-123.\n"
     ]
    }
   ],
   "source": [
    "!python -m modal setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb3fc0",
   "metadata": {},
   "source": [
    "## Imports and Modal App Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c16c2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modal app configured\n",
      "  App name: pii-deberta-dora-finetune\n",
      "  Volume mount: /checkpoints\n"
     ]
    }
   ],
   "source": [
    "import modal\n",
    "\n",
    "APP_NAME: str = \"pii-deberta-dora-finetune\"\n",
    "\n",
    "# Define the Modal App\n",
    "app = modal.App(name=APP_NAME)\n",
    "\n",
    "# HuggingFace Hub configuration\n",
    "# Create the secret first by running in a terminal:\n",
    "#   modal secret create huggingface HF_TOKEN=hf_your_token_here\n",
    "HF_SECRET = modal.Secret.from_name(\"huggingface\")\n",
    "\n",
    "# Create a persistent volume to store checkpoints and final model\n",
    "# This persists between runs so you don't lose progress if something fails\n",
    "volume = modal.Volume.from_name(\"pii-model-checkpoints\", create_if_missing=True)\n",
    "VOLUME_MOUNT_PATH: str = \"/checkpoints\"\n",
    "\n",
    "print(\"‚úì Modal app configured\")\n",
    "print(f\"  App name: {APP_NAME}\")\n",
    "print(f\"  Volume mount: {VOLUME_MOUNT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed616d",
   "metadata": {},
   "source": [
    "## Docker Image Definition\n",
    "\n",
    "Build a custom image with all required dependencies for DoRA fine-tuning. The image is cached, so subsequent runs are\n",
    "fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45cd1d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training image defined\n",
      "  Base: debian_slim (Python 3.13)\n",
      "  PyTorch: >=2.4.0 (CUDA 12.1)\n",
      "  Key packages: transformers, peft, datasets, seqeval\n"
     ]
    }
   ],
   "source": [
    "training_image = (\n",
    "    modal.Image.debian_slim(python_version=\"3.13\")\n",
    "    # Install system dependencies\n",
    "    .apt_install(\"git\", \"curl\", \"build-essential\")\n",
    "    # Install PyTorch with CUDA 12.1 support (H100 compatible)\n",
    "    .pip_install(\n",
    "        \"torch>=2.4.0\",\n",
    "        \"torchvision\",\n",
    "        \"torchaudio\",\n",
    "        extra_index_url=\"https://download.pytorch.org/whl/cu121\",\n",
    "    )\n",
    "    # Install core ML dependencies\n",
    "    # Let pip resolve compatible versions by using minimum version constraints\n",
    "    # rather than exact pins, which cause conflicts\n",
    "    .pip_install(\n",
    "        # Transformers ecosystem - use compatible ranges\n",
    "        \"transformers>=4.46.0\",\n",
    "        \"peft>=0.14.0\",\n",
    "        \"accelerate>=1.0.0\",\n",
    "        \"datasets>=3.0.0\",\n",
    "        \"huggingface_hub>=0.26.0\",  # Don't pin exact version - let pip resolve\n",
    "        # Training utilities\n",
    "        \"scikit-learn>=1.5.0\",\n",
    "        \"seqeval>=1.2.2\",\n",
    "        \"numpy>=1.26.0\",\n",
    "        \"tqdm>=4.66.0\",\n",
    "        # Sentencepiece for DeBERTa tokenizer\n",
    "        \"sentencepiece>=0.2.0\",\n",
    "        \"protobuf>=4.25.0\",\n",
    "        # BitsAndBytes for potential quantization\n",
    "        \"bitsandbytes>=0.44.0\",\n",
    "        \"tensorboard>=2.15.0\",\n",
    "        gpu=\"H100\",  # Build with H100 to ensure CUDA compatibility\n",
    "    )\n",
    "    # Set environment variables\n",
    "    .env({\n",
    "        \"HF_HOME\": \"/root/.cache/huggingface\",\n",
    "        \"TRANSFORMERS_CACHE\": \"/root/.cache/huggingface/hub\",\n",
    "        \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "    })\n",
    ")\n",
    "\n",
    "print(\"‚úì Training image defined\")\n",
    "print(\"  Base: debian_slim (Python 3.13)\")\n",
    "print(\"  PyTorch: >=2.4.0 (CUDA 12.1)\")\n",
    "print(\"  Key packages: transformers, peft, datasets, seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262aae08",
   "metadata": {},
   "source": [
    "## Training Configuration Dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f11a5814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì TrainingConfig dataclass defined\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Configuration for DeBERTa-v3-large DoRA fine-tuning on PII detection.\n",
    "\n",
    "    This configuration is optimized for an H100 GPU (80GB VRAM) and targets\n",
    "    the specific modules recommended for DeBERTa attention layers.\n",
    "\n",
    "    Attributes:\n",
    "        model_name: HuggingFace model identifier for the base model.\n",
    "        dataset_name: HuggingFace dataset identifier for training data.\n",
    "        output_dir: Local directory for saving checkpoints during training.\n",
    "        hub_model_id: HuggingFace Hub repository ID for pushing the final adapter.\n",
    "        max_length: Maximum sequence length for tokenization (DeBERTa supports 512).\n",
    "        learning_rate: AdamW learning rate. DoRA typically needs slightly lower LR.\n",
    "        num_train_epochs: Maximum number of training epochs.\n",
    "        per_device_train_batch_size: Batch size per GPU for training.\n",
    "        per_device_eval_batch_size: Batch size per GPU for evaluation.\n",
    "        gradient_accumulation_steps: Number of steps to accumulate gradients.\n",
    "        warmup_ratio: Proportion of training steps for learning rate warmup.\n",
    "        weight_decay: L2 regularization coefficient.\n",
    "        lora_r: LoRA/DoRA rank (higher = more parameters, better capacity).\n",
    "        lora_alpha: LoRA/DoRA scaling factor (typically 2x rank).\n",
    "        lora_dropout: Dropout probability for LoRA layers.\n",
    "        target_modules: DeBERTa attention modules to apply DoRA to.\n",
    "        early_stopping_patience: Number of evaluations without improvement before stopping.\n",
    "        eval_steps: Evaluate every N steps.\n",
    "        save_steps: Save checkpoint every N steps.\n",
    "        logging_steps: Log metrics every N steps.\n",
    "        seed: Random seed for reproducibility.\n",
    "        bf16: Use bfloat16 mixed precision (recommended for H100).\n",
    "        dataloader_num_workers: Number of workers for data loading.\n",
    "        push_to_hub: Whether to push the final model to HuggingFace Hub.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model and dataset\n",
    "    model_name: str = \"microsoft/deberta-v3-large\"\n",
    "    dataset_name: str = \"Ari-S-123/better-english-pii-anonymizer\"\n",
    "    output_dir: str = \"/checkpoints/pii-deberta-dora\"\n",
    "    hub_model_id: str = \"Ari-S-123/deberta-v3-large-pii-dora\"\n",
    "\n",
    "    # Tokenization\n",
    "    max_length: int = 512\n",
    "\n",
    "    # Training hyperparameters\n",
    "    learning_rate: float = 2e-5\n",
    "    num_train_epochs: int = 5\n",
    "    per_device_train_batch_size: int = 16\n",
    "    per_device_eval_batch_size: int = 32\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "\n",
    "    # DoRA configuration (per PLAN.md recommendations)\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: list[str] = field(default_factory=lambda: [\n",
    "        \"query_proj\",\n",
    "        \"key_proj\",\n",
    "        \"value_proj\",\n",
    "        \"dense\",\n",
    "    ])\n",
    "\n",
    "    # Early stopping and evaluation\n",
    "    early_stopping_patience: int = 3\n",
    "    eval_steps: int = 500\n",
    "    save_steps: int = 500\n",
    "    logging_steps: int = 100\n",
    "\n",
    "    # Reproducibility and performance\n",
    "    seed: int = 69\n",
    "    bf16: bool = True  # H100 has excellent bf16 support\n",
    "    dataloader_num_workers: int = 4\n",
    "\n",
    "    # Hub configuration\n",
    "    push_to_hub: bool = True\n",
    "\n",
    "\n",
    "print(\"‚úì TrainingConfig dataclass defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee12799d",
   "metadata": {},
   "source": [
    "## Label Mapping and Tokenization Functions\n",
    "\n",
    "These functions are defined at module level so they can be serialized and sent to the remote Modal container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d42ef2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined: build_label_vocabulary, tokenize_and_align_labels\n",
      "  Now handles both list-of-dicts and dict-of-lists formats for privacy_mask\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "def build_label_vocabulary(dataset: Any) -> tuple[list[str], dict[str, int], dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Build a complete BIO label vocabulary from the dataset's privacy_mask annotations.\n",
    "\n",
    "    Scans all unique entity labels in the dataset and creates a BIO-formatted\n",
    "    label vocabulary with \"O\" (Outside) plus B-/I- prefixed entity labels.\n",
    "\n",
    "    This function handles BOTH possible data formats:\n",
    "        1. List of dicts: [{\"label\": \"EMAIL\", \"start\": 10, ...}, ...]\n",
    "        2. Dict of lists: {\"label\": [\"EMAIL\", ...], \"start\": [10, ...], ...}\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace Dataset or DatasetDict containing privacy_mask field.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - label_list: Ordered list of all BIO labels (e.g., [\"O\", \"B-EMAIL\", \"I-EMAIL\", ...])\n",
    "            - label_to_id: Mapping from label string to integer ID\n",
    "            - id_to_label: Mapping from integer ID to label string\n",
    "    \"\"\"\n",
    "    unique_labels: set[str] = set()\n",
    "\n",
    "    # Handle both Dataset and DatasetDict\n",
    "    splits = dataset.keys() if hasattr(dataset, \"keys\") else [\"train\"]\n",
    "\n",
    "    for split in splits:\n",
    "        split_data = dataset[split] if hasattr(dataset, \"keys\") else dataset\n",
    "        for example in split_data:\n",
    "            privacy_mask = example.get(\"privacy_mask\", [])\n",
    "\n",
    "            # Handle BOTH data formats:\n",
    "            # Format 1: List of dicts - [{\"label\": \"EMAIL\", \"start\": 10, ...}, ...]\n",
    "            # Format 2: Dict of lists - {\"label\": [\"EMAIL\", ...], \"start\": [10, ...], ...}\n",
    "            if isinstance(privacy_mask, list):\n",
    "                # Format 1: List of entity dictionaries\n",
    "                for entity in privacy_mask:\n",
    "                    if isinstance(entity, dict) and \"label\" in entity:\n",
    "                        unique_labels.add(entity[\"label\"])\n",
    "            elif isinstance(privacy_mask, dict):\n",
    "                # Format 2: Dict with parallel lists (HuggingFace columnar format)\n",
    "                labels = privacy_mask.get(\"label\", [])\n",
    "                if isinstance(labels, list):\n",
    "                    unique_labels.update(labels)\n",
    "                elif isinstance(labels, str):\n",
    "                    # Single entity case\n",
    "                    unique_labels.add(labels)\n",
    "\n",
    "    # Sort for deterministic ordering\n",
    "    sorted_labels = sorted(unique_labels)\n",
    "\n",
    "    # Build BIO vocabulary: O + B-X/I-X for each entity type\n",
    "    label_list: list[str] = [\"O\"]\n",
    "    for label in sorted_labels:\n",
    "        label_list.append(f\"B-{label}\")\n",
    "        label_list.append(f\"I-{label}\")\n",
    "\n",
    "    label_to_id: dict[str, int] = {label: i for i, label in enumerate(label_list)}\n",
    "    id_to_label: dict[int, str] = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "    return label_list, label_to_id, id_to_label\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(\n",
    "    examples: dict[str, Any],\n",
    "    tokenizer: Any,\n",
    "    label_to_id: dict[str, int],\n",
    "    max_length: int = 512,\n",
    ") -> dict[str, list]:\n",
    "    \"\"\"\n",
    "    Tokenize text and align span annotations to subword token boundaries.\n",
    "\n",
    "    This function handles the critical alignment between character-level entity\n",
    "    spans (from privacy_mask) and subword token-level BIO labels required for\n",
    "    DeBERTa's token classification head.\n",
    "\n",
    "    Handles BOTH data formats:\n",
    "        1. List of dicts: [{\"label\": \"EMAIL\", \"start\": 10, \"end\": 25}, ...]\n",
    "        2. Dict of lists: {\"label\": [\"EMAIL\", ...], \"start\": [10, ...], \"end\": [25, ...]}\n",
    "\n",
    "    The alignment strategy:\n",
    "        1. Tokenize with offset_mapping to get character‚Üítoken correspondence\n",
    "        2. For each entity span, find overlapping tokens\n",
    "        3. Assign B- to first overlapping token, I- to subsequent tokens\n",
    "        4. Special tokens ([CLS], [SEP], [PAD]) get label -100 (ignored in loss)\n",
    "\n",
    "    Args:\n",
    "        examples: Batch of examples with \"source_text\" and \"privacy_mask\" fields.\n",
    "        tokenizer: HuggingFace tokenizer instance (DeBERTa tokenizer).\n",
    "        label_to_id: Mapping from BIO label strings to integer IDs.\n",
    "        max_length: Maximum sequence length for truncation.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with keys:\n",
    "            - input_ids: Tokenized input sequences\n",
    "            - attention_mask: Attention masks (1 for real tokens, 0 for padding)\n",
    "            - labels: Aligned BIO label IDs (-100 for special tokens)\n",
    "    \"\"\"\n",
    "    # Tokenize WITHOUT padding (let DataCollator handle it dynamically)\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"source_text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        # REMOVED: padding=\"max_length\"  <-- Don't pad here\n",
    "        return_offsets_mapping=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    all_labels: list[list[int]] = []\n",
    "\n",
    "    for batch_idx, offset_mapping in enumerate(tokenized[\"offset_mapping\"]):\n",
    "        # Initialize all labels as \"O\" (Outside)\n",
    "        labels: list[int] = [label_to_id[\"O\"]] * len(offset_mapping)\n",
    "\n",
    "        # Get entity spans for this example\n",
    "        privacy_mask = examples[\"privacy_mask\"][batch_idx]\n",
    "\n",
    "        # Normalize to a consistent format: list of (label, start, end) tuples\n",
    "        entities: list[tuple[str, int, int]] = []\n",
    "\n",
    "        if isinstance(privacy_mask, list):\n",
    "            # Format 1: List of entity dictionaries\n",
    "            # [{\"label\": \"EMAIL\", \"start\": 10, \"end\": 25, ...}, ...]\n",
    "            for entity in privacy_mask:\n",
    "                if isinstance(entity, dict):\n",
    "                    ent_label = entity.get(\"label\", \"\")\n",
    "                    ent_start = entity.get(\"start\", 0)\n",
    "                    ent_end = entity.get(\"end\", 0)\n",
    "                    if ent_label and ent_end > ent_start:\n",
    "                        entities.append((ent_label, ent_start, ent_end))\n",
    "\n",
    "        elif isinstance(privacy_mask, dict):\n",
    "            # Format 2: Dict with parallel lists (HuggingFace columnar format)\n",
    "            # {\"label\": [\"EMAIL\", \"PHONE\"], \"start\": [10, 45], \"end\": [25, 57], ...}\n",
    "            entity_labels = privacy_mask.get(\"label\", [])\n",
    "            entity_starts = privacy_mask.get(\"start\", [])\n",
    "            entity_ends = privacy_mask.get(\"end\", [])\n",
    "\n",
    "            # Ensure all are lists (might be single values)\n",
    "            if not isinstance(entity_labels, list):\n",
    "                entity_labels = [entity_labels]\n",
    "            if not isinstance(entity_starts, list):\n",
    "                entity_starts = [entity_starts]\n",
    "            if not isinstance(entity_ends, list):\n",
    "                entity_ends = [entity_ends]\n",
    "\n",
    "            for ent_label, ent_start, ent_end in zip(entity_labels, entity_starts, entity_ends):\n",
    "                if ent_label and ent_end > ent_start:\n",
    "                    entities.append((ent_label, ent_start, ent_end))\n",
    "\n",
    "        # Process each entity span and align to tokens\n",
    "        for ent_label, ent_start, ent_end in entities:\n",
    "            is_first_token = True\n",
    "\n",
    "            for token_idx, (tok_start, tok_end) in enumerate(offset_mapping):\n",
    "                # Skip special tokens (offset is (0, 0) for [CLS], [SEP], [PAD])\n",
    "                if tok_start == tok_end == 0:\n",
    "                    labels[token_idx] = -100  # Ignored in loss calculation\n",
    "                    continue\n",
    "\n",
    "                # Check if this token overlaps with the entity span\n",
    "                if tok_start < ent_end and tok_end > ent_start:\n",
    "                    bio_label = f\"B-{ent_label}\" if is_first_token else f\"I-{ent_label}\"\n",
    "\n",
    "                    # Only assign if the label exists in our vocabulary\n",
    "                    if bio_label in label_to_id:\n",
    "                        labels[token_idx] = label_to_id[bio_label]\n",
    "                        is_first_token = False\n",
    "\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Remove offset_mapping from output (not needed for training)\n",
    "    tokenized.pop(\"offset_mapping\")\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "print(\"‚úì Helper functions defined: build_label_vocabulary, tokenize_and_align_labels\")\n",
    "print(\"  Now handles both list-of-dicts and dict-of-lists formats for privacy_mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c63395a",
   "metadata": {},
   "source": [
    "## Metrics Computation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88702e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Metrics function defined: create_compute_metrics\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_compute_metrics(id_to_label: dict[int, str]) -> callable:\n",
    "    \"\"\"\n",
    "    Create a metrics computation function for NER evaluation.\n",
    "\n",
    "    Uses seqeval library for proper NER metrics (precision, recall, F1) that\n",
    "    account for entity boundaries, not just individual token labels.\n",
    "\n",
    "    Args:\n",
    "        id_to_label: Mapping from integer label IDs to BIO label strings.\n",
    "\n",
    "    Returns:\n",
    "        Callable that computes metrics from EvalPrediction object.\n",
    "    \"\"\"\n",
    "    from seqeval.metrics import (\n",
    "        classification_report,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred: Any) -> dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute NER evaluation metrics from model predictions.\n",
    "\n",
    "        Args:\n",
    "            eval_pred: EvalPrediction object with predictions and label_ids.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing precision, recall, and F1 scores.\n",
    "        \"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Convert IDs to label strings, filtering out special tokens (-100)\n",
    "        true_predictions: list[list[str]] = []\n",
    "        true_labels: list[list[str]] = []\n",
    "\n",
    "        for prediction, label in zip(predictions, labels):\n",
    "            pred_tags: list[str] = []\n",
    "            true_tags: list[str] = []\n",
    "\n",
    "            for pred_id, label_id in zip(prediction, label):\n",
    "                # Skip special tokens (label_id == -100)\n",
    "                if label_id == -100:\n",
    "                    continue\n",
    "\n",
    "                pred_tags.append(id_to_label.get(pred_id, \"O\"))\n",
    "                true_tags.append(id_to_label.get(label_id, \"O\"))\n",
    "\n",
    "            true_predictions.append(pred_tags)\n",
    "            true_labels.append(true_tags)\n",
    "\n",
    "        # Compute seqeval metrics (entity-level, not token-level)\n",
    "        metrics = {\n",
    "            \"precision\": precision_score(true_labels, true_predictions),\n",
    "            \"recall\": recall_score(true_labels, true_predictions),\n",
    "            \"f1\": f1_score(true_labels, true_predictions),\n",
    "        }\n",
    "\n",
    "        # Log detailed classification report to console\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ENTITY-LEVEL CLASSIFICATION REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(classification_report(true_labels, true_predictions))\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "print(\"‚úì Metrics function defined: create_compute_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dda4f9",
   "metadata": {},
   "source": [
    "## Main Training Function (Modal Remote Execution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27f2cbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Remote training function defined: train_deberta_dora\n",
      "  GPU: H100\n",
      "  Timeout: 6 hours\n",
      "  Memory: 64GB\n"
     ]
    }
   ],
   "source": [
    "@app.function(\n",
    "    image=training_image,\n",
    "    gpu=\"H100\",\n",
    "    timeout=60 * 60 * 6,  # 6 hour timeout for full training\n",
    "    secrets=[HF_SECRET],\n",
    "    volumes={VOLUME_MOUNT_PATH: volume},\n",
    "    memory=65536,  # 64GB RAM\n",
    ")\n",
    "def train_deberta_dora(config_dict: dict[str, Any] | None = None) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fine-tune DeBERTa-v3-large with DoRA for PII Named Entity Recognition.\n",
    "\n",
    "    This function runs REMOTELY on a Modal H100 GPU and performs:\n",
    "        1. Dataset loading from HuggingFace Hub\n",
    "        2. Tokenization and label alignment\n",
    "        3. DoRA adapter initialization on attention modules\n",
    "        4. Training with early stopping\n",
    "        5. Final model saving and Hub upload\n",
    "\n",
    "    Args:\n",
    "        config_dict: Training configuration as a dictionary (dataclasses don't\n",
    "                     serialize well across Modal's boundary). Uses defaults if None.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing training metrics and final model location.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "    from transformers import (\n",
    "        AutoModelForTokenClassification,\n",
    "        AutoTokenizer,\n",
    "        DataCollatorForTokenClassification,\n",
    "        EarlyStoppingCallback,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "    )\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    # =========================================================================\n",
    "    # Reconstruct config from dict (dataclasses don't serialize across Modal)\n",
    "    # =========================================================================\n",
    "    if config_dict is None:\n",
    "        config_dict = {}\n",
    "\n",
    "    # Default values\n",
    "    model_name = config_dict.get(\"model_name\", \"microsoft/deberta-v3-large\")\n",
    "    dataset_name = config_dict.get(\"dataset_name\", \"Ari-S-123/better-english-pii-anonymizer\")\n",
    "    output_dir = config_dict.get(\"output_dir\", \"/checkpoints/pii-deberta-dora\")\n",
    "    hub_model_id = config_dict.get(\"hub_model_id\", \"Ari-S-123/deberta-v3-large-pii-dora\")\n",
    "    max_length = config_dict.get(\"max_length\", 512)\n",
    "    learning_rate = config_dict.get(\"learning_rate\", 2e-5)\n",
    "    num_train_epochs = config_dict.get(\"num_train_epochs\", 5)\n",
    "    per_device_train_batch_size = config_dict.get(\"per_device_train_batch_size\", 16)\n",
    "    per_device_eval_batch_size = config_dict.get(\"per_device_eval_batch_size\", 32)\n",
    "    gradient_accumulation_steps = config_dict.get(\"gradient_accumulation_steps\", 2)\n",
    "    warmup_ratio = config_dict.get(\"warmup_ratio\", 0.1)\n",
    "    weight_decay = config_dict.get(\"weight_decay\", 0.01)\n",
    "    lora_r = config_dict.get(\"lora_r\", 16)\n",
    "    lora_alpha = config_dict.get(\"lora_alpha\", 32)\n",
    "    lora_dropout = config_dict.get(\"lora_dropout\", 0.05)\n",
    "    target_modules = config_dict.get(\"target_modules\", [\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"])\n",
    "    early_stopping_patience = config_dict.get(\"early_stopping_patience\", 3)\n",
    "    eval_steps = config_dict.get(\"eval_steps\", 500)\n",
    "    save_steps = config_dict.get(\"save_steps\", 500)\n",
    "    logging_steps = config_dict.get(\"logging_steps\", 100)\n",
    "    seed = config_dict.get(\"seed\", 42)\n",
    "    bf16 = config_dict.get(\"bf16\", True)\n",
    "    dataloader_num_workers = config_dict.get(\"dataloader_num_workers\", 4)\n",
    "    push_to_hub = config_dict.get(\"push_to_hub\", True)\n",
    "\n",
    "    # Authenticate with HuggingFace Hub\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"‚úì Authenticated with HuggingFace Hub\")\n",
    "    else:\n",
    "        print(\"‚ö† No HF_TOKEN found. Hub operations may fail.\")\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DeBERTa-v3-large DoRA Fine-Tuning for PII Detection\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"DoRA Rank: {lora_r}, Alpha: {lora_alpha}\")\n",
    "    print(f\"Target Modules: {target_modules}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 1: Load Dataset\n",
    "    # =========================================================================\n",
    "    print(\"üì¶ Loading dataset from HuggingFace Hub...\")\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    print(f\"   Train samples: {len(dataset['train']):,}\")\n",
    "    print(f\"   Test samples: {len(dataset['test']):,}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 2: Build Label Vocabulary\n",
    "    # =========================================================================\n",
    "    print(\"\\nüè∑Ô∏è  Building label vocabulary...\")\n",
    "    label_list, label_to_id, id_to_label = build_label_vocabulary(dataset)\n",
    "    num_labels = len(label_list)\n",
    "    print(f\"   Total BIO labels: {num_labels}\")\n",
    "    print(f\"   Entity types: {(num_labels - 1) // 2}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 3: Load Tokenizer\n",
    "    # =========================================================================\n",
    "    print(f\"\\nüìù Loading tokenizer: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        add_prefix_space=True,  # Important for consistent tokenization\n",
    "    )\n",
    "    print(f\"   Vocab size: {tokenizer.vocab_size:,}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 4: Tokenize Dataset\n",
    "    # =========================================================================\n",
    "    print(\"\\n‚öôÔ∏è  Tokenizing and aligning labels...\")\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_and_align_labels(\n",
    "            examples,\n",
    "            tokenizer=tokenizer,\n",
    "            label_to_id=label_to_id,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    # NOTE: We intentionally do NOT use num_proc here.\n",
    "    # Multiprocessing requires pickling the tokenize_fn closure, which fails\n",
    "    # because Modal's async runtime or notebook event loops create unpicklable\n",
    "    # asyncio.Task objects that get captured in the closure chain.\n",
    "    # Single-process tokenization is still fast (~2-5 min for 125k examples).\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    print(f\"   Train tokens: {len(tokenized_dataset['train']):,} examples\")\n",
    "    print(f\"   Test tokens: {len(tokenized_dataset['test']):,} examples\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 5: Load Base Model\n",
    "    # =========================================================================\n",
    "    print(f\"\\nü§ñ Loading base model: {model_name}\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id_to_label,\n",
    "        label2id=label_to_id,\n",
    "        dtype=torch.bfloat16 if bf16 else torch.float32,\n",
    "    )\n",
    "    print(f\"   Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 6: Configure DoRA Adapter\n",
    "    # =========================================================================\n",
    "    print(\"\\nüîß Configuring DoRA adapter...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.TOKEN_CLS,\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=target_modules,\n",
    "        use_dora=True,  # Enable Weight-Decomposed Low-Rank Adaptation\n",
    "        bias=\"none\",\n",
    "        inference_mode=False,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    print(\"\\n‚ö° Compiling model with torch.compile()...\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 7: Setup Training Arguments\n",
    "    # =========================================================================\n",
    "    print(\"\\nüìã Configuring training arguments...\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        # Training hyperparameters\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=weight_decay,\n",
    "        # Precision\n",
    "        bf16=bf16,\n",
    "        # Evaluation and saving\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        # Logging\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=logging_steps,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        # Performance\n",
    "        dataloader_num_workers=dataloader_num_workers,\n",
    "        dataloader_pin_memory=True,\n",
    "        # Hub (adapter will be pushed separately)\n",
    "        push_to_hub=False,\n",
    "        # Reproducibility\n",
    "        seed=seed,\n",
    "        data_seed=seed,\n",
    "        # Don't auto-remove columns (torch.compile breaks signature detection)\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 8: Initialize Trainer\n",
    "    # =========================================================================\n",
    "    print(\"\\nüèãÔ∏è Initializing Trainer...\")\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,  # Dynamic padding per batch\n",
    "        label_pad_token_id=-100,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=create_compute_metrics(id_to_label),\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=early_stopping_patience,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 9: Train!\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üöÄ STARTING TRAINING\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úì TRAINING COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Total steps: {train_result.global_step}\")\n",
    "    print(f\"   Training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 10: Final Evaluation\n",
    "    # =========================================================================\n",
    "    print(\"\\nüìä Running final evaluation...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"   Final F1: {eval_results.get('eval_f1', 'N/A'):.4f}\")\n",
    "    print(f\"   Final Precision: {eval_results.get('eval_precision', 'N/A'):.4f}\")\n",
    "    print(f\"   Final Recall: {eval_results.get('eval_recall', 'N/A'):.4f}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 11: Save Model Locally\n",
    "    # =========================================================================\n",
    "    final_output_dir = f\"{output_dir}/final\"\n",
    "    print(f\"\\nüíæ Saving model to {final_output_dir}...\")\n",
    "\n",
    "    # Save adapter weights\n",
    "    model.save_pretrained(final_output_dir)\n",
    "    tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "    # Commit volume to persist checkpoint\n",
    "    volume.commit()\n",
    "    print(\"   ‚úì Checkpoint persisted to Modal volume\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 12: Push to HuggingFace Hub\n",
    "    # =========================================================================\n",
    "    if push_to_hub and hf_token:\n",
    "        print(f\"\\n‚òÅÔ∏è  Pushing adapter to HuggingFace Hub: {hub_model_id}\")\n",
    "        model.push_to_hub(\n",
    "            hub_model_id,\n",
    "            use_auth_token=hf_token,\n",
    "            commit_message=\"DoRA fine-tuned DeBERTa-v3-large for PII detection\",\n",
    "        )\n",
    "        tokenizer.push_to_hub(\n",
    "            hub_model_id,\n",
    "            use_auth_token=hf_token,\n",
    "        )\n",
    "        print(f\"   ‚úì Adapter available at: https://huggingface.co/{hub_model_id}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 13: Return Results\n",
    "    # =========================================================================\n",
    "    results = {\n",
    "        \"training_loss\": train_result.training_loss,\n",
    "        \"global_step\": train_result.global_step,\n",
    "        \"eval_f1\": eval_results.get(\"eval_f1\"),\n",
    "        \"eval_precision\": eval_results.get(\"eval_precision\"),\n",
    "        \"eval_recall\": eval_results.get(\"eval_recall\"),\n",
    "        \"model_path\": final_output_dir,\n",
    "        \"hub_model_id\": hub_model_id if push_to_hub else None,\n",
    "        \"num_labels\": num_labels,\n",
    "        \"trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ FINE-TUNING PIPELINE COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    for key, value in results.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úì Remote training function defined: train_deberta_dora\")\n",
    "print(\"  GPU: H100\")\n",
    "print(\"  Timeout: 6 hours\")\n",
    "print(\"  Memory: 64GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2013a",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "This cell dispatches the training job to Modal's cloud infrastructure. This local machine just orchestrates - all heavy\n",
    "computation happens remotely.\n",
    "\n",
    "In a notebook, we must wrap the .remote() call inside app.run(). This tells Modal to \"start\" the app, hydrate the\n",
    "function metadata, andestablish the connection to Modal's infrastructure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e05c5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ DISPATCHING TRAINING TO MODAL H100 GPU\n",
      "======================================================================\n",
      "Model: microsoft/deberta-v3-large\n",
      "Dataset: Ari-S-123/better-english-pii-anonymizer\n",
      "DoRA Rank: 16, Alpha: 32\n",
      "Learning Rate: 2e-05\n",
      "Epochs: 5\n",
      "Batch Size: 32 (effective: 32)\n",
      "======================================================================\n",
      "\n",
      "This will:\n",
      "  1. Build the Docker image (cached after first run)\n",
      "  2. Spin up an H100 GPU on Modal\n",
      "  3. Download dataset and model from HuggingFace\n",
      "  4. Train with DoRA for up to 5 epochs (early stopping enabled)\n",
      "  5. Push final adapter to HuggingFace Hub\n",
      "\n",
      "Logs will stream below. This may take 1-3 hours depending on dataset size.\n",
      "======================================================================\n",
      "\n",
      "\u001b[2K\u001b[32m‚úì\u001b[0m Initialized. \u001b[37mView run at \u001b[0m\n",
      "\u001b[4;37mhttps://modal.com/apps/ari-s-123/main/ap-yH196O2vUMuQhc08M61ufk\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m Initializing...\n",
      "\u001b[2K\u001b[34m/\u001b[0m Creating objects...objects...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m Creating objects...train_deberta_dora...\n",
      "\u001b[90m‚îî‚îÄ‚îÄ \u001b[0müî® Created function train_deberta_dora.\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m‚úì\u001b[0m Created objects.\n",
      "\u001b[90m‚îî‚îÄ‚îÄ \u001b[0müî® Created function train_deberta_dora.\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mWorker assigned...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mM61ufk\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mM61ufk\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m/usr/local/lib/python3.13/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m‚úì Authenticated with HuggingFace HubO2vUMuQhc08M61ufk\u001b[0m\n",
      "\u001b[2K\u001b[31mNote: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34mdal.com/apps/ari-s-123/main/ap-yH196O2vUMuQhc08M61ufk\u001b[0m\n",
      "======================================================================\n",
      "DeBERTa-v3-large DoRA Fine-Tuning for PII Detection\n",
      "======================================================================\n",
      "Model: microsoft/deberta-v3-large\n",
      "Dataset: Ari-S-123/better-english-pii-anonymizer\n",
      "DoRA Rank: 16, Alpha: 32\n",
      "Target Modules: ['query_proj', 'key_proj', 'value_proj', 'dense']\n",
      "\u001b[2K\u001b[34mGPU: NVIDIA H100 NVLRunning (1/1 containers active)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "VRAM: 100.0 GB\n",
      "======================================================================\n",
      "\n",
      "üì¶ Loading dataset from HuggingFace Hub...\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31mdal.com/apps/ari-s-123/main/ap-yH196O2vUMuQhc08M61ufk\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1AGenerating train split:   0%|          | 0/125327 [00:00<?, ? examples/s]Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125327/125327 [00:00<00:00, 741667.12 examples/s]Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125327/125327 [00:00<00:00, 739394.96 examples/s]\n",
      "\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1AGenerating test split:   0%|          | 0/31361 [00:00<?, ? examples/s]Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31361/31361 [00:00<00:00, 754622.93 examples/s]\n",
      "\u001b[2K\u001b[34m   Train samples: 125,327ng (1/1 containers active)...\u001b[0m\u001b[31m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "   Test samples: 31,361\n",
      "\n",
      "üè∑Ô∏è  Building label vocabulary...\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m   Total BIO labels: 901ain/ap-yH196O2vUMuQhc08M61ufk\u001b[0m\n",
      "   Entity types: 450\n",
      "\n",
      "üìù Loading tokenizer: microsoft/deberta-v3-large\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m/usr/local/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m   Vocab size: 128,000/main/ap-yH196O2vUMuQhc08M61ufk\u001b[0m\n",
      "\n",
      "‚öôÔ∏è  Tokenizing and aligning labels...\n",
      "\u001b[2K\u001b[31m|\u001b[0m\u001b[34m \u001b[0m\u001b[34mRunning (1/1 containers active)...\u001b[0m\u001b[34m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:   0%|          | 0/125327 [00:00<?, ? examples/s][0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:   1%|          | 1000/125327 [00:00<00:14, 8323.17 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:   2%|‚ñè         | 3000/125327 [00:00<00:12, 9659.86 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:   4%|‚ñç         | 5000/125327 [00:00<00:11, 10153.90 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:   6%|‚ñå         | 7000/125327 [00:00<00:11, 10419.29 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:   7%|‚ñã         | 9000/125327 [00:00<00:11, 10479.08 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:   9%|‚ñâ         | 11000/125327 [00:01<00:10, 10509.39 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  10%|‚ñà         | 13000/125327 [00:01<00:13, 8245.72 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  11%|‚ñà         | 14000/125327 [00:01<00:13, 8496.39 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  13%|‚ñà‚ñé        | 16000/125327 [00:01<00:11, 9369.74 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  14%|‚ñà‚ñç        | 18000/125327 [00:01<00:10, 9775.93 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  16%|‚ñà‚ñå        | 20000/125327 [00:02<00:10, 9962.36 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  18%|‚ñà‚ñä        | 22000/125327 [00:02<00:10, 10310.88 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  19%|‚ñà‚ñâ        | 24000/125327 [00:02<00:12, 8263.99 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  21%|‚ñà‚ñà        | 26000/125327 [00:02<00:11, 8805.42 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  22%|‚ñà‚ñà‚ñè       | 28000/125327 [00:02<00:10, 9278.70 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  24%|‚ñà‚ñà‚ñç       | 30000/125327 [00:03<00:09, 9638.81 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  26%|‚ñà‚ñà‚ñå       | 32000/125327 [00:03<00:09, 9838.32 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  27%|‚ñà‚ñà‚ñã       | 34000/125327 [00:03<00:09, 10099.48 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  29%|‚ñà‚ñà‚ñä       | 36000/125327 [00:03<00:10, 8341.30 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  30%|‚ñà‚ñà‚ñâ       | 37000/125327 [00:03<00:10, 8578.83 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  31%|‚ñà‚ñà‚ñà       | 39000/125327 [00:04<00:09, 9119.55 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  33%|‚ñà‚ñà‚ñà‚ñé      | 41000/125327 [00:04<00:08, 9483.81 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  34%|‚ñà‚ñà‚ñà‚ñç      | 43000/125327 [00:04<00:08, 9911.41 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  36%|‚ñà‚ñà‚ñà‚ñå      | 45000/125327 [00:04<00:07, 10102.46 examples/s]p at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  38%|‚ñà‚ñà‚ñà‚ñä      | 47000/125327 [00:04<00:07, 10159.38 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  39%|‚ñà‚ñà‚ñà‚ñâ      | 49000/125327 [00:05<00:09, 8179.57 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  41%|‚ñà‚ñà‚ñà‚ñà      | 51000/125327 [00:05<00:08, 8866.45 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 53000/125327 [00:05<00:07, 9229.80 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 55000/125327 [00:05<00:07, 9584.80 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 56000/125327 [00:05<00:07, 9646.98 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 58000/125327 [00:06<00:06, 9936.78 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 60000/125327 [00:06<00:08, 8130.76 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 61000/125327 [00:06<00:07, 8381.57 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 63000/125327 [00:06<00:06, 9123.32 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 64000/125327 [00:06<00:06, 9279.94 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 66000/125327 [00:07<00:05, 9924.48 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 68000/125327 [00:07<00:05, 10008.18 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 70000/125327 [00:07<00:05, 10198.96 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 72000/125327 [00:07<00:06, 8098.11 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 74000/125327 [00:07<00:05, 8778.51 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 76000/125327 [00:08<00:05, 9216.16 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 77000/125327 [00:08<00:05, 9331.58 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 79000/125327 [00:08<00:04, 9782.87 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 81000/125327 [00:08<00:04, 10121.57 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 83000/125327 [00:08<00:04, 10151.24 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 85000/125327 [00:09<00:04, 8167.28 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 87000/125327 [00:09<00:04, 8784.81 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 89000/125327 [00:09<00:03, 9271.30 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 91000/125327 [00:09<00:03, 9620.01 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 92000/125327 [00:09<00:03, 9610.84 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 94000/125327 [00:10<00:03, 10111.61 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 96000/125327 [00:10<00:02, 10159.37 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 98000/125327 [00:10<00:03, 8270.57 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 100000/125327 [00:10<00:02, 8829.28 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 102000/125327 [00:10<00:02, 9270.23 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 104000/125327 [00:11<00:02, 9600.70 examples/s]p at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 106000/125327 [00:11<00:01, 9994.05 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 108000/125327 [00:11<00:01, 10156.75 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 110000/125327 [00:11<00:01, 8366.90 examples/s]p at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 112000/125327 [00:12<00:01, 9046.95 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 113000/125327 [00:12<00:01, 9135.68 examples/s]p at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 115000/125327 [00:12<00:01, 9513.13 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 117000/125327 [00:12<00:00, 9831.04 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 119000/125327 [00:12<00:00, 10109.08 examples/s]\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 121000/125327 [00:12<00:00, 9284.17 examples/s]\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 122000/125327 [00:13<00:00, 6198.18 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 123000/125327 [00:13<00:00, 5682.62 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 124000/125327 [00:13<00:00, 5295.46 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 125000/125327 [00:14<00:00, 4994.82 examples/s]Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125327/125327 [00:14<00:00, 8856.85 examples/s]\n",
      "\u001b[2K\u001b[31m/\u001b[0m\u001b[31m \u001b[0m\u001b[34mRunning (1/1 containers active)...\u001b[0m\u001b[31m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:   0%|          | 0/31361 [00:00<?, ? examples/s]\u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:   3%|‚ñé         | 1000/31361 [00:00<00:03, 9907.70 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  10%|‚ñâ         | 3000/31361 [00:00<00:02, 10706.35 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  16%|‚ñà‚ñå        | 5000/31361 [00:00<00:02, 10656.35 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  22%|‚ñà‚ñà‚ñè       | 7000/31361 [00:00<00:03, 7758.64 examples/s] \n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  29%|‚ñà‚ñà‚ñä       | 9000/31361 [00:01<00:02, 8633.32 examples/s] app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  35%|‚ñà‚ñà‚ñà‚ñå      | 11000/31361 [00:01<00:02, 9356.33 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 13000/31361 [00:01<00:01, 9768.55 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 15000/31361 [00:01<00:01, 10042.49 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 17000/31361 [00:01<00:01, 10144.13 examples/s]pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 19000/31361 [00:01<00:01, 10216.46 examples/s]\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 21000/31361 [00:02<00:01, 8409.87 examples/s] pp at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 23000/31361 [00:02<00:00, 8911.69 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 25000/31361 [00:02<00:00, 9393.20 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 27000/31361 [00:02<00:00, 9682.91 examples/s]\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ATokenizing:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 29000/31361 [00:03<00:00, 10052.60 examples/s]\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m   Train tokens: 125,327 examples196O2vUMuQhc08M61ufk\u001b[0m\n",
      "   Test tokens: 31,361 examples\n",
      "\n",
      "ü§ñ Loading base model: microsoft/deberta-v3-large\n",
      "\u001b[2K\u001b[31m\u001b[1ATokenizing:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 31000/31361 [00:03<00:00, 7110.44 examples/s] Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31361/31361 [00:03<00:00, 8695.78 examples/s]\n",
      "\u001b[2K\u001b[31m`torch_dtype` is deprecated! Use `dtype` instead!e)...\u001b[0m\u001b[31m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31mSome weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m   Model parameters: 434,935,685H196O2vUMuQhc08M61ufk\u001b[0m\n",
      "\n",
      "üîß Configuring DoRA adapter...\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34mtrainable params: 8,222,597 || all params: 443,158,282 || trainable%: 1.8555\n",
      "\n",
      "‚ö° Compiling model with torch.compile()...\n",
      "\n",
      "üìã Configuring training arguments...\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34mdal.com/apps/ari-s-123/main/ap-yH196O2vUMuQhc08M61ufk\u001b[0m\n",
      "üèãÔ∏è Initializing Trainer...\n",
      "\u001b[2K\u001b[31mDetected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31mThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "\u001b[2K\u001b[34m/\u001b[0m\u001b[31m \u001b[0m\u001b[34mRunning (1/1 containers active)...\u001b[0m\u001b[31m \u001b[0m\u001b[37mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "======================================================================\n",
      "üöÄ STARTING TRAINING\n",
      "======================================================================\n",
      "\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31mTraceback (most recent call last):96O2vUMuQhc08M61ufk\u001b[0m\n",
      "  File \"/pkg/modal/_runtime/container_io_manager.py\", line 946, in handle_input_exception\n",
      "    yield\n",
      "  File \"/pkg/modal/_container_entrypoint.py\", line 254, in run_input_sync\n",
      "    values = io_context.call_function_sync()\n",
      "  File \"/pkg/modal/_runtime/container_io_manager.py\", line 225, in call_function_sync\n",
      "    expected_value_or_values = self.finalized_function.callable(*args, **kwargs)\n",
      "  File \"C:\\Users\\Ari\\AppData\\Local\\Temp\\ipykernel_21020\\1369297583.py\", line 263, in train_deberta_dora\n",
      "  File \"/usr/local/lib/python3.13/site-packages/transformers/trainer.py\", line 2325, in train\n",
      "    return inner_training_loop(\n",
      "        args=args,\n",
      "    ...<2 lines>...\n",
      "        ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "    )\n",
      "  File \"/usr/local/lib/python3.13/site-packages/transformers/trainer.py\", line 2375, in _inner_training_loop\n",
      "    train_dataloader = self.get_train_dataloader()\n",
      "  File \"/usr/local/lib/python3.13/site-packages/transformers/trainer.py\", line 1140, in get_train_dataloader\n",
      "    return self._get_dataloader(\n",
      "           ~~~~~~~~~~~~~~~~~~~~^\n",
      "        dataset=self.train_dataset,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        is_training=True,\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/usr/local/lib/python3.13/site-packages/transformers/trainer.py\", line 1095, in _get_dataloader\n",
      "    dataset = self._remove_unused_columns(dataset, description=description)\n",
      "  File \"/usr/local/lib/python3.13/site-packages/transformers/trainer.py\", line 1021, in _remove_unused_columns\n",
      "    raise ValueError(\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "ValueError: No columns in the dataset match the model's forward method signature: (args, kwargs, label, label_ids). The following columns have been ignored: [labels, attention_mask, input_ids, token_type_ids]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m7mView app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/ari-s-1\u001b[0m\n",
      "\u001b[4;37mhttps://modal.com/apps/ari-s-123/main/ap-yH196O2vUMuQhc08M61ufk\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[33mStopping app - uncaught exception raised in remote container: ValueError(\"No columns in the dataset match the model's forward method signature: (args, kwargs, label, label_ids). The following columns have been ignored: [labels, attention_mask, input_ids, token_type_ids]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\").\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature: (args, kwargs, label, label_ids). The following columns have been ignored: [labels, attention_mask, input_ids, token_type_ids]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m modal\u001b[38;5;241m.\u001b[39menable_output():\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m app\u001b[38;5;241m.\u001b[39mrun():\n\u001b[1;32m---> 67\u001b[0m         results \u001b[38;5;241m=\u001b[39m train_deberta_dora\u001b[38;5;241m.\u001b[39mremote(config_dict)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Display final results (this runs after training completes)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_object.py:339\u001b[0m, in \u001b[0;36mlive_method.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhydrate()\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_functions.py:1760\u001b[0m, in \u001b[0;36m_Function.remote\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_generator:\n\u001b[0;32m   1756\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidError(\n\u001b[0;32m   1757\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA generator function cannot be called with `.remote(...)`. Use `.remote_gen(...)` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1758\u001b[0m     )\n\u001b[1;32m-> 1760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_function(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_functions.py:1704\u001b[0m, in \u001b[0;36m_Function._call_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m     invocation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _Invocation\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m   1697\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1698\u001b[0m         args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1701\u001b[0m         function_call_invocation_type\u001b[38;5;241m=\u001b[39mapi_pb2\u001b[38;5;241m.\u001b[39mFUNCTION_CALL_INVOCATION_TYPE_SYNC,\n\u001b[0;32m   1702\u001b[0m     )\n\u001b[1;32m-> 1704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m invocation\u001b[38;5;241m.\u001b[39mrun_function()\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_functions.py:291\u001b[0m, in \u001b[0;36m_Invocation.run_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m ctx\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mretry_policy\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39msync_client_retries_enabled\n\u001b[0;32m    289\u001b[0m ):\n\u001b[0;32m    290\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_output()\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _process_result(item\u001b[38;5;241m.\u001b[39mresult, item\u001b[38;5;241m.\u001b[39mdata_format, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# User errors including timeouts are managed by the user specified retry policy.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m user_retry_manager \u001b[38;5;241m=\u001b[39m RetryManager(ctx\u001b[38;5;241m.\u001b[39mretry_policy)\n",
      "File \u001b[1;32mc:\\Users\\Ari\\anaconda3\\Lib\\site-packages\\modal\\_utils\\function_utils.py:533\u001b[0m, in \u001b[0;36m_process_result\u001b[1;34m(result, data_format, stub, client)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_with_hints(exc)\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteError(result\u001b[38;5;241m.\u001b[39mexception)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m<ta-01KBRK177C17N66JST319PBYXC>:C:\\Users\\Ari\\AppData\\Local\\Temp\\ipykernel_21020\\1369297583.py:263\u001b[0m, in \u001b[0;36mtrain_deberta_dora\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KBRK177C17N66JST319PBYXC>:/usr/local/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KBRK177C17N66JST319PBYXC>:/usr/local/lib/python3.13/site-packages/transformers/trainer.py:2375\u001b[0m, in \u001b[0;36m_inner_training_loop\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KBRK177C17N66JST319PBYXC>:/usr/local/lib/python3.13/site-packages/transformers/trainer.py:1140\u001b[0m, in \u001b[0;36mget_train_dataloader\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KBRK177C17N66JST319PBYXC>:/usr/local/lib/python3.13/site-packages/transformers/trainer.py:1095\u001b[0m, in \u001b[0;36m_get_dataloader\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KBRK177C17N66JST319PBYXC>:/usr/local/lib/python3.13/site-packages/transformers/trainer.py:1021\u001b[0m, in \u001b[0;36m_remove_unused_columns\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No columns in the dataset match the model's forward method signature: (args, kwargs, label, label_ids). The following columns have been ignored: [labels, attention_mask, input_ids, token_type_ids]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "# Create your training configuration\n",
    "# Modify these values as needed before running\n",
    "config = TrainingConfig(\n",
    "    # Model settings\n",
    "    model_name=\"microsoft/deberta-v3-large\",\n",
    "    dataset_name=\"Ari-S-123/better-english-pii-anonymizer\",\n",
    "    hub_model_id=\"Ari-S-123/deberta-v3-large-pii-dora\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=1,\n",
    "    \n",
    "    # DoRA settings (per PLAN.md)\n",
    "    lora_r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"],\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_patience=3,\n",
    "    \n",
    "    # Hub upload\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ DISPATCHING TRAINING TO MODAL H100 GPU\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Dataset: {config.dataset_name}\")\n",
    "print(f\"DoRA Rank: {config.lora_r}, Alpha: {config.lora_alpha}\")\n",
    "print(f\"Learning Rate: {config.learning_rate}\")\n",
    "print(f\"Epochs: {config.num_train_epochs}\")\n",
    "print(f\"Batch Size: {config.per_device_train_batch_size} (effective: {config.per_device_train_batch_size * config.gradient_accumulation_steps})\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nThis will:\")\n",
    "print(\"  1. Build the Docker image (cached after first run)\")\n",
    "print(\"  2. Spin up an H100 GPU on Modal\")\n",
    "print(\"  3. Download dataset and model from HuggingFace\")\n",
    "print(\"  4. Train with DoRA for up to 5 epochs (early stopping enabled)\")\n",
    "print(\"  5. Push final adapter to HuggingFace Hub\")\n",
    "print(\"\\nLogs will stream below. This may take 1-3 hours depending on dataset size.\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Convert dataclass to dict for serialization across Modal boundary\n",
    "config_dict = asdict(config)\n",
    "\n",
    "# =============================================================================\n",
    "# THE KEY FIX: Wrap .remote() calls inside app.run() context manager\n",
    "# =============================================================================\n",
    "# When running Modal from a notebook (not via `modal run`), you must explicitly\n",
    "# \"run\" the app. The context manager:\n",
    "#   1. Connects to Modal's API\n",
    "#   2. Registers all functions and their metadata (hydration)\n",
    "#   3. Builds/fetches the Docker image if needed\n",
    "#   4. Keeps the connection alive while your code runs\n",
    "#\n",
    "# modal.enable_output() ensures logs stream back to your notebook in real-time.\n",
    "\n",
    "with modal.enable_output():\n",
    "    with app.run():\n",
    "        results = train_deberta_dora.remote(config_dict)\n",
    "\n",
    "# Display final results (this runs after training completes)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING COMPLETE - RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227f224",
   "metadata": {},
   "source": [
    "## How to Use the Trained Model\n",
    "\n",
    "After training completes, use the adapter like this:\n",
    "\n",
    "```python\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "\n",
    "# Load base model and apply your DoRA adapter\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-large\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"Ari-S-123/deberta-v3-large-pii-dora\"  # Your trained adapter\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Ari-S-123/deberta-v3-large-pii-dora\"\n",
    ")\n",
    "\n",
    "# Create NER pipeline for easy inference\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Test it!\n",
    "text = \"Contact John Smith at john.smith@email.com or call 555-123-4567\"\n",
    "entities = ner(text)\n",
    "\n",
    "for ent in entities:\n",
    "    print(f\"  {ent['entity_group']}: '{ent['word']}' (confidence: {ent['score']:.2%})\")\n",
    "```\n",
    "\n",
    "The model will be available at: https://huggingface.co/Ari-S-123/deberta-v3-large-pii-dora\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
