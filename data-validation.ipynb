{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c97d70",
   "metadata": {},
   "source": [
    "# Synthetic PII Data Validation using OpenAI GPT-5.1\n",
    "\n",
    "This notebook validates the synthetically-generated PII training data\n",
    "using OpenAI's GPT-5.1 model with low reasoning effort via the Batch API.\n",
    "\n",
    "The validation performs semantic analysis that programmatic checks cannot:\n",
    "\n",
    "-   Text coherence and naturalness\n",
    "-   Entity label correctness\n",
    "-   Span boundary accuracy\n",
    "-   Feature dimension characteristic verification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4919e7",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1285deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment loaded successfully\n",
      "  OPENAI_API_KEY: ********...GLEA\n",
      "  Model: gpt-5.1-2025-11-13\n",
      "  Reasoning Effort: low\n",
      "  Synthetic Data Path: data\\synthetic\\synthetic_samples.jsonl\n",
      "  Output Directory: data\\validation\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify required API key is present\n",
    "OPENAI_API_KEY: str | None = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        \"OPENAI_API_KEY not found in environment. \"\n",
    "        \"Create a .env file with your OpenAI API key.\"\n",
    "    )\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID: str = \"gpt-5.1-2025-11-13\"\n",
    "REASONING_EFFORT: str = \"low\"\n",
    "BATCH_COMPLETION_WINDOW: str = \"24h\"\n",
    "SYNTHETIC_DATA_PATH: Path = Path(\"./data/synthetic/synthetic_samples.jsonl\")\n",
    "BATCH_OUTPUT_DIR: Path = Path(\"./data/validation\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "BATCH_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Environment loaded successfully\")\n",
    "print(f\"  OPENAI_API_KEY: {'*' * 8}...{OPENAI_API_KEY[-4:]}\")\n",
    "print(f\"  Model: {MODEL_ID}\")\n",
    "print(f\"  Reasoning Effort: {REASONING_EFFORT}\")\n",
    "print(f\"  Synthetic Data Path: {SYNTHETIC_DATA_PATH}\")\n",
    "print(f\"  Output Directory: {BATCH_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5bc2c",
   "metadata": {},
   "source": [
    "## Pydantic Schemas\n",
    "\n",
    "Pydantic schemas for synthetic PII samples.\n",
    "\n",
    "These must match the schemas used in `data-generation.ipynb` for proper\n",
    "deserialization of the generated samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998d5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Pydantic schemas defined\n",
      "  SyntheticSample fields: ['text', 'entities', 'feature_dimension', 'seed_pii_type', 'seed_pii_value', 'seed_pii_locale', 'scenario', 'type_variant', 'generation_id', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "class FeatureDimension(str, Enum):\n",
    "    \"\"\"\n",
    "    The six NER failure mode dimensions from Singh & Narayanan (2025).\n",
    "    \n",
    "    Each dimension represents a specific type of challenge for PII detection:\n",
    "        - basic: Standard, well-formatted entities with clear boundaries\n",
    "        - contextual: Ambiguous entities requiring surrounding context\n",
    "        - noisy: Real-world text imperfections and formatting variations\n",
    "        - evolving: Modern/emerging PII formats not in traditional training data\n",
    "        - multilingual: International formats embedded in English prose\n",
    "        - adversarial: Intentionally deceptive patterns designed to evade detection\n",
    "    \"\"\"\n",
    "    BASIC = \"basic\"\n",
    "    CONTEXTUAL = \"contextual\"\n",
    "    NOISY = \"noisy\"\n",
    "    EVOLVING = \"evolving\"\n",
    "    MULTILINGUAL = \"multilingual\"\n",
    "    ADVERSARIAL = \"adversarial\"\n",
    "\n",
    "\n",
    "class EntitySpan(BaseModel):\n",
    "    \"\"\"\n",
    "    A single PII entity annotation with character-level span positions.\n",
    "    \n",
    "    Attributes:\n",
    "        start: Starting character index (0-based, inclusive).\n",
    "        end: Ending character index (exclusive, like Python slicing).\n",
    "        label: PII type label from the unified taxonomy.\n",
    "        text: The actual text content of the entity (for verification).\n",
    "    \"\"\"\n",
    "    start: int = Field(..., ge=0, description=\"Start character index (inclusive)\")\n",
    "    end: int = Field(..., gt=0, description=\"End character index (exclusive)\")\n",
    "    label: str = Field(..., description=\"PII type label\")\n",
    "    text: str = Field(..., min_length=1, description=\"Entity text content\")\n",
    "    \n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_span_bounds(self) -> \"EntitySpan\":\n",
    "        \"\"\"Ensure start < end for valid span.\"\"\"\n",
    "        if self.start >= self.end:\n",
    "            raise ValueError(f\"Invalid span: start ({self.start}) must be < end ({self.end})\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class SyntheticSample(BaseModel):\n",
    "    \"\"\"\n",
    "    A complete synthetic PII training sample with text and annotations.\n",
    "    \n",
    "    This schema captures everything needed for training and validation:\n",
    "    the generated text, all entity annotations, metadata about the\n",
    "    generation process, and the feature dimension being targeted.\n",
    "    \n",
    "    Attributes:\n",
    "        text: The generated English text containing PII entities.\n",
    "        entities: List of all PII entity annotations with spans.\n",
    "        feature_dimension: Which NER challenge dimension this targets.\n",
    "        seed_pii_type: The primary PII type used to seed generation.\n",
    "        seed_pii_value: The actual PII value that was seeded.\n",
    "        seed_pii_locale: Locale/region for international formats.\n",
    "        scenario: Brief description of the text scenario/context.\n",
    "        type_variant: Specific variant or sub-type of the PII.\n",
    "        generation_id: Unique identifier for this generation attempt.\n",
    "        timestamp: When this sample was generated.\n",
    "    \"\"\"\n",
    "    text: str = Field(..., min_length=50, max_length=600, description=\"Generated text\")\n",
    "    entities: list[EntitySpan] = Field(..., min_length=1, description=\"Entity annotations\")\n",
    "    feature_dimension: FeatureDimension = Field(..., description=\"Target dimension\")\n",
    "    seed_pii_type: str = Field(..., description=\"Primary PII type\")\n",
    "    seed_pii_value: str = Field(..., description=\"Seeded PII value\")\n",
    "    seed_pii_locale: str | None = Field(None, description=\"Locale for international formats\")\n",
    "    scenario: str = Field(..., description=\"Text scenario description\")\n",
    "    type_variant: str = Field(..., description=\"PII format variant\")\n",
    "    generation_id: str = Field(..., description=\"Unique generation ID\")\n",
    "    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n",
    "\n",
    "\n",
    "print(\"âœ“ Pydantic schemas defined\")\n",
    "print(f\"  SyntheticSample fields: {list(SyntheticSample.model_fields.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08527ee9",
   "metadata": {},
   "source": [
    "## Load Synthetic Data\n",
    "\n",
    "Load the synthetically-generated samples from JSONL.\n",
    "\n",
    "This loads all samples that were generated in `data-generation.ipynb`\n",
    "and prepares them for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ffb286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 10836 synthetic samples\n",
      "\n",
      "Samples by feature dimension:\n",
      "  adversarial: 1766\n",
      "  basic: 1818\n",
      "  contextual: 1819\n",
      "  evolving: 1813\n",
      "  multilingual: 1819\n",
      "  noisy: 1801\n",
      "\n",
      "Samples by PII type (top 10):\n",
      "  PASSPORT_NUMBER: 684\n",
      "  DATE_OF_BIRTH: 684\n",
      "  POSTAL_CODE: 682\n",
      "  NAMES_OF_PLACES_OR_NOUNS: 682\n",
      "  PHONE: 682\n",
      "  NATIONAL_IDENTITY_SSN_AADHAR: 681\n",
      "  VEHICLE_REGISTRATION: 680\n",
      "  BANK_UPI_ID: 680\n",
      "  OTHER_NATIONAL_IDENTITY: 679\n",
      "  TAX_IDENTIFICATION: 677\n"
     ]
    }
   ],
   "source": [
    "def load_synthetic_samples(path: Path) -> list[SyntheticSample]:\n",
    "    \"\"\"\n",
    "    Load synthetic samples from a JSONL file.\n",
    "    \n",
    "    Each line in the file should be a valid JSON object conforming\n",
    "    to the SyntheticSample schema.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the JSONL file containing synthetic samples.\n",
    "        \n",
    "    Returns:\n",
    "        List of parsed SyntheticSample objects.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file does not exist.\n",
    "        ValueError: If any line fails Pydantic validation.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Synthetic data file not found: {path}\")\n",
    "    \n",
    "    samples: list[SyntheticSample] = []\n",
    "    errors: list[tuple[int, str]] = []\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                sample = SyntheticSample.model_validate(data)\n",
    "                samples.append(sample)\n",
    "            except json.JSONDecodeError as e:\n",
    "                errors.append((line_num, f\"JSON decode error: {e}\"))\n",
    "            except Exception as e:\n",
    "                errors.append((line_num, f\"Validation error: {e}\"))\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"âš  {len(errors)} samples failed to load:\")\n",
    "        for line_num, error in errors[:10]:\n",
    "            print(f\"  Line {line_num}: {error}\")\n",
    "        if len(errors) > 10:\n",
    "            print(f\"  ... and {len(errors) - 10} more errors\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "# Load samples\n",
    "samples: list[SyntheticSample] = load_synthetic_samples(SYNTHETIC_DATA_PATH)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(samples)} synthetic samples\")\n",
    "\n",
    "# Display distribution by feature dimension\n",
    "dimension_counts = Counter(s.feature_dimension.value for s in samples)\n",
    "print(\"\\nSamples by feature dimension:\")\n",
    "for dim, count in sorted(dimension_counts.items()):\n",
    "    print(f\"  {dim}: {count}\")\n",
    "\n",
    "# Display distribution by PII type\n",
    "pii_type_counts = Counter(s.seed_pii_type for s in samples)\n",
    "print(\"\\nSamples by PII type (top 10):\")\n",
    "for pii_type, count in pii_type_counts.most_common(10):\n",
    "    print(f\"  {pii_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc9411",
   "metadata": {},
   "source": [
    "## Programmatic Pre-Filter\n",
    "\n",
    "Fast programmatic pre-filter to catch obvious issues before sending to GPT-5.1.\n",
    "\n",
    "This is a cost optimization to avoid wasting API calls on samples that are\n",
    "clearly broken. It is NOT a replacement for semantic validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21489839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf269dcc8594faa8a81390176825832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pre-filtering:   0%|          | 0/10836 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Pre-filter complete\n",
      "  Passed: 10836\n",
      "  Failed: 0\n",
      "  Pass rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class PreFilterResult:\n",
    "    \"\"\"\n",
    "    Result from programmatic pre-filtering.\n",
    "    \n",
    "    Attributes:\n",
    "        passes_prefilter: Whether the sample passes basic checks.\n",
    "        issues: List of issues found (empty if passes).\n",
    "    \"\"\"\n",
    "    passes_prefilter: bool\n",
    "    issues: list[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class ProgrammaticPreFilter:\n",
    "    \"\"\"\n",
    "    Fast programmatic pre-filter for obvious issues.\n",
    "    \n",
    "    This catches cheap-to-detect problems before sending to GPT-5.1:\n",
    "    - Empty or too-short/too-long text\n",
    "    - Missing required fields\n",
    "    - Invalid character positions\n",
    "    - Non-English text (high non-ASCII ratio)\n",
    "    \n",
    "    Attributes:\n",
    "        min_text_length: Minimum acceptable text length.\n",
    "        max_text_length: Maximum acceptable text length.\n",
    "        max_non_ascii_ratio: Maximum ratio of non-ASCII characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        min_text_length: int = 50,\n",
    "        max_text_length: int = 600,\n",
    "        max_non_ascii_ratio: float = 0.3,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize pre-filter with thresholds.\n",
    "        \n",
    "        Args:\n",
    "            min_text_length: Minimum acceptable text length.\n",
    "            max_text_length: Maximum acceptable text length.\n",
    "            max_non_ascii_ratio: Maximum ratio of non-ASCII characters.\n",
    "        \"\"\"\n",
    "        self.min_text_length = min_text_length\n",
    "        self.max_text_length = max_text_length\n",
    "        self.max_non_ascii_ratio = max_non_ascii_ratio\n",
    "    \n",
    "    def check(self, sample: SyntheticSample) -> PreFilterResult:\n",
    "        \"\"\"\n",
    "        Run fast programmatic checks on a sample.\n",
    "        \n",
    "        Args:\n",
    "            sample: The synthetic sample to pre-filter.\n",
    "            \n",
    "        Returns:\n",
    "            PreFilterResult indicating pass/fail and any issues.\n",
    "        \"\"\"\n",
    "        issues: list[str] = []\n",
    "        \n",
    "        # Check 1: Text length bounds\n",
    "        text_len = len(sample.text)\n",
    "        if text_len < self.min_text_length:\n",
    "            issues.append(f\"Text too short: {text_len} < {self.min_text_length}\")\n",
    "        elif text_len > self.max_text_length:\n",
    "            issues.append(f\"Text too long: {text_len} > {self.max_text_length}\")\n",
    "        \n",
    "        # Check 2: Seed PII must be present in text\n",
    "        if sample.seed_pii_value not in sample.text:\n",
    "            issues.append(f\"Seed PII '{sample.seed_pii_value}' not found in text\")\n",
    "        \n",
    "        # Check 3: Entity spans must be valid positions\n",
    "        for entity in sample.entities:\n",
    "            if entity.start < 0 or entity.end > len(sample.text):\n",
    "                issues.append(\n",
    "                    f\"Invalid span [{entity.start}:{entity.end}] for text length {len(sample.text)}\"\n",
    "                )\n",
    "            elif entity.start >= entity.end:\n",
    "                issues.append(f\"Invalid span [{entity.start}:{entity.end}]: start >= end\")\n",
    "            else:\n",
    "                # Check span text matches entity text\n",
    "                actual_text = sample.text[entity.start:entity.end]\n",
    "                if actual_text != entity.text:\n",
    "                    issues.append(\n",
    "                        f\"Span text mismatch at [{entity.start}:{entity.end}]: \"\n",
    "                        f\"expected '{entity.text}', got '{actual_text}'\"\n",
    "                    )\n",
    "        \n",
    "        # Check 4: At least one entity required\n",
    "        if len(sample.entities) == 0:\n",
    "            issues.append(\"No entities annotated\")\n",
    "        \n",
    "        # Check 5: Text should be primarily English (basic heuristic)\n",
    "        non_ascii = sum(1 for c in sample.text if ord(c) > 127)\n",
    "        non_ascii_ratio = non_ascii / len(sample.text) if sample.text else 0\n",
    "        if non_ascii_ratio > self.max_non_ascii_ratio:\n",
    "            issues.append(\n",
    "                f\"High non-ASCII ratio: {non_ascii_ratio:.2%} > {self.max_non_ascii_ratio:.0%}\"\n",
    "            )\n",
    "        \n",
    "        return PreFilterResult(\n",
    "            passes_prefilter=len(issues) == 0,\n",
    "            issues=issues,\n",
    "        )\n",
    "\n",
    "\n",
    "# Run pre-filter on all samples\n",
    "prefilter = ProgrammaticPreFilter()\n",
    "\n",
    "prefilter_results: list[tuple[SyntheticSample, PreFilterResult]] = []\n",
    "passed_prefilter: list[SyntheticSample] = []\n",
    "failed_prefilter: list[tuple[SyntheticSample, PreFilterResult]] = []\n",
    "\n",
    "for sample in tqdm(samples, desc=\"Pre-filtering\"):\n",
    "    result = prefilter.check(sample)\n",
    "    prefilter_results.append((sample, result))\n",
    "    \n",
    "    if result.passes_prefilter:\n",
    "        passed_prefilter.append(sample)\n",
    "    else:\n",
    "        failed_prefilter.append((sample, result))\n",
    "\n",
    "print(f\"\\nâœ“ Pre-filter complete\")\n",
    "print(f\"  Passed: {len(passed_prefilter)}\")\n",
    "print(f\"  Failed: {len(failed_prefilter)}\")\n",
    "print(f\"  Pass rate: {len(passed_prefilter) / len(samples) * 100:.1f}%\")\n",
    "\n",
    "if failed_prefilter:\n",
    "    print(\"\\nSample pre-filter failures (first 5):\")\n",
    "    for sample, result in failed_prefilter[:5]:\n",
    "        print(f\"  {sample.generation_id}: {result.issues}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b93f39",
   "metadata": {},
   "source": [
    "## Validation System Prompt and Request Formatting\n",
    "\n",
    "OpenAI GPT-5.1 validation prompt and request formatting.\n",
    "\n",
    "The validation system prompt instructs GPT-5.1 to perform deep semantic\n",
    "analysis of each synthetic sample, checking:\n",
    "\n",
    "-   Text coherence and naturalness\n",
    "-   Entity label correctness\n",
    "-   Span boundary accuracy\n",
    "-   Feature dimension characteristic verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2041e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample validation prompt (first sample):\n",
      "------------------------------------------------------------\n",
      "SAMPLE TO VALIDATE:\n",
      "\n",
      "TEXT:\n",
      "\"\"\"Guten Tag Herr MÃ¼ller, vielen Dank fÃ¼r Ihre Fintech-App-Anmeldung. Wir haben Ihren deutschen Reisepass C41531085 verifiziert. Bitte bestÃ¤tigen Sie die UPI-ID netflixuser@paytm und Ihr Ethereum-Wallet 0x742d35Cc6634C0532925a3b8D87e061Df4B15f62 fÃ¼r die Auszahlungen. Der 2FA-Code ist 847392, gÃ¼ltig fÃ¼r 5 Minuten. Falls Probleme, kontaktieren Sie support@fintech.de.\"\"\"\n",
      "\n",
      "ENTITY ANNOTATIONS:\n",
      "  - 'C41531085' [103:112] â†’ PASSPORT_NUMBER\n",
      "  - 'netflixuser@paytm' [158:175] â†’ UPI_ID\n",
      "  - '0x742d35Cc6634C0532925a3b8D87e061Df4B15f62' [200:242] â†’ CRYPTO_WALLET\n",
      "  - '847392' [282:288] â†’ TOTP_CODE\n",
      "\n",
      "CLAIMED FEATURE DIMENSION: evolving\n",
      "SEED PII TYPE: PASSPORT_NUMBER\n",
      "SEED PII VALUE: C41531085\n",
      "SEED PII LOCALE: de_DE\n",
      "SCENARIO: Fintech app onboarding email verifying passport and linking modern digital payment identifiers for a German user\n",
      "TYPE VARIANT: standard\n",
      "\n",
      "Validate this sample according to your criteria.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# System prompt for GPT-5.1 validation\n",
    "VALIDATION_SYSTEM_PROMPT: str = \"\"\"You are an expert data quality validator for PII (Personally Identifiable Information) detection training data.\n",
    "\n",
    "Your task is to validate synthetic training samples for a Named Entity Recognition (NER) model. Each sample contains:\n",
    "1. A text passage (should be natural, coherent English)\n",
    "2. Entity annotations with character-level spans and labels\n",
    "3. A claimed \"feature dimension\" representing the type of NER challenge\n",
    "\n",
    "FEATURE DIMENSIONS (from Singh & Narayanan 2025):\n",
    "- basic: Straightforward, well-formatted entities in clear context\n",
    "- contextual: Entities requiring disambiguation (e.g., \"Apple\" as company vs fruit)\n",
    "- noisy: Real-world imperfections (typos, OCR errors, abbreviations, formatting issues)\n",
    "- evolving: New/emerging PII formats (crypto addresses, UPI IDs, modern handles)\n",
    "- multilingual: PII in international formats embedded in English text\n",
    "- adversarial: Intentionally confusing inputs designed to fool NER models\n",
    "\n",
    "PII TYPES TO VALIDATE:\n",
    "NAME, EMAIL, PHONE, DATE_OF_BIRTH, POSTAL_CODE, CREDIT_CARD, BANK_ACCOUNT,\n",
    "DRIVER_LICENSE, PASSPORT_NUMBER, NATIONAL_IDENTITY_SSN_AADHAR, OTHER_NATIONAL_IDENTITY,\n",
    "TAX_IDENTIFICATION, VEHICLE_REGISTRATION, INSURANCE_NUMBER, BANK_UPI_ID,\n",
    "NAMES_OF_PLACES_OR_NOUNS\n",
    "\n",
    "YOUR VALIDATION CRITERIA:\n",
    "\n",
    "1. TEXT COHERENCE (1-5 scale):\n",
    "   - 5: Perfectly natural, indistinguishable from human-written text\n",
    "   - 4: Minor awkwardness but clearly understandable\n",
    "   - 3: Noticeable issues but usable for training\n",
    "   - 2: Significant problems affecting training quality\n",
    "   - 1: Incoherent, unusable garbage\n",
    "   \n",
    "2. ENTITY VALIDATION:\n",
    "   - Is the label semantically correct for the entity text?\n",
    "   - Are the span boundaries accurate (no missing/extra characters)?\n",
    "   - Is the PII format realistic for its claimed locale?\n",
    "   - Could this entity be confused with something else?\n",
    "\n",
    "3. DIMENSION VALIDATION:\n",
    "   - Does the sample ACTUALLY exhibit the claimed dimension's characteristics?\n",
    "   - For \"noisy\": Are there realistic typos/OCR errors/formatting issues?\n",
    "   - For \"contextual\": Is there genuine ambiguity requiring context?\n",
    "   - For \"adversarial\": Would this actually fool an NER model?\n",
    "\n",
    "4. OVERALL VALIDITY:\n",
    "   - ERROR (invalid): Critical issues that would harm model training\n",
    "   - WARNING (valid with issues): Minor issues, usable but suboptimal\n",
    "   - PASS (valid): Good quality sample ready for training\n",
    "\n",
    "Respond with a JSON object matching this exact schema:\n",
    "{\n",
    "    \"is_valid\": boolean,\n",
    "    \"severity\": \"pass\" | \"warning\" | \"error\",\n",
    "    \"text_coherence_score\": 1-5,\n",
    "    \"text_coherence_issues\": [\"issue1\", \"issue2\", ...],\n",
    "    \"entity_validations\": [\n",
    "        {\n",
    "            \"entity_text\": \"...\",\n",
    "            \"entity_label\": \"...\",\n",
    "            \"is_correct_label\": boolean,\n",
    "            \"is_correct_boundary\": boolean,\n",
    "            \"suggested_label\": \"...\" or null,\n",
    "            \"issue_description\": \"...\" or null\n",
    "        }\n",
    "    ],\n",
    "    \"dimension_validation\": {\n",
    "        \"claimed_dimension\": \"...\",\n",
    "        \"exhibits_characteristics\": boolean,\n",
    "        \"characteristics_found\": [\"...\"],\n",
    "        \"missing_characteristics\": [\"...\"]\n",
    "    },\n",
    "    \"overall_assessment\": \"Free-form assessment\",\n",
    "    \"suggested_fixes\": [\"fix1\", \"fix2\", ...]\n",
    "}\n",
    "\n",
    "Be rigorous but fair. We need high-quality training data, but don't reject samples over trivial issues.\"\"\"\n",
    "\n",
    "\n",
    "def format_sample_for_validation(sample: SyntheticSample) -> str:\n",
    "    \"\"\"\n",
    "    Format a synthetic sample into a prompt for GPT-5.1 validation.\n",
    "    \n",
    "    This creates a structured representation of the sample that includes\n",
    "    all information needed for thorough validation.\n",
    "    \n",
    "    Args:\n",
    "        sample: The synthetic sample to validate.\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string representation for the validation prompt.\n",
    "    \"\"\"\n",
    "    entities_formatted: list[str] = []\n",
    "    for entity in sample.entities:\n",
    "        entity_text = sample.text[entity.start:entity.end]\n",
    "        entities_formatted.append(\n",
    "            f\"  - '{entity_text}' [{entity.start}:{entity.end}] â†’ {entity.label}\"\n",
    "        )\n",
    "    \n",
    "    return f\"\"\"SAMPLE TO VALIDATE:\n",
    "\n",
    "TEXT:\n",
    "\\\"\\\"\\\"{sample.text}\\\"\\\"\\\"\n",
    "\n",
    "ENTITY ANNOTATIONS:\n",
    "{chr(10).join(entities_formatted)}\n",
    "\n",
    "CLAIMED FEATURE DIMENSION: {sample.feature_dimension.value}\n",
    "SEED PII TYPE: {sample.seed_pii_type}\n",
    "SEED PII VALUE: {sample.seed_pii_value}\n",
    "SEED PII LOCALE: {sample.seed_pii_locale or \"unspecified\"}\n",
    "SCENARIO: {sample.scenario}\n",
    "TYPE VARIANT: {sample.type_variant}\n",
    "\n",
    "Validate this sample according to your criteria.\"\"\"\n",
    "\n",
    "\n",
    "# Test with a sample\n",
    "if passed_prefilter:\n",
    "    test_sample = passed_prefilter[0]\n",
    "    print(\"Sample validation prompt (first sample):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(format_sample_for_validation(test_sample))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881c08d",
   "metadata": {},
   "source": [
    "## Test validation with a small sample set (synchronous, single-threaded).\n",
    "\n",
    "This cell validates a handful of samples using direct API calls to verify:\n",
    "\n",
    "1. The Responses API format is correct\n",
    "2. GPT-5.1 returns properly structured JSON responses\n",
    "3. The response parsing works correctly\n",
    "4. The validation logic produces sensible results\n",
    "\n",
    "Run this BEFORE running the full concurrent validation to catch issues early.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98303337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 3 samples:\n",
      "  1. evolving_PASSPORT_NUMBER_e519c0e4 (evolving, PASSPORT_NUMBER)\n",
      "  2. multilingual_OTHER_NATIONAL_IDENTITY_b00d3f36 (multilingual, OTHER_NATIONAL_IDENTITY)\n",
      "  3. basic_PASSPORT_NUMBER_06a75db0 (basic, PASSPORT_NUMBER)\n",
      "\n",
      "======================================================================\n",
      "RUNNING TEST VALIDATION (Responses API)\n",
      "Model: gpt-5.1-2025-11-13 | Reasoning: low\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203ba0358f054efca47a7edb615c5e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SAMPLE 1: evolving_PASSPORT_NUMBER_e519c0e4\n",
      "  Dimension: evolving | PII: PASSPORT_NUMBER\n",
      "  Text: Guten Tag Herr MÃ¼ller, vielen Dank fÃ¼r Ihre Fintech-App-Anme...\n",
      "\n",
      "  âœ… RESULT:\n",
      "     Severity: warning | Coherence: 5/5\n",
      "     Valid: False | Exhibits Dimension: True\n",
      "     Assessment: The text is coherent, natural German and fits the described fintech onboarding s...\n",
      "     Tokens: 2569\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SAMPLE 2: multilingual_OTHER_NATIONAL_IDENTITY_b00d3f36\n",
      "  Dimension: multilingual | PII: OTHER_NATIONAL_IDENTITY\n",
      "  Text: Dear Mr. Patel, thank you for submitting your employment app...\n",
      "\n",
      "  âœ… RESULT:\n",
      "     Severity: warning | Coherence: 5/5\n",
      "     Valid: True | Exhibits Dimension: True\n",
      "     Assessment: The text is coherent, natural, and suitable for training. Entity boundaries are ...\n",
      "     Tokens: 2174\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "SAMPLE 3: basic_PASSPORT_NUMBER_06a75db0\n",
      "  Dimension: basic | PII: PASSPORT_NUMBER\n",
      "  Text: Passport Application Confirmation\n",
      "\n",
      "Dear Mr. Hans MÃ¼ller,\n",
      "\n",
      "Yo...\n",
      "\n",
      "  âœ… RESULT:\n",
      "     Severity: warning | Coherence: 5/5\n",
      "     Valid: True | Exhibits Dimension: True\n",
      "     Assessment: The sample is high-quality, coherent, and realistic for a German passport confir...\n",
      "     Tokens: 2101\n",
      "\n",
      "======================================================================\n",
      "TEST SUMMARY\n",
      "======================================================================\n",
      "  Successful: 3/3\n",
      "  Total tokens: 6844\n",
      "  Avg tokens/sample: 2281\n",
      "\n",
      "  ðŸ“ˆ FULL RUN ESTIMATES (10836 samples):\n",
      "     Total tokens: ~24,720,528\n",
      "     Time (40 concurrent): ~113 minutes\n",
      "\n",
      "âœ… All tests passed! Safe to proceed with full validation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATION RESULT SCHEMAS (self-contained definitions for testing)\n",
    "# =============================================================================\n",
    "\n",
    "class ValidationSeverity(str, Enum):\n",
    "    \"\"\"Severity levels for validation issues.\"\"\"\n",
    "    PASS = \"pass\"\n",
    "    WARNING = \"warning\"\n",
    "    ERROR = \"error\"\n",
    "\n",
    "\n",
    "class EntityValidation(BaseModel):\n",
    "    \"\"\"Validation result for a single entity annotation.\"\"\"\n",
    "    entity_text: str\n",
    "    entity_label: str\n",
    "    is_correct_label: bool\n",
    "    is_correct_boundary: bool\n",
    "    suggested_label: str | None = None\n",
    "    issue_description: str | None = None\n",
    "\n",
    "\n",
    "class DimensionValidation(BaseModel):\n",
    "    \"\"\"Validation result for feature dimension characteristics.\"\"\"\n",
    "    claimed_dimension: str\n",
    "    exhibits_characteristics: bool\n",
    "    characteristics_found: list[str] = Field(default_factory=list)\n",
    "    missing_characteristics: list[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class OpenAIValidationResult(BaseModel):\n",
    "    \"\"\"Complete validation result from OpenAI GPT-5.1 analysis.\"\"\"\n",
    "    is_valid: bool\n",
    "    severity: ValidationSeverity\n",
    "    text_coherence_score: int = Field(ge=1, le=5)\n",
    "    text_coherence_issues: list[str] = Field(default_factory=list)\n",
    "    entity_validations: list[EntityValidation] = Field(default_factory=list)\n",
    "    dimension_validation: DimensionValidation\n",
    "    overall_assessment: str\n",
    "    suggested_fixes: list[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TEST CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "TEST_SAMPLE_COUNT: int = 3  # Keep small for quick testing\n",
    "TEST_MODEL: str = MODEL_ID\n",
    "TEST_REASONING_EFFORT: str = REASONING_EFFORT\n",
    "\n",
    "# Verify prerequisites\n",
    "if not passed_prefilter:\n",
    "    raise ValueError(\"No samples passed pre-filter. Run Cell 4 first.\")\n",
    "\n",
    "# Select diverse test samples (one per dimension if possible)\n",
    "test_samples: list[SyntheticSample] = []\n",
    "seen_dimensions: set[str] = set()\n",
    "\n",
    "for sample in passed_prefilter:\n",
    "    dim = sample.feature_dimension.value\n",
    "    if dim not in seen_dimensions and len(test_samples) < TEST_SAMPLE_COUNT:\n",
    "        test_samples.append(sample)\n",
    "        seen_dimensions.add(dim)\n",
    "\n",
    "# Fill remaining slots\n",
    "while len(test_samples) < TEST_SAMPLE_COUNT and len(test_samples) < len(passed_prefilter):\n",
    "    for sample in passed_prefilter:\n",
    "        if sample not in test_samples:\n",
    "            test_samples.append(sample)\n",
    "            break\n",
    "\n",
    "print(f\"Testing with {len(test_samples)} samples:\")\n",
    "for i, sample in enumerate(test_samples, 1):\n",
    "    print(f\"  {i}. {sample.generation_id} ({sample.feature_dimension.value}, {sample.seed_pii_type})\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATION FUNCTION (Responses API)\n",
    "# =============================================================================\n",
    "\n",
    "def validate_single_sample_sync(\n",
    "    sample: SyntheticSample,\n",
    "    client: OpenAI,\n",
    "    model: str = TEST_MODEL,\n",
    "    reasoning_effort: str = TEST_REASONING_EFFORT,\n",
    ") -> tuple[OpenAIValidationResult | None, dict[str, Any], str | None]:\n",
    "    \"\"\"\n",
    "    Validate a single sample synchronously using the Responses API.\n",
    "    \n",
    "    Args:\n",
    "        sample: The synthetic sample to validate.\n",
    "        client: OpenAI client instance.\n",
    "        model: Model ID to use.\n",
    "        reasoning_effort: Reasoning effort level.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (parsed_result, raw_info, error_message).\n",
    "    \"\"\"\n",
    "    user_prompt = format_sample_for_validation(sample)\n",
    "    content: str | None = None\n",
    "    \n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=model,\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"developer\",\n",
    "                    \"content\": [{\"type\": \"input_text\", \"text\": VALIDATION_SYSTEM_PROMPT}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"input_text\", \"text\": user_prompt}],\n",
    "                },\n",
    "            ],\n",
    "            reasoning={\"effort\": reasoning_effort},\n",
    "            text={\"format\": {\"type\": \"json_object\"}},\n",
    "            max_output_tokens=2000,\n",
    "        )\n",
    "        \n",
    "        content = response.output_text\n",
    "        if not content:\n",
    "            return None, {}, \"Empty response (output_text is None)\"\n",
    "        \n",
    "        result_dict = json.loads(content)\n",
    "        parsed_result = OpenAIValidationResult.model_validate(result_dict)\n",
    "        \n",
    "        tokens = 0\n",
    "        if response.usage:\n",
    "            tokens = response.usage.input_tokens + response.usage.output_tokens\n",
    "        \n",
    "        raw_info = {\n",
    "            \"model\": response.model,\n",
    "            \"id\": response.id,\n",
    "            \"usage\": {\n",
    "                \"input_tokens\": response.usage.input_tokens if response.usage else 0,\n",
    "                \"output_tokens\": response.usage.output_tokens if response.usage else 0,\n",
    "                \"total_tokens\": tokens,\n",
    "            },\n",
    "            \"raw_content\": content,\n",
    "        }\n",
    "        \n",
    "        return parsed_result, raw_info, None\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, {\"raw_content\": content}, f\"JSON parse error: {e}\"\n",
    "    except Exception as e:\n",
    "        return None, {}, f\"API error: {type(e).__name__}: {e}\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN TEST\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RUNNING TEST VALIDATION (Responses API)\")\n",
    "print(f\"Model: {TEST_MODEL} | Reasoning: {TEST_REASONING_EFFORT}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "test_results: list[tuple[SyntheticSample, OpenAIValidationResult | None, dict[str, Any], str | None]] = []\n",
    "\n",
    "for sample in tqdm(test_samples, desc=\"Validating\"):\n",
    "    result, raw_info, error = validate_single_sample_sync(sample, client)\n",
    "    test_results.append((sample, result, raw_info, error))\n",
    "\n",
    "# Display results\n",
    "total_success = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for i, (sample, result, raw_info, error) in enumerate(test_results, 1):\n",
    "    print(f\"\\n{'â”€'*50}\")\n",
    "    print(f\"SAMPLE {i}: {sample.generation_id}\")\n",
    "    print(f\"  Dimension: {sample.feature_dimension.value} | PII: {sample.seed_pii_type}\")\n",
    "    print(f\"  Text: {sample.text[:60]}...\")\n",
    "    \n",
    "    if error:\n",
    "        print(f\"\\n  âŒ ERROR: {error}\")\n",
    "    else:\n",
    "        total_success += 1\n",
    "        total_tokens += raw_info[\"usage\"][\"total_tokens\"]\n",
    "        \n",
    "        print(f\"\\n  âœ… RESULT:\")\n",
    "        print(f\"     Severity: {result.severity.value} | Coherence: {result.text_coherence_score}/5\")\n",
    "        print(f\"     Valid: {result.is_valid} | Exhibits Dimension: {result.dimension_validation.exhibits_characteristics}\")\n",
    "        print(f\"     Assessment: {result.overall_assessment[:80]}...\")\n",
    "        print(f\"     Tokens: {raw_info['usage']['total_tokens']}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Successful: {total_success}/{len(test_samples)}\")\n",
    "print(f\"  Total tokens: {total_tokens}\")\n",
    "\n",
    "if total_success > 0:\n",
    "    avg_tokens = total_tokens / total_success\n",
    "    est_total = avg_tokens * len(passed_prefilter)\n",
    "    est_time_min = len(passed_prefilter) / 40 * 25 / 60\n",
    "    print(f\"  Avg tokens/sample: {avg_tokens:.0f}\")\n",
    "    print(f\"\\n  ðŸ“ˆ FULL RUN ESTIMATES ({len(passed_prefilter)} samples):\")\n",
    "    print(f\"     Total tokens: ~{est_total:,.0f}\")\n",
    "    print(f\"     Time (40 concurrent): ~{est_time_min:.0f} minutes\")\n",
    "\n",
    "if total_success == len(test_samples):\n",
    "    print(f\"\\nâœ… All tests passed! Safe to proceed with full validation.\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Some tests failed. Review errors before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6295b2",
   "metadata": {},
   "source": [
    "## Concurrent async validation using the Responses API.\n",
    "\n",
    "Uses asyncio with a semaphore to run 40 requests simultaneously.\n",
    "\n",
    "With 40 concurrent requests and ~25s average per request:\n",
    "\n",
    "-   10,836 samples Ã· 40 = 271 batches\n",
    "-   271 Ã— 25s = 6,775s â‰ˆ 113 minutes â‰ˆ 2 hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37eb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from typing import Any\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# Handle nested event loops in Jupyter\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    pass  # Not needed if not in nested loop situation\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATION RESULT SCHEMAS\n",
    "# =============================================================================\n",
    "\n",
    "class ValidationSeverity(str, Enum):\n",
    "    \"\"\"Severity levels for validation issues.\"\"\"\n",
    "    PASS = \"pass\"\n",
    "    WARNING = \"warning\"\n",
    "    ERROR = \"error\"\n",
    "\n",
    "\n",
    "class EntityValidation(BaseModel):\n",
    "    \"\"\"Validation result for a single entity annotation.\"\"\"\n",
    "    entity_text: str\n",
    "    entity_label: str\n",
    "    is_correct_label: bool\n",
    "    is_correct_boundary: bool\n",
    "    suggested_label: str | None = None\n",
    "    issue_description: str | None = None\n",
    "\n",
    "\n",
    "class DimensionValidation(BaseModel):\n",
    "    \"\"\"Validation result for feature dimension characteristics.\"\"\"\n",
    "    claimed_dimension: str\n",
    "    exhibits_characteristics: bool\n",
    "    characteristics_found: list[str] = Field(default_factory=list)\n",
    "    missing_characteristics: list[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class OpenAIValidationResult(BaseModel):\n",
    "    \"\"\"Complete validation result from OpenAI GPT-5.1 analysis.\"\"\"\n",
    "    is_valid: bool\n",
    "    severity: ValidationSeverity\n",
    "    text_coherence_score: int = Field(ge=1, le=5)\n",
    "    text_coherence_issues: list[str] = Field(default_factory=list)\n",
    "    entity_validations: list[EntityValidation] = Field(default_factory=list)\n",
    "    dimension_validation: DimensionValidation\n",
    "    overall_assessment: str\n",
    "    suggested_fixes: list[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RESULT WRAPPER\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ValidationResultWrapper:\n",
    "    \"\"\"\n",
    "    Container for validation results with metadata.\n",
    "    \n",
    "    Attributes:\n",
    "        sample: Original synthetic sample.\n",
    "        result: Parsed validation result (None if failed).\n",
    "        error: Error message if failed.\n",
    "        tokens_used: Total tokens consumed.\n",
    "        duration_seconds: Request duration.\n",
    "    \"\"\"\n",
    "    sample: SyntheticSample\n",
    "    result: OpenAIValidationResult | None = None\n",
    "    error: str | None = None\n",
    "    tokens_used: int = 0\n",
    "    duration_seconds: float = 0.0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "MAX_CONCURRENT: int = 40\n",
    "MAX_RETRIES: int = 3\n",
    "RETRY_DELAY_BASE: float = 2.0\n",
    "CHECKPOINT_EVERY: int = 500\n",
    "\n",
    "async_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"CONCURRENT VALIDATION CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Model: {MODEL_ID}\")\n",
    "print(f\"  Reasoning effort: {REASONING_EFFORT}\")\n",
    "print(f\"  Concurrent requests: {MAX_CONCURRENT}\")\n",
    "print(f\"  Samples to validate: {len(passed_prefilter)}\")\n",
    "print(f\"  Checkpoint interval: every {CHECKPOINT_EVERY} samples\")\n",
    "est_minutes = len(passed_prefilter) / MAX_CONCURRENT * 25 / 60\n",
    "print(f\"  Estimated time: ~{est_minutes:.0f} minutes ({est_minutes/60:.1f} hours)\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ASYNC VALIDATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "async def validate_sample_async(\n",
    "    sample: SyntheticSample,\n",
    "    client: AsyncOpenAI,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "    model: str = MODEL_ID,\n",
    "    reasoning_effort: str = REASONING_EFFORT,\n",
    ") -> ValidationResultWrapper:\n",
    "    \"\"\"\n",
    "    Validate a single sample asynchronously with retry logic.\n",
    "    \n",
    "    Uses semaphore to limit concurrency. Implements exponential backoff\n",
    "    for transient errors like rate limits and timeouts.\n",
    "    \"\"\"\n",
    "    user_prompt = format_sample_for_validation(sample)\n",
    "    start_time = asyncio.get_event_loop().time()\n",
    "    \n",
    "    async with semaphore:\n",
    "        last_error: str | None = None\n",
    "        \n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = await client.responses.create(\n",
    "                    model=model,\n",
    "                    input=[\n",
    "                        {\n",
    "                            \"role\": \"developer\",\n",
    "                            \"content\": [{\"type\": \"input_text\", \"text\": VALIDATION_SYSTEM_PROMPT}],\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [{\"type\": \"input_text\", \"text\": user_prompt}],\n",
    "                        },\n",
    "                    ],\n",
    "                    reasoning={\"effort\": reasoning_effort},\n",
    "                    text={\"format\": {\"type\": \"json_object\"}},\n",
    "                    max_output_tokens=2000,\n",
    "                )\n",
    "                \n",
    "                content = response.output_text\n",
    "                if not content:\n",
    "                    raise ValueError(\"Empty response (output_text is None)\")\n",
    "                \n",
    "                result_dict = json.loads(content)\n",
    "                parsed_result = OpenAIValidationResult.model_validate(result_dict)\n",
    "                \n",
    "                tokens = 0\n",
    "                if response.usage:\n",
    "                    tokens = response.usage.input_tokens + response.usage.output_tokens\n",
    "                \n",
    "                duration = asyncio.get_event_loop().time() - start_time\n",
    "                \n",
    "                return ValidationResultWrapper(\n",
    "                    sample=sample,\n",
    "                    result=parsed_result,\n",
    "                    tokens_used=tokens,\n",
    "                    duration_seconds=duration,\n",
    "                )\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                last_error = f\"JSON parse error: {e}\"\n",
    "                break  # Don't retry JSON errors\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_error = f\"{type(e).__name__}: {e}\"\n",
    "                \n",
    "                # Check if retryable\n",
    "                err_str = str(e).lower()\n",
    "                retryable = any(x in err_str for x in [\n",
    "                    \"rate limit\", \"timeout\", \"connection\", \"server error\",\n",
    "                    \"503\", \"502\", \"429\", \"500\"\n",
    "                ])\n",
    "                \n",
    "                if retryable and attempt < MAX_RETRIES - 1:\n",
    "                    delay = RETRY_DELAY_BASE * (2 ** attempt)\n",
    "                    await asyncio.sleep(delay)\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        duration = asyncio.get_event_loop().time() - start_time\n",
    "        return ValidationResultWrapper(\n",
    "            sample=sample,\n",
    "            error=last_error,\n",
    "            duration_seconds=duration,\n",
    "        )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CHECKPOINT FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def save_checkpoint(\n",
    "    results: list[ValidationResultWrapper],\n",
    "    path: Path,\n",
    ") -> None:\n",
    "    \"\"\"Save current results to checkpoint file.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in results:\n",
    "            record = {\n",
    "                \"generation_id\": r.sample.generation_id,\n",
    "                \"success\": r.result is not None,\n",
    "                \"error\": r.error,\n",
    "                \"tokens_used\": r.tokens_used,\n",
    "                \"duration_seconds\": r.duration_seconds,\n",
    "            }\n",
    "            if r.result:\n",
    "                record[\"validation_result\"] = r.result.model_dump()\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN VALIDATION RUNNER\n",
    "# =============================================================================\n",
    "\n",
    "async def run_validation(\n",
    "    samples: list[SyntheticSample],\n",
    "    client: AsyncOpenAI,\n",
    ") -> list[ValidationResultWrapper]:\n",
    "    \"\"\"Run concurrent validation on all samples with progress tracking.\"\"\"\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "    tasks = [validate_sample_async(s, client, semaphore) for s in samples]\n",
    "    \n",
    "    results: list[ValidationResultWrapper] = []\n",
    "    checkpoint_count = 0\n",
    "    \n",
    "    for coro in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"Validating\"):\n",
    "        result = await coro\n",
    "        results.append(result)\n",
    "        \n",
    "        # Periodic checkpoint\n",
    "        if len(results) % CHECKPOINT_EVERY == 0:\n",
    "            checkpoint_count += 1\n",
    "            cp_path = BATCH_OUTPUT_DIR / f\"checkpoint_{checkpoint_count:03d}.jsonl\"\n",
    "            save_checkpoint(results, cp_path)\n",
    "            \n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXECUTE VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STARTING VALIDATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Run the async validation\n",
    "validation_results = await run_validation(passed_prefilter, async_client)\n",
    "\n",
    "print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PROCESS RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "successful = [r for r in validation_results if r.result is not None]\n",
    "failed = [r for r in validation_results if r.result is None]\n",
    "\n",
    "total_tokens = sum(r.tokens_used for r in validation_results)\n",
    "total_duration = sum(r.duration_seconds for r in validation_results)\n",
    "avg_duration = total_duration / len(validation_results) if validation_results else 0\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"VALIDATION COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Total samples: {len(validation_results)}\")\n",
    "print(f\"  Successful: {len(successful)} ({len(successful)/len(validation_results)*100:.1f}%)\")\n",
    "print(f\"  Failed: {len(failed)} ({len(failed)/len(validation_results)*100:.1f}%)\")\n",
    "print(f\"  Total tokens: {total_tokens:,}\")\n",
    "print(f\"  Avg duration/sample: {avg_duration:.1f}s\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\nFailed samples (first 10):\")\n",
    "    error_counts: dict[str, int] = {}\n",
    "    for r in failed:\n",
    "        err_type = r.error.split(\":\")[0] if r.error else \"Unknown\"\n",
    "        error_counts[err_type] = error_counts.get(err_type, 0) + 1\n",
    "    for err_type, count in sorted(error_counts.items(), key=lambda x: -x[1])[:5]:\n",
    "        print(f\"  {err_type}: {count}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE FINAL RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "final_path = BATCH_OUTPUT_DIR / \"validation_results_final.jsonl\"\n",
    "with open(final_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in validation_results:\n",
    "        record = {\n",
    "            \"generation_id\": r.sample.generation_id,\n",
    "            \"sample\": r.sample.model_dump(),\n",
    "            \"success\": r.result is not None,\n",
    "            \"error\": r.error,\n",
    "            \"tokens_used\": r.tokens_used,\n",
    "            \"duration_seconds\": r.duration_seconds,\n",
    "        }\n",
    "        if r.result:\n",
    "            record[\"validation_result\"] = r.result.model_dump()\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"\\nâœ“ Saved: {final_path}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE OUTPUT FOR DOWNSTREAM CELLS\n",
    "# =============================================================================\n",
    "\n",
    "# This is the format expected by Cell 10 (Generate Validation Report)\n",
    "successful_validations: list[tuple[SyntheticSample, OpenAIValidationResult]] = [\n",
    "    (r.sample, r.result) for r in successful\n",
    "]\n",
    "\n",
    "# Also track failed validations for Cell 11\n",
    "failed_validations: list[tuple[SyntheticSample, str]] = [\n",
    "    (r.sample, r.error or \"Unknown error\") for r in failed\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created successful_validations: {len(successful_validations)} entries\")\n",
    "print(f\"âœ“ Created failed_validations: {len(failed_validations)} entries\")\n",
    "print(f\"\\nâ†’ Ready for Cell 10 (Generate Validation Report)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe791b",
   "metadata": {},
   "source": [
    "## Generate Validation Report\n",
    "\n",
    "Generate comprehensive validation report from the results.\n",
    "\n",
    "This provides aggregate statistics, per-dimension breakdowns, and\n",
    "actionable insights for understanding synthetic data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_report(\n",
    "    results: list[tuple[SyntheticSample, OpenAIValidationResult]],\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive validation report.\n",
    "    \n",
    "    Args:\n",
    "        results: List of (sample, result) tuples from validation.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing aggregate statistics and breakdowns.\n",
    "    \"\"\"\n",
    "    total = len(results)\n",
    "    if total == 0:\n",
    "        return {\"error\": \"No results to report\"}\n",
    "    \n",
    "    passed = sum(1 for _, r in results if r.severity == ValidationSeverity.PASS)\n",
    "    warnings = sum(1 for _, r in results if r.severity == ValidationSeverity.WARNING)\n",
    "    errors = sum(1 for _, r in results if r.severity == ValidationSeverity.ERROR)\n",
    "    \n",
    "    # Coherence score distribution\n",
    "    coherence_scores = [r.text_coherence_score for _, r in results]\n",
    "    avg_coherence = sum(coherence_scores) / len(coherence_scores)\n",
    "    \n",
    "    # Per-dimension breakdown\n",
    "    dimension_stats: dict[str, dict[str, int]] = {}\n",
    "    for sample, result in results:\n",
    "        dim = sample.feature_dimension.value\n",
    "        if dim not in dimension_stats:\n",
    "            dimension_stats[dim] = {\n",
    "                \"total\": 0,\n",
    "                \"pass\": 0,\n",
    "                \"warning\": 0,\n",
    "                \"error\": 0,\n",
    "                \"exhibits_dim\": 0,\n",
    "            }\n",
    "        dimension_stats[dim][\"total\"] += 1\n",
    "        dimension_stats[dim][result.severity.value] += 1\n",
    "        if result.dimension_validation.exhibits_characteristics:\n",
    "            dimension_stats[dim][\"exhibits_dim\"] += 1\n",
    "    \n",
    "    # Per-PII-type breakdown\n",
    "    pii_type_stats: dict[str, dict[str, int]] = {}\n",
    "    for sample, result in results:\n",
    "        pii_type = sample.seed_pii_type\n",
    "        if pii_type not in pii_type_stats:\n",
    "            pii_type_stats[pii_type] = {\n",
    "                \"total\": 0,\n",
    "                \"pass\": 0,\n",
    "                \"warning\": 0,\n",
    "                \"error\": 0,\n",
    "            }\n",
    "        pii_type_stats[pii_type][\"total\"] += 1\n",
    "        pii_type_stats[pii_type][result.severity.value] += 1\n",
    "    \n",
    "    # Most common issues\n",
    "    all_issues: list[str] = []\n",
    "    for _, result in results:\n",
    "        all_issues.extend(result.text_coherence_issues)\n",
    "        for ev in result.entity_validations:\n",
    "            if ev.issue_description:\n",
    "                all_issues.append(ev.issue_description)\n",
    "    \n",
    "    issue_counts = Counter(all_issues).most_common(20)\n",
    "    \n",
    "    # Entity label accuracy\n",
    "    total_entities = 0\n",
    "    correct_labels = 0\n",
    "    correct_boundaries = 0\n",
    "    for _, result in results:\n",
    "        for ev in result.entity_validations:\n",
    "            total_entities += 1\n",
    "            if ev.is_correct_label:\n",
    "                correct_labels += 1\n",
    "            if ev.is_correct_boundary:\n",
    "                correct_boundaries += 1\n",
    "    \n",
    "    return {\n",
    "        \"summary\": {\n",
    "            \"total_samples\": total,\n",
    "            \"passed\": passed,\n",
    "            \"warnings\": warnings,\n",
    "            \"errors\": errors,\n",
    "            \"pass_rate\": passed / total if total > 0 else 0,\n",
    "            \"usable_rate\": (passed + warnings) / total if total > 0 else 0,\n",
    "        },\n",
    "        \"coherence\": {\n",
    "            \"average_score\": avg_coherence,\n",
    "            \"score_distribution\": {\n",
    "                score: coherence_scores.count(score) for score in range(1, 6)\n",
    "            },\n",
    "        },\n",
    "        \"entity_accuracy\": {\n",
    "            \"total_entities\": total_entities,\n",
    "            \"correct_labels\": correct_labels,\n",
    "            \"correct_boundaries\": correct_boundaries,\n",
    "            \"label_accuracy\": correct_labels / total_entities if total_entities > 0 else 0,\n",
    "            \"boundary_accuracy\": correct_boundaries / total_entities if total_entities > 0 else 0,\n",
    "        },\n",
    "        \"per_dimension\": dimension_stats,\n",
    "        \"per_pii_type\": pii_type_stats,\n",
    "        \"top_issues\": issue_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_validation_report(report: dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Print a formatted validation report to stdout.\n",
    "    \n",
    "    Args:\n",
    "        report: Report dictionary from generate_validation_report.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"OPENAI GPT-5.1 VALIDATION REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    s = report[\"summary\"]\n",
    "    print(f\"\\nOVERALL SUMMARY:\")\n",
    "    print(f\"  Total samples validated: {s['total_samples']}\")\n",
    "    print(f\"  Passed: {s['passed']} ({s['pass_rate']*100:.1f}%)\")\n",
    "    print(f\"  Warnings: {s['warnings']}\")\n",
    "    print(f\"  Errors: {s['errors']}\")\n",
    "    print(f\"  Usable (pass + warning): {s['usable_rate']*100:.1f}%\")\n",
    "    \n",
    "    c = report[\"coherence\"]\n",
    "    print(f\"\\nTEXT COHERENCE:\")\n",
    "    print(f\"  Average score: {c['average_score']:.2f}/5.0\")\n",
    "    print(f\"  Distribution: {c['score_distribution']}\")\n",
    "    \n",
    "    e = report[\"entity_accuracy\"]\n",
    "    print(f\"\\nENTITY ANNOTATION ACCURACY:\")\n",
    "    print(f\"  Total entities validated: {e['total_entities']}\")\n",
    "    print(f\"  Correct labels: {e['correct_labels']} ({e['label_accuracy']*100:.1f}%)\")\n",
    "    print(f\"  Correct boundaries: {e['correct_boundaries']} ({e['boundary_accuracy']*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nPER-DIMENSION BREAKDOWN:\")\n",
    "    for dim, stats in sorted(report[\"per_dimension\"].items()):\n",
    "        if stats[\"total\"] > 0:\n",
    "            exhibit_rate = stats[\"exhibits_dim\"] / stats[\"total\"] * 100\n",
    "            pass_rate = stats[\"pass\"] / stats[\"total\"] * 100\n",
    "            print(f\"  {dim}:\")\n",
    "            print(\n",
    "                f\"    Total: {stats['total']}, Pass: {stats['pass']} ({pass_rate:.1f}%), \"\n",
    "                f\"Warning: {stats['warning']}, Error: {stats['error']}\"\n",
    "            )\n",
    "            print(f\"    Exhibits dimension characteristics: {exhibit_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nPER-PII-TYPE BREAKDOWN (top 10 by error rate):\")\n",
    "    pii_stats = report[\"per_pii_type\"]\n",
    "    # Sort by error rate descending\n",
    "    sorted_pii = sorted(\n",
    "        pii_stats.items(),\n",
    "        key=lambda x: x[1][\"error\"] / x[1][\"total\"] if x[1][\"total\"] > 0 else 0,\n",
    "        reverse=True,\n",
    "    )\n",
    "    for pii_type, stats in sorted_pii[:10]:\n",
    "        if stats[\"total\"] > 0:\n",
    "            error_rate = stats[\"error\"] / stats[\"total\"] * 100\n",
    "            print(\n",
    "                f\"  {pii_type}: {stats['total']} samples, \"\n",
    "                f\"Pass: {stats['pass']}, Warning: {stats['warning']}, \"\n",
    "                f\"Error: {stats['error']} ({error_rate:.1f}%)\"\n",
    "            )\n",
    "    \n",
    "    print(f\"\\nTOP 20 ISSUES:\")\n",
    "    for i, (issue, count) in enumerate(report[\"top_issues\"], 1):\n",
    "        truncated = issue[:70] + \"...\" if len(issue) > 70 else issue\n",
    "        print(f\"  {i:2}. [{count:3}x] {truncated}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Generate and print report\n",
    "report = generate_validation_report(successful_validations)\n",
    "print_validation_report(report)\n",
    "\n",
    "# Save report to JSON\n",
    "report_path = BATCH_OUTPUT_DIR / \"validation_report.json\"\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(f\"\\nâœ“ Saved report: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577e5b9",
   "metadata": {},
   "source": [
    "## Filter Valid Samples and Save\n",
    "\n",
    "Samples that pass validation (or have only warnings) are kept.\n",
    "Samples with coherence score < 3 are filtered out per `PLAN.md` requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe22c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_valid_samples(\n",
    "    validation_results: list[tuple[SyntheticSample, OpenAIValidationResult]],\n",
    "    include_warnings: bool = True,\n",
    "    min_coherence_score: int = 3,\n",
    ") -> tuple[list[SyntheticSample], list[tuple[SyntheticSample, OpenAIValidationResult, str]]]:\n",
    "    \"\"\"\n",
    "    Filter samples based on validation results.\n",
    "    \n",
    "    Args:\n",
    "        validation_results: List of (sample, result) tuples.\n",
    "        include_warnings: Whether to include samples with warnings.\n",
    "        min_coherence_score: Minimum coherence score to pass (default 3 per PLAN.md).\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (valid_samples, rejected_with_reasons).\n",
    "    \"\"\"\n",
    "    valid: list[SyntheticSample] = []\n",
    "    rejected: list[tuple[SyntheticSample, OpenAIValidationResult, str]] = []\n",
    "    \n",
    "    for sample, result in validation_results:\n",
    "        rejection_reason = None\n",
    "        \n",
    "        # Check coherence score threshold\n",
    "        if result.text_coherence_score < min_coherence_score:\n",
    "            rejection_reason = f\"Coherence score {result.text_coherence_score} < {min_coherence_score}\"\n",
    "        \n",
    "        # Check severity\n",
    "        elif result.severity == ValidationSeverity.ERROR:\n",
    "            rejection_reason = f\"Severity ERROR: {result.overall_assessment[:100]}\"\n",
    "        \n",
    "        elif result.severity == ValidationSeverity.WARNING and not include_warnings:\n",
    "            rejection_reason = f\"Severity WARNING (not included): {result.overall_assessment[:100]}\"\n",
    "        \n",
    "        # Check dimension characteristics\n",
    "        elif not result.dimension_validation.exhibits_characteristics:\n",
    "            rejection_reason = (\n",
    "                f\"Does not exhibit {result.dimension_validation.claimed_dimension} characteristics\"\n",
    "            )\n",
    "        \n",
    "        if rejection_reason:\n",
    "            rejected.append((sample, result, rejection_reason))\n",
    "        else:\n",
    "            valid.append(sample)\n",
    "    \n",
    "    return valid, rejected\n",
    "\n",
    "\n",
    "# Filter samples\n",
    "valid_samples, rejected_samples = filter_valid_samples(\n",
    "    successful_validations,\n",
    "    include_warnings=True,\n",
    "    min_coherence_score=3,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Filtering complete\")\n",
    "print(f\"  Valid samples: {len(valid_samples)}\")\n",
    "print(f\"  Rejected samples: {len(rejected_samples)}\")\n",
    "print(f\"  Pre-filter failures: {len(failed_prefilter)}\")\n",
    "print(f\"  API/parse failures: {len(failed_validations)}\")\n",
    "\n",
    "total_input = len(samples)\n",
    "total_valid = len(valid_samples)\n",
    "print(f\"\\n  Final valid rate: {total_valid}/{total_input} ({total_valid/total_input*100:.1f}%)\")\n",
    "\n",
    "# Show rejection reasons breakdown\n",
    "if rejected_samples:\n",
    "    reason_counts: Counter[str] = Counter()\n",
    "    for _, _, reason in rejected_samples:\n",
    "        # Categorize reason\n",
    "        if \"Coherence score\" in reason:\n",
    "            reason_counts[\"Low coherence score\"] += 1\n",
    "        elif \"ERROR\" in reason:\n",
    "            reason_counts[\"Severity ERROR\"] += 1\n",
    "        elif \"characteristics\" in reason:\n",
    "            reason_counts[\"Missing dimension characteristics\"] += 1\n",
    "        else:\n",
    "            reason_counts[\"Other\"] += 1\n",
    "    \n",
    "    print(\"\\nRejection reasons breakdown:\")\n",
    "    for reason, count in reason_counts.most_common():\n",
    "        print(f\"  {reason}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adce12d",
   "metadata": {},
   "source": [
    "## Export Validated Samples\n",
    "\n",
    "Export validated samples in multiple formats for downstream use.\n",
    "\n",
    "The validated samples are saved in JSONL, CSV, and Parquet formats\n",
    "for flexibility in the training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81795ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_validated_samples(\n",
    "    valid_samples: list[SyntheticSample],\n",
    "    rejected_samples: list[tuple[SyntheticSample, OpenAIValidationResult, str]],\n",
    "    output_dir: Path,\n",
    ") -> dict[str, Path]:\n",
    "    \"\"\"\n",
    "    Export validated samples in multiple formats.\n",
    "    \n",
    "    Args:\n",
    "        valid_samples: List of samples that passed validation.\n",
    "        rejected_samples: List of (sample, result, reason) tuples for rejected samples.\n",
    "        output_dir: Directory to save output files.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping format names to file paths.\n",
    "    \"\"\"\n",
    "    exported: dict[str, Path] = {}\n",
    "    \n",
    "    # 1. Valid samples - JSONL\n",
    "    valid_jsonl_path = output_dir / \"validated_samples.jsonl\"\n",
    "    with open(valid_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sample in valid_samples:\n",
    "            f.write(json.dumps(sample.model_dump(), ensure_ascii=False) + \"\\n\")\n",
    "    exported[\"valid_jsonl\"] = valid_jsonl_path\n",
    "    print(f\"âœ“ Exported valid samples (JSONL): {valid_jsonl_path}\")\n",
    "    \n",
    "    # 2. Valid samples - CSV (flattened)\n",
    "    valid_csv_data = []\n",
    "    for sample in valid_samples:\n",
    "        valid_csv_data.append({\n",
    "            \"generation_id\": sample.generation_id,\n",
    "            \"text\": sample.text,\n",
    "            \"feature_dimension\": sample.feature_dimension.value,\n",
    "            \"seed_pii_type\": sample.seed_pii_type,\n",
    "            \"seed_pii_value\": sample.seed_pii_value,\n",
    "            \"seed_pii_locale\": sample.seed_pii_locale,\n",
    "            \"scenario\": sample.scenario,\n",
    "            \"type_variant\": sample.type_variant,\n",
    "            \"num_entities\": len(sample.entities),\n",
    "            \"entities_json\": json.dumps([e.model_dump() for e in sample.entities]),\n",
    "            \"timestamp\": sample.timestamp,\n",
    "        })\n",
    "    \n",
    "    valid_df = pd.DataFrame(valid_csv_data)\n",
    "    valid_csv_path = output_dir / \"validated_samples.csv\"\n",
    "    valid_df.to_csv(valid_csv_path, index=False, encoding=\"utf-8\")\n",
    "    exported[\"valid_csv\"] = valid_csv_path\n",
    "    print(f\"âœ“ Exported valid samples (CSV): {valid_csv_path}\")\n",
    "    \n",
    "    # 3. Valid samples - Parquet\n",
    "    try:\n",
    "        valid_parquet_path = output_dir / \"validated_samples.parquet\"\n",
    "        valid_df.to_parquet(valid_parquet_path, index=False)\n",
    "        exported[\"valid_parquet\"] = valid_parquet_path\n",
    "        print(f\"âœ“ Exported valid samples (Parquet): {valid_parquet_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Note: Parquet export skipped ({e})\")\n",
    "    \n",
    "    # 4. Rejected samples - JSONL with reasons\n",
    "    rejected_jsonl_path = output_dir / \"rejected_samples.jsonl\"\n",
    "    with open(rejected_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sample, result, reason in rejected_samples:\n",
    "            record = {\n",
    "                \"sample\": sample.model_dump(),\n",
    "                \"validation_result\": result.model_dump(),\n",
    "                \"rejection_reason\": reason,\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    exported[\"rejected_jsonl\"] = rejected_jsonl_path\n",
    "    print(f\"âœ“ Exported rejected samples (JSONL): {rejected_jsonl_path}\")\n",
    "    \n",
    "    # 5. Pre-filter failures - JSONL\n",
    "    prefilter_failures_path = output_dir / \"prefilter_failures.jsonl\"\n",
    "    with open(prefilter_failures_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sample, result in failed_prefilter:\n",
    "            record = {\n",
    "                \"sample\": sample.model_dump(),\n",
    "                \"prefilter_issues\": result.issues,\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "    exported[\"prefilter_failures\"] = prefilter_failures_path\n",
    "    print(f\"âœ“ Exported pre-filter failures (JSONL): {prefilter_failures_path}\")\n",
    "    \n",
    "    # 6. Per-dimension valid samples\n",
    "    for dimension in FeatureDimension:\n",
    "        dim_samples = [s for s in valid_samples if s.feature_dimension == dimension]\n",
    "        if dim_samples:\n",
    "            dim_path = output_dir / f\"validated_{dimension.value}.json\"\n",
    "            with open(dim_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    [s.model_dump() for s in dim_samples],\n",
    "                    f,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "            exported[f\"valid_{dimension.value}\"] = dim_path\n",
    "    print(f\"âœ“ Exported dimension-specific JSON files\")\n",
    "    \n",
    "    return exported\n",
    "\n",
    "\n",
    "# Export all files\n",
    "exported_files = export_validated_samples(\n",
    "    valid_samples,\n",
    "    rejected_samples,\n",
    "    BATCH_OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal files exported: {len(exported_files)}\")\n",
    "print(\"\\nExported files:\")\n",
    "for name, path in exported_files.items():\n",
    "    size_kb = path.stat().st_size / 1024\n",
    "    print(f\"  {name}: {path} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0156deb",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Final summary of the validation pipeline results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"VALIDATION PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“Š INPUT:\")\n",
    "print(f\"   Total synthetic samples: {len(samples)}\")\n",
    "\n",
    "print(f\"\\nðŸ” PRE-FILTER (Programmatic):\")\n",
    "print(f\"   Passed: {len(passed_prefilter)}\")\n",
    "print(f\"   Failed: {len(failed_prefilter)}\")\n",
    "print(f\"   Pass rate: {len(passed_prefilter)/len(samples)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ¤– GPT-5.1 VALIDATION:\")\n",
    "print(f\"   Samples validated: {len(successful_validations)}\")\n",
    "print(f\"   API/parse failures: {len(failed_validations)}\")\n",
    "\n",
    "print(f\"\\nâœ… FINAL OUTPUT:\")\n",
    "print(f\"   Valid samples: {len(valid_samples)}\")\n",
    "print(f\"   Rejected (semantic): {len(rejected_samples)}\")\n",
    "print(f\"   Final valid rate: {len(valid_samples)/len(samples)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“ OUTPUT FILES:\")\n",
    "print(f\"   Directory: {BATCH_OUTPUT_DIR}\")\n",
    "print(f\"   Main file: validated_samples.jsonl\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ QUALITY METRICS:\")\n",
    "if report.get(\"coherence\"):\n",
    "    print(f\"   Avg coherence score: {report['coherence']['average_score']:.2f}/5.0\")\n",
    "if report.get(\"entity_accuracy\"):\n",
    "    print(f\"   Entity label accuracy: {report['entity_accuracy']['label_accuracy']*100:.1f}%\")\n",
    "    print(f\"   Entity boundary accuracy: {report['entity_accuracy']['boundary_accuracy']*100:.1f}%\")\n",
    "\n",
    "# Distribution of valid samples by dimension\n",
    "valid_by_dim = Counter(s.feature_dimension.value for s in valid_samples)\n",
    "print(f\"\\nðŸ“Š VALID SAMPLES BY DIMENSION:\")\n",
    "for dim in sorted(valid_by_dim.keys()):\n",
    "    print(f\"   {dim}: {valid_by_dim[dim]}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ“ Validation pipeline complete. Ready for Notebook 3 (training).\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
